{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa35439-d3c8-4e93-a6bb-89a7475be1fd",
   "metadata": {},
   "source": [
    "# ImageToImage Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bd2f07-b489-43f8-82e4-6a952ce1bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function to resize images\n",
    "def preprocess_image(image, target_size=(64, 64)):\n",
    "    image = tf.image.resize(image, target_size)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e64630c9-518b-4140-9110-0c7bb23f81fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the images (real_faces and real_sketches)\n",
    "def preprocess_train_data(real_faces, real_sketches):\n",
    "    real_faces = preprocess_image(real_faces)\n",
    "    real_sketches = preprocess_image(real_sketches)\n",
    "    return real_faces, real_sketches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "633489a5-ba2b-47f9-8ab3-781401ee067e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Batch 1, G1 Loss: 1.3369, G2 Loss: 2.2836, D1 Loss: 1.6694, D2 Loss: 1.4059\n",
      "Batch 2, G1 Loss: 0.7684, G2 Loss: 1.3253, D1 Loss: 2.5402, D2 Loss: 2.6238\n",
      "Batch 3, G1 Loss: 0.9999, G2 Loss: 1.3330, D1 Loss: 1.7343, D2 Loss: 2.3830\n",
      "Batch 4, G1 Loss: 1.6713, G2 Loss: 2.2116, D1 Loss: 0.6262, D2 Loss: 0.6089\n",
      "Batch 5, G1 Loss: 1.2475, G2 Loss: 1.4250, D1 Loss: 1.0459, D2 Loss: 1.8272\n",
      "Batch 6, G1 Loss: 1.4370, G2 Loss: 1.7080, D1 Loss: 0.9162, D2 Loss: 1.2866\n",
      "Batch 7, G1 Loss: 1.1752, G2 Loss: 1.9318, D1 Loss: 1.1045, D2 Loss: 0.8458\n",
      "Batch 8, G1 Loss: 1.5356, G2 Loss: 1.4222, D1 Loss: 0.6887, D2 Loss: 1.3773\n",
      "Batch 9, G1 Loss: 1.4618, G2 Loss: 2.3071, D1 Loss: 0.8114, D2 Loss: 0.6073\n",
      "Batch 10, G1 Loss: 2.1694, G2 Loss: 1.1219, D1 Loss: 0.5640, D2 Loss: 2.1010\n",
      "Batch 11, G1 Loss: 1.7287, G2 Loss: 3.6728, D1 Loss: 0.8271, D2 Loss: 0.2397\n",
      "Batch 12, G1 Loss: 2.4116, G2 Loss: 0.8159, D1 Loss: 0.3850, D2 Loss: 3.0139\n",
      "Batch 13, G1 Loss: 1.9183, G2 Loss: 5.4205, D1 Loss: 0.6376, D2 Loss: 0.2918\n",
      "Batch 14, G1 Loss: 2.7616, G2 Loss: 1.3010, D1 Loss: 0.2519, D2 Loss: 1.1898\n",
      "Batch 15, G1 Loss: 1.5342, G2 Loss: 2.3068, D1 Loss: 1.0898, D2 Loss: 0.4411\n",
      "Batch 16, G1 Loss: 4.4356, G2 Loss: 0.8719, D1 Loss: 0.1279, D2 Loss: 2.0902\n",
      "Batch 17, G1 Loss: 1.5471, G2 Loss: 5.9924, D1 Loss: 1.3070, D2 Loss: 0.1813\n",
      "Batch 18, G1 Loss: 5.4824, G2 Loss: 0.8594, D1 Loss: 0.1118, D2 Loss: 1.9341\n",
      "Batch 19, G1 Loss: 2.7503, G2 Loss: 4.6613, D1 Loss: 0.3392, D2 Loss: 0.3414\n",
      "Batch 20, G1 Loss: 1.4938, G2 Loss: 0.6022, D1 Loss: 0.9195, D2 Loss: 2.3554\n",
      "Batch 21, G1 Loss: 6.7024, G2 Loss: 6.3353, D1 Loss: 0.2262, D2 Loss: 0.4799\n",
      "Batch 22, G1 Loss: 3.6054, G2 Loss: 1.3846, D1 Loss: 0.1854, D2 Loss: 0.9238\n",
      "Batch 23, G1 Loss: 1.2990, G2 Loss: 0.6838, D1 Loss: 1.1874, D2 Loss: 1.7427\n",
      "Batch 24, G1 Loss: 8.3098, G2 Loss: 6.7363, D1 Loss: 0.2405, D2 Loss: 0.3530\n",
      "Batch 25, G1 Loss: 4.8199, G2 Loss: 1.8205, D1 Loss: 0.1751, D2 Loss: 0.3812\n",
      "Batch 26, G1 Loss: 0.8136, G2 Loss: 0.2813, D1 Loss: 2.3872, D2 Loss: 2.8929\n",
      "Batch 27, G1 Loss: 10.0481, G2 Loss: 8.3117, D1 Loss: 1.2922, D2 Loss: 0.4061\n",
      "Batch 28, G1 Loss: 3.0625, G2 Loss: 4.6689, D1 Loss: 0.3285, D2 Loss: 0.5528\n",
      "Batch 29, G1 Loss: 0.5207, G2 Loss: 0.1193, D1 Loss: 4.9075, D2 Loss: 4.2719\n",
      "Batch 30, G1 Loss: 10.1046, G2 Loss: 6.6716, D1 Loss: 6.5999, D2 Loss: 0.1862\n",
      "Batch 31, G1 Loss: 0.6120, G2 Loss: 4.2421, D1 Loss: 2.9104, D2 Loss: 0.3489\n",
      "Batch 32, G1 Loss: 3.2479, G2 Loss: 0.1422, D1 Loss: 2.6053, D2 Loss: 3.3129\n",
      "Epoch 1 Average -> G1 Loss: 2.906639575958252, G2 Loss: 2.655355453491211, D1 Loss: 1.2732340097427368, D2 Loss: 1.3436791896820068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5_/6fblyny50t1ffbsx4k05bgw80000gn/T/ipykernel_47728/3570531442.py:221: RuntimeWarning: invalid value encountered in cast\n",
      "  img_array = (fake_images[i].numpy() * 255).astype(np.uint8)  # Convert tensor to numpy array\n",
      "/var/folders/5_/6fblyny50t1ffbsx4k05bgw80000gn/T/ipykernel_47728/3570531442.py:225: RuntimeWarning: invalid value encountered in cast\n",
      "  sketch_array = (fake_sketches[i].numpy() * 255).astype(np.uint8)  # Convert tensor to numpy array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Batch 1, G1 Loss: 0.7539, G2 Loss: 8.2144, D1 Loss: 1.7634, D2 Loss: 0.3604\n",
      "Batch 2, G1 Loss: 3.0385, G2 Loss: 6.4097, D1 Loss: 1.0710, D2 Loss: 0.1661\n",
      "Batch 3, G1 Loss: 1.6272, G2 Loss: 0.7198, D1 Loss: 0.7252, D2 Loss: 1.2460\n",
      "Batch 4, G1 Loss: 1.6435, G2 Loss: 4.5464, D1 Loss: 0.8195, D2 Loss: 0.6102\n",
      "Batch 5, G1 Loss: 2.1699, G2 Loss: 1.0066, D1 Loss: 0.6628, D2 Loss: 1.1074\n",
      "Batch 6, G1 Loss: 1.8035, G2 Loss: 4.4633, D1 Loss: 1.0076, D2 Loss: 0.5811\n",
      "Batch 7, G1 Loss: 1.4849, G2 Loss: 0.9140, D1 Loss: 0.9777, D2 Loss: 0.8813\n",
      "Batch 8, G1 Loss: 1.7964, G2 Loss: 4.5828, D1 Loss: 0.9211, D2 Loss: 0.6683\n",
      "Batch 9, G1 Loss: 1.1096, G2 Loss: 0.8857, D1 Loss: 1.2968, D2 Loss: 1.1513\n",
      "Batch 10, G1 Loss: 3.2426, G2 Loss: 5.3692, D1 Loss: 1.1167, D2 Loss: 0.3936\n",
      "Batch 11, G1 Loss: 0.7215, G2 Loss: 1.7785, D1 Loss: 1.9430, D2 Loss: 0.3303\n",
      "Batch 12, G1 Loss: 3.8969, G2 Loss: 1.4165, D1 Loss: 1.3384, D2 Loss: 1.0684\n",
      "Batch 13, G1 Loss: 0.6353, G2 Loss: 3.3823, D1 Loss: 2.3963, D2 Loss: 0.3513\n",
      "Batch 14, G1 Loss: 3.0936, G2 Loss: 1.4986, D1 Loss: 1.7296, D2 Loss: 0.9335\n",
      "Batch 15, G1 Loss: 0.8284, G2 Loss: 4.1293, D1 Loss: 1.8798, D2 Loss: 0.1325\n",
      "Batch 16, G1 Loss: 2.2823, G2 Loss: 1.5487, D1 Loss: 1.1373, D2 Loss: 0.6697\n",
      "Batch 17, G1 Loss: 0.7376, G2 Loss: 3.1243, D1 Loss: 1.9674, D2 Loss: 0.3044\n",
      "Batch 18, G1 Loss: 2.6412, G2 Loss: 1.0757, D1 Loss: 1.6158, D2 Loss: 1.4351\n",
      "Batch 19, G1 Loss: 1.1381, G2 Loss: 6.8214, D1 Loss: 1.2583, D2 Loss: 0.5817\n",
      "Batch 20, G1 Loss: 1.4755, G2 Loss: 3.2480, D1 Loss: 1.1045, D2 Loss: 0.3073\n",
      "Batch 21, G1 Loss: 1.6835, G2 Loss: 0.4288, D1 Loss: 1.0190, D2 Loss: 1.6810\n",
      "Batch 22, G1 Loss: 1.6800, G2 Loss: 11.6926, D1 Loss: 1.0141, D2 Loss: 1.4429\n",
      "Batch 23, G1 Loss: 1.1745, G2 Loss: 9.7866, D1 Loss: 1.3398, D2 Loss: 0.3298\n",
      "Batch 24, G1 Loss: 2.0432, G2 Loss: 3.9787, D1 Loss: 1.0710, D2 Loss: 0.4556\n",
      "Batch 25, G1 Loss: 1.5718, G2 Loss: 0.2759, D1 Loss: 0.9736, D2 Loss: 2.4675\n",
      "Batch 26, G1 Loss: 1.3417, G2 Loss: 10.9208, D1 Loss: 1.1075, D2 Loss: 0.4568\n",
      "Batch 27, G1 Loss: 1.9761, G2 Loss: 11.1381, D1 Loss: 1.2001, D2 Loss: 0.7129\n",
      "Batch 28, G1 Loss: 0.9429, G2 Loss: 6.1995, D1 Loss: 1.8454, D2 Loss: 0.9273\n",
      "Batch 29, G1 Loss: 2.3311, G2 Loss: 0.6344, D1 Loss: 1.6056, D2 Loss: 1.1543\n",
      "Batch 30, G1 Loss: 0.6709, G2 Loss: 7.2549, D1 Loss: 2.2376, D2 Loss: 0.2256\n",
      "Batch 31, G1 Loss: 3.4743, G2 Loss: 6.5951, D1 Loss: 1.4514, D2 Loss: 0.1666\n",
      "Batch 32, G1 Loss: 0.7676, G2 Loss: 2.7584, D1 Loss: 1.9548, D2 Loss: 0.3528\n",
      "Epoch 2 Average -> G1 Loss: 1.743057131767273, G2 Loss: 4.274966716766357, D1 Loss: 1.3610103130340576, D2 Loss: 0.7391600608825684\n",
      "Epoch 3/100\n",
      "Batch 1, G1 Loss: 1.8646, G2 Loss: 1.3555, D1 Loss: 1.7361, D2 Loss: 0.5829\n",
      "Batch 2, G1 Loss: 1.0311, G2 Loss: 5.4610, D1 Loss: 1.5274, D2 Loss: 0.5218\n",
      "Batch 3, G1 Loss: 1.4934, G2 Loss: 3.4946, D1 Loss: 1.2677, D2 Loss: 0.4515\n",
      "Batch 4, G1 Loss: 1.4048, G2 Loss: 1.1307, D1 Loss: 1.3178, D2 Loss: 1.1362\n",
      "Batch 5, G1 Loss: 1.2319, G2 Loss: 6.0318, D1 Loss: 1.5587, D2 Loss: 0.5934\n",
      "Batch 6, G1 Loss: 2.1107, G2 Loss: 3.6615, D1 Loss: 1.2420, D2 Loss: 0.4810\n",
      "Batch 7, G1 Loss: 0.9758, G2 Loss: 1.0766, D1 Loss: 1.3555, D2 Loss: 1.2098\n",
      "Batch 8, G1 Loss: 2.2863, G2 Loss: 7.4128, D1 Loss: 0.7866, D2 Loss: 0.4055\n",
      "Batch 9, G1 Loss: 1.2240, G2 Loss: 6.5665, D1 Loss: 1.6289, D2 Loss: 1.1149\n",
      "Batch 10, G1 Loss: 1.1479, G2 Loss: 1.5435, D1 Loss: 1.5802, D2 Loss: 0.4334\n",
      "Batch 11, G1 Loss: 1.9371, G2 Loss: 4.1397, D1 Loss: 1.8994, D2 Loss: 0.2538\n",
      "Batch 12, G1 Loss: 0.6506, G2 Loss: 2.9999, D1 Loss: 2.3344, D2 Loss: 0.1166\n",
      "Batch 13, G1 Loss: 1.9872, G2 Loss: 2.3654, D1 Loss: 1.8997, D2 Loss: 0.4877\n",
      "Batch 14, G1 Loss: 0.9600, G2 Loss: 2.5374, D1 Loss: 1.8789, D2 Loss: 0.4927\n",
      "Batch 15, G1 Loss: 1.1793, G2 Loss: 2.9654, D1 Loss: 1.3740, D2 Loss: 0.2374\n",
      "Batch 16, G1 Loss: 1.7311, G2 Loss: 2.9890, D1 Loss: 1.4557, D2 Loss: 0.7594\n",
      "Batch 17, G1 Loss: 0.8073, G2 Loss: 1.2745, D1 Loss: 1.6959, D2 Loss: 1.0963\n",
      "Batch 18, G1 Loss: 1.5643, G2 Loss: 6.4482, D1 Loss: 1.5111, D2 Loss: 0.8289\n",
      "Batch 19, G1 Loss: 0.9752, G2 Loss: 2.3801, D1 Loss: 1.5774, D2 Loss: 0.3256\n",
      "Batch 20, G1 Loss: 1.7099, G2 Loss: 1.9807, D1 Loss: 1.3878, D2 Loss: 0.9812\n",
      "Batch 21, G1 Loss: 0.8378, G2 Loss: 5.3156, D1 Loss: 1.5904, D2 Loss: 0.9472\n",
      "Batch 22, G1 Loss: 1.6794, G2 Loss: 2.6127, D1 Loss: 1.6676, D2 Loss: 0.5371\n",
      "Batch 23, G1 Loss: 0.8649, G2 Loss: 2.7045, D1 Loss: 1.6818, D2 Loss: 0.3705\n",
      "Batch 24, G1 Loss: 1.7474, G2 Loss: 2.8349, D1 Loss: 1.4244, D2 Loss: 0.3142\n",
      "Batch 25, G1 Loss: 0.7543, G2 Loss: 2.7671, D1 Loss: 1.7329, D2 Loss: 0.5919\n",
      "Batch 26, G1 Loss: 1.7344, G2 Loss: 1.6423, D1 Loss: 1.7725, D2 Loss: 0.8795\n",
      "Batch 27, G1 Loss: 0.8348, G2 Loss: 5.7898, D1 Loss: 1.6078, D2 Loss: 0.6941\n",
      "Batch 28, G1 Loss: 1.3763, G2 Loss: 2.7523, D1 Loss: 1.4586, D2 Loss: 0.4307\n",
      "Batch 29, G1 Loss: 1.0408, G2 Loss: 0.9752, D1 Loss: 1.3945, D2 Loss: 0.9593\n",
      "Batch 30, G1 Loss: 1.3070, G2 Loss: 10.0098, D1 Loss: 1.4226, D2 Loss: 1.0329\n",
      "Batch 31, G1 Loss: 1.0468, G2 Loss: 7.6128, D1 Loss: 1.5074, D2 Loss: 0.6601\n",
      "Batch 32, G1 Loss: 1.2901, G2 Loss: 1.4428, D1 Loss: 1.3646, D2 Loss: 1.1175\n",
      "Epoch 3 Average -> G1 Loss: 1.3370784521102905, G2 Loss: 3.5710768699645996, D1 Loss: 1.5512580871582031, D2 Loss: 0.6576544046401978\n",
      "Epoch 4/100\n",
      "Batch 1, G1 Loss: 1.0799, G2 Loss: 1.9507, D1 Loss: 1.5041, D2 Loss: 0.4409\n",
      "Batch 2, G1 Loss: 1.2693, G2 Loss: 4.4206, D1 Loss: 1.4886, D2 Loss: 0.4232\n",
      "Batch 3, G1 Loss: 0.9145, G2 Loss: 2.0872, D1 Loss: 1.5262, D2 Loss: 0.7535\n",
      "Batch 4, G1 Loss: 1.5289, G2 Loss: 3.1687, D1 Loss: 1.7807, D2 Loss: 0.6637\n",
      "Batch 5, G1 Loss: 0.7542, G2 Loss: 1.0457, D1 Loss: 1.9940, D2 Loss: 0.9539\n",
      "Batch 6, G1 Loss: 1.5915, G2 Loss: 8.3990, D1 Loss: 1.5007, D2 Loss: 1.8358\n",
      "Batch 7, G1 Loss: 0.8591, G2 Loss: 5.6225, D1 Loss: 1.8257, D2 Loss: 0.7728\n",
      "Batch 8, G1 Loss: 1.1122, G2 Loss: 1.0885, D1 Loss: 1.6455, D2 Loss: 0.9020\n",
      "Batch 9, G1 Loss: 1.0830, G2 Loss: 6.1272, D1 Loss: 1.5977, D2 Loss: 0.4511\n",
      "Batch 10, G1 Loss: 0.9482, G2 Loss: 3.9058, D1 Loss: 2.1245, D2 Loss: 0.3237\n",
      "Batch 11, G1 Loss: 0.8949, G2 Loss: 1.3539, D1 Loss: 1.9631, D2 Loss: 0.8990\n",
      "Batch 12, G1 Loss: 1.0727, G2 Loss: 6.9938, D1 Loss: 1.7132, D2 Loss: 0.6153\n",
      "Batch 13, G1 Loss: 0.9516, G2 Loss: 5.1373, D1 Loss: 1.5802, D2 Loss: 1.0997\n",
      "Batch 14, G1 Loss: 1.0681, G2 Loss: 0.6864, D1 Loss: 1.4956, D2 Loss: 1.8464\n",
      "Batch 15, G1 Loss: 1.0029, G2 Loss: 8.8061, D1 Loss: 1.5658, D2 Loss: 0.7784\n",
      "Batch 16, G1 Loss: 1.1838, G2 Loss: 7.3726, D1 Loss: 1.4123, D2 Loss: 0.7507\n",
      "Batch 17, G1 Loss: 0.9288, G2 Loss: 0.9464, D1 Loss: 1.4164, D2 Loss: 1.0102\n",
      "Batch 18, G1 Loss: 1.4848, G2 Loss: 6.9747, D1 Loss: 1.6579, D2 Loss: 1.7409\n",
      "Batch 19, G1 Loss: 0.7093, G2 Loss: 2.8234, D1 Loss: 1.8195, D2 Loss: 0.5989\n",
      "Batch 20, G1 Loss: 1.9762, G2 Loss: 0.8948, D1 Loss: 1.8241, D2 Loss: 1.2456\n",
      "Batch 21, G1 Loss: 0.7495, G2 Loss: 9.6555, D1 Loss: 1.9112, D2 Loss: 0.8950\n",
      "Batch 22, G1 Loss: 1.3490, G2 Loss: 6.0321, D1 Loss: 1.2900, D2 Loss: 0.3064\n",
      "Batch 23, G1 Loss: 0.9988, G2 Loss: 1.7223, D1 Loss: 1.6334, D2 Loss: 0.5936\n",
      "Batch 24, G1 Loss: 0.8509, G2 Loss: 3.2666, D1 Loss: 1.8345, D2 Loss: 0.7355\n",
      "Batch 25, G1 Loss: 1.1206, G2 Loss: 2.2903, D1 Loss: 1.7676, D2 Loss: 0.5308\n",
      "Batch 26, G1 Loss: 0.8014, G2 Loss: 3.1693, D1 Loss: 1.9640, D2 Loss: 0.5655\n",
      "Batch 27, G1 Loss: 1.1451, G2 Loss: 1.9981, D1 Loss: 1.6143, D2 Loss: 0.6876\n",
      "Batch 28, G1 Loss: 0.7123, G2 Loss: 3.7459, D1 Loss: 1.6944, D2 Loss: 0.8204\n",
      "Batch 29, G1 Loss: 1.2348, G2 Loss: 1.8860, D1 Loss: 1.6625, D2 Loss: 1.0941\n",
      "Batch 30, G1 Loss: 0.6477, G2 Loss: 5.3231, D1 Loss: 1.8396, D2 Loss: 0.5261\n",
      "Batch 31, G1 Loss: 1.3916, G2 Loss: 3.7534, D1 Loss: 1.6348, D2 Loss: 0.6597\n",
      "Batch 32, G1 Loss: 0.6463, G2 Loss: 0.8140, D1 Loss: 1.8307, D2 Loss: 1.2108\n",
      "Epoch 4 Average -> G1 Loss: 1.064435362815857, G2 Loss: 3.8581857681274414, D1 Loss: 1.6910252571105957, D2 Loss: 0.8353560566902161\n",
      "Epoch 5/100\n",
      "Batch 1, G1 Loss: 1.3199, G2 Loss: 7.4710, D1 Loss: 1.6134, D2 Loss: 1.3084\n",
      "Batch 2, G1 Loss: 0.7082, G2 Loss: 4.9611, D1 Loss: 1.7753, D2 Loss: 1.0592\n",
      "Batch 3, G1 Loss: 1.3019, G2 Loss: 0.7830, D1 Loss: 1.5915, D2 Loss: 1.1999\n",
      "Batch 4, G1 Loss: 0.8043, G2 Loss: 7.3930, D1 Loss: 1.6306, D2 Loss: 0.8760\n",
      "Batch 5, G1 Loss: 1.3739, G2 Loss: 6.2674, D1 Loss: 1.5144, D2 Loss: 1.3374\n",
      "Batch 6, G1 Loss: 0.7281, G2 Loss: 2.3255, D1 Loss: 1.6290, D2 Loss: 0.6535\n",
      "Batch 7, G1 Loss: 1.4598, G2 Loss: 1.6671, D1 Loss: 1.6821, D2 Loss: 0.7119\n",
      "Batch 8, G1 Loss: 0.7621, G2 Loss: 3.6564, D1 Loss: 1.6850, D2 Loss: 0.2200\n",
      "Batch 9, G1 Loss: 1.4842, G2 Loss: 2.8180, D1 Loss: 1.5883, D2 Loss: 0.4811\n",
      "Batch 10, G1 Loss: 0.7403, G2 Loss: 1.1510, D1 Loss: 1.6860, D2 Loss: 0.9012\n",
      "Batch 11, G1 Loss: 1.4235, G2 Loss: 4.9075, D1 Loss: 1.5183, D2 Loss: 0.2395\n",
      "Batch 12, G1 Loss: 0.8407, G2 Loss: 4.6153, D1 Loss: 1.5421, D2 Loss: 0.6315\n",
      "Batch 13, G1 Loss: 1.3624, G2 Loss: 1.5350, D1 Loss: 1.4282, D2 Loss: 0.8943\n",
      "Batch 14, G1 Loss: 0.8925, G2 Loss: 3.3553, D1 Loss: 1.4963, D2 Loss: 0.4279\n",
      "Batch 15, G1 Loss: 1.4136, G2 Loss: 2.8064, D1 Loss: 1.3815, D2 Loss: 0.5681\n",
      "Batch 16, G1 Loss: 0.8774, G2 Loss: 1.2706, D1 Loss: 1.4197, D2 Loss: 0.9226\n",
      "Batch 17, G1 Loss: 1.5539, G2 Loss: 6.2784, D1 Loss: 1.4282, D2 Loss: 0.1994\n",
      "Batch 18, G1 Loss: 0.7988, G2 Loss: 6.1607, D1 Loss: 1.5608, D2 Loss: 0.1288\n",
      "Batch 19, G1 Loss: 1.5492, G2 Loss: 2.8522, D1 Loss: 1.4632, D2 Loss: 0.1888\n",
      "Batch 20, G1 Loss: 0.8389, G2 Loss: 1.8229, D1 Loss: 1.5448, D2 Loss: 1.0750\n",
      "Batch 21, G1 Loss: 1.3649, G2 Loss: 3.5928, D1 Loss: 1.4476, D2 Loss: 0.7577\n",
      "Batch 22, G1 Loss: 0.9512, G2 Loss: 1.9490, D1 Loss: 1.5933, D2 Loss: 0.8637\n",
      "Batch 23, G1 Loss: 1.2266, G2 Loss: 2.2338, D1 Loss: 1.4108, D2 Loss: 0.6355\n",
      "Batch 24, G1 Loss: 1.0418, G2 Loss: 2.9973, D1 Loss: 1.4561, D2 Loss: 0.7713\n",
      "Batch 25, G1 Loss: 0.9505, G2 Loss: 0.7692, D1 Loss: 1.4210, D2 Loss: 1.2163\n",
      "Batch 26, G1 Loss: 1.2115, G2 Loss: 7.8532, D1 Loss: 1.5497, D2 Loss: 0.8317\n",
      "Batch 27, G1 Loss: 0.7640, G2 Loss: 5.9948, D1 Loss: 1.5386, D2 Loss: 1.2602\n",
      "Batch 28, G1 Loss: 1.4387, G2 Loss: 1.4830, D1 Loss: 1.5567, D2 Loss: 0.6715\n",
      "Batch 29, G1 Loss: 0.7280, G2 Loss: 2.9276, D1 Loss: 1.6901, D2 Loss: 0.2181\n",
      "Batch 30, G1 Loss: 1.3289, G2 Loss: 4.5914, D1 Loss: 1.7356, D2 Loss: 0.3938\n",
      "Batch 31, G1 Loss: 0.7613, G2 Loss: 3.1619, D1 Loss: 1.6804, D2 Loss: 0.2581\n",
      "Batch 32, G1 Loss: 1.3562, G2 Loss: 1.9238, D1 Loss: 1.5329, D2 Loss: 0.4792\n",
      "Epoch 5 Average -> G1 Loss: 1.1049107313156128, G2 Loss: 3.549241781234741, D1 Loss: 1.5559831857681274, D2 Loss: 0.6994237899780273\n",
      "Epoch 6/100\n",
      "Batch 1, G1 Loss: 0.8062, G2 Loss: 4.9949, D1 Loss: 1.6964, D2 Loss: 0.5273\n",
      "Batch 2, G1 Loss: 1.3569, G2 Loss: 3.4367, D1 Loss: 1.5805, D2 Loss: 0.5582\n",
      "Batch 3, G1 Loss: 0.8413, G2 Loss: 1.2249, D1 Loss: 1.5145, D2 Loss: 1.0602\n",
      "Batch 4, G1 Loss: 1.5128, G2 Loss: 6.5559, D1 Loss: 1.4289, D2 Loss: 0.1797\n",
      "Batch 5, G1 Loss: 0.8650, G2 Loss: 6.4507, D1 Loss: 1.4039, D2 Loss: 0.6846\n",
      "Batch 6, G1 Loss: 1.4495, G2 Loss: 2.9127, D1 Loss: 1.4248, D2 Loss: 0.1757\n",
      "Batch 7, G1 Loss: 0.7988, G2 Loss: 1.9631, D1 Loss: 1.5368, D2 Loss: 0.4968\n",
      "Batch 8, G1 Loss: 1.4575, G2 Loss: 3.5762, D1 Loss: 1.5229, D2 Loss: 0.3518\n",
      "Batch 9, G1 Loss: 0.7865, G2 Loss: 2.1582, D1 Loss: 1.7170, D2 Loss: 0.3114\n",
      "Batch 10, G1 Loss: 0.9974, G2 Loss: 1.9455, D1 Loss: 1.6705, D2 Loss: 0.7923\n",
      "Batch 11, G1 Loss: 0.9285, G2 Loss: 2.4964, D1 Loss: 1.7056, D2 Loss: 0.6720\n",
      "Batch 12, G1 Loss: 1.0583, G2 Loss: 1.9004, D1 Loss: 1.4588, D2 Loss: 0.6448\n",
      "Batch 13, G1 Loss: 0.9586, G2 Loss: 2.4062, D1 Loss: 1.4219, D2 Loss: 0.6279\n",
      "Batch 14, G1 Loss: 1.4057, G2 Loss: 2.0224, D1 Loss: 1.2798, D2 Loss: 0.4437\n",
      "Batch 15, G1 Loss: 0.8452, G2 Loss: 2.8985, D1 Loss: 1.3358, D2 Loss: 0.4693\n",
      "Batch 16, G1 Loss: 1.6748, G2 Loss: 1.4774, D1 Loss: 1.2396, D2 Loss: 0.3993\n",
      "Batch 17, G1 Loss: 0.6544, G2 Loss: 5.6585, D1 Loss: 1.9606, D2 Loss: 0.6169\n",
      "Batch 18, G1 Loss: 1.6731, G2 Loss: 3.8752, D1 Loss: 2.0217, D2 Loss: 0.4689\n",
      "Batch 19, G1 Loss: 0.6116, G2 Loss: 1.2867, D1 Loss: 1.9540, D2 Loss: 0.9178\n",
      "Batch 20, G1 Loss: 1.3697, G2 Loss: 4.1674, D1 Loss: 1.8341, D2 Loss: 0.5745\n",
      "Batch 21, G1 Loss: 0.8198, G2 Loss: 3.4572, D1 Loss: 1.6608, D2 Loss: 0.5900\n",
      "Batch 22, G1 Loss: 1.0811, G2 Loss: 1.5646, D1 Loss: 1.5286, D2 Loss: 0.4927\n",
      "Batch 23, G1 Loss: 1.0819, G2 Loss: 5.4705, D1 Loss: 1.5230, D2 Loss: 0.3725\n",
      "Batch 24, G1 Loss: 0.9195, G2 Loss: 4.4323, D1 Loss: 1.5478, D2 Loss: 0.1232\n",
      "Batch 25, G1 Loss: 1.0182, G2 Loss: 2.4540, D1 Loss: 1.5651, D2 Loss: 0.6314\n",
      "Batch 26, G1 Loss: 0.9988, G2 Loss: 1.6495, D1 Loss: 1.4473, D2 Loss: 0.4603\n",
      "Batch 27, G1 Loss: 1.1196, G2 Loss: 4.8126, D1 Loss: 1.3196, D2 Loss: 0.3055\n",
      "Batch 28, G1 Loss: 1.1004, G2 Loss: 3.4590, D1 Loss: 1.2793, D2 Loss: 0.6816\n",
      "Batch 29, G1 Loss: 0.9861, G2 Loss: 1.0274, D1 Loss: 1.3097, D2 Loss: 1.4959\n",
      "Batch 30, G1 Loss: 1.0147, G2 Loss: 4.9970, D1 Loss: 1.3839, D2 Loss: 0.5551\n",
      "Batch 31, G1 Loss: 1.1261, G2 Loss: 3.2514, D1 Loss: 1.4917, D2 Loss: 0.5033\n",
      "Batch 32, G1 Loss: 0.7300, G2 Loss: 2.1827, D1 Loss: 1.6895, D2 Loss: 1.1234\n",
      "Epoch 6 Average -> G1 Loss: 1.0640013217926025, G2 Loss: 3.192701816558838, D1 Loss: 1.5454541444778442, D2 Loss: 0.5721266269683838\n",
      "Epoch 7/100\n",
      "Batch 1, G1 Loss: 1.2133, G2 Loss: 2.3637, D1 Loss: 1.4772, D2 Loss: 0.7748\n",
      "Batch 2, G1 Loss: 0.8900, G2 Loss: 3.5049, D1 Loss: 1.4423, D2 Loss: 0.7127\n",
      "Batch 3, G1 Loss: 1.1588, G2 Loss: 2.0127, D1 Loss: 1.4279, D2 Loss: 0.4026\n",
      "Batch 4, G1 Loss: 1.0152, G2 Loss: 4.6723, D1 Loss: 1.4226, D2 Loss: 0.6227\n",
      "Batch 5, G1 Loss: 0.9671, G2 Loss: 2.0874, D1 Loss: 1.5717, D2 Loss: 0.4993\n",
      "Batch 6, G1 Loss: 1.0503, G2 Loss: 1.2008, D1 Loss: 1.5329, D2 Loss: 0.7646\n",
      "Batch 7, G1 Loss: 0.9383, G2 Loss: 5.1892, D1 Loss: 1.6184, D2 Loss: 0.9522\n",
      "Batch 8, G1 Loss: 1.0057, G2 Loss: 2.4678, D1 Loss: 1.6555, D2 Loss: 0.4153\n",
      "Batch 9, G1 Loss: 0.9647, G2 Loss: 1.1513, D1 Loss: 1.6413, D2 Loss: 0.6320\n",
      "Batch 10, G1 Loss: 1.0052, G2 Loss: 6.1030, D1 Loss: 1.4899, D2 Loss: 0.5126\n",
      "Batch 11, G1 Loss: 1.1930, G2 Loss: 5.0995, D1 Loss: 1.4234, D2 Loss: 0.8587\n",
      "Batch 12, G1 Loss: 0.8744, G2 Loss: 1.6100, D1 Loss: 1.5099, D2 Loss: 0.4782\n",
      "Batch 13, G1 Loss: 1.2772, G2 Loss: 2.7399, D1 Loss: 1.4303, D2 Loss: 0.2254\n",
      "Batch 14, G1 Loss: 0.7895, G2 Loss: 4.0187, D1 Loss: 1.6740, D2 Loss: 0.3828\n",
      "Batch 15, G1 Loss: 1.4382, G2 Loss: 3.5510, D1 Loss: 1.7062, D2 Loss: 0.1949\n",
      "Batch 16, G1 Loss: 0.6962, G2 Loss: 2.7511, D1 Loss: 1.8661, D2 Loss: 0.3432\n",
      "Batch 17, G1 Loss: 1.6756, G2 Loss: 2.8342, D1 Loss: 1.6915, D2 Loss: 0.2348\n",
      "Batch 18, G1 Loss: 0.7473, G2 Loss: 2.9239, D1 Loss: 1.6507, D2 Loss: 0.4906\n",
      "Batch 19, G1 Loss: 1.6227, G2 Loss: 2.2989, D1 Loss: 1.5696, D2 Loss: 0.5462\n",
      "Batch 20, G1 Loss: 0.7443, G2 Loss: 2.7996, D1 Loss: 1.6637, D2 Loss: 0.3913\n",
      "Batch 21, G1 Loss: 1.6326, G2 Loss: 2.8874, D1 Loss: 1.5477, D2 Loss: 0.4301\n",
      "Batch 22, G1 Loss: 0.8218, G2 Loss: 3.1101, D1 Loss: 1.5561, D2 Loss: 0.2739\n",
      "Batch 23, G1 Loss: 1.3439, G2 Loss: 2.0456, D1 Loss: 1.3914, D2 Loss: 1.3496\n",
      "Batch 24, G1 Loss: 1.0506, G2 Loss: 0.2537, D1 Loss: 1.3995, D2 Loss: 2.7425\n",
      "Batch 25, G1 Loss: 0.9787, G2 Loss: 17.2214, D1 Loss: 1.3837, D2 Loss: 5.8906\n",
      "Batch 26, G1 Loss: 1.3103, G2 Loss: 5.2452, D1 Loss: 1.3951, D2 Loss: 0.9957\n",
      "Batch 27, G1 Loss: 1.0443, G2 Loss: 0.1058, D1 Loss: 1.3882, D2 Loss: 5.1123\n",
      "Batch 28, G1 Loss: 1.1919, G2 Loss: 10.6591, D1 Loss: 1.4778, D2 Loss: 0.1215\n",
      "Batch 29, G1 Loss: 1.1192, G2 Loss: 13.8954, D1 Loss: 1.5026, D2 Loss: 0.4261\n",
      "Batch 30, G1 Loss: 1.1636, G2 Loss: 13.1558, D1 Loss: 1.4717, D2 Loss: 1.0087\n",
      "Batch 31, G1 Loss: 1.1350, G2 Loss: 9.8151, D1 Loss: 1.7308, D2 Loss: 0.4194\n",
      "Batch 32, G1 Loss: 0.9580, G2 Loss: 6.3842, D1 Loss: 1.8411, D2 Loss: 0.0601\n",
      "Epoch 7 Average -> G1 Loss: 1.094278335571289, G2 Loss: 4.567468166351318, D1 Loss: 1.5484572649002075, D2 Loss: 0.9145416617393494\n",
      "Epoch 8/100\n",
      "Batch 1, G1 Loss: 1.5063, G2 Loss: 3.9210, D1 Loss: 1.4130, D2 Loss: 0.1072\n",
      "Batch 2, G1 Loss: 0.9668, G2 Loss: 2.3564, D1 Loss: 1.5769, D2 Loss: 0.1226\n",
      "Batch 3, G1 Loss: 1.5990, G2 Loss: 3.3040, D1 Loss: 1.4257, D2 Loss: 0.0526\n",
      "Batch 4, G1 Loss: 0.8497, G2 Loss: 1.8384, D1 Loss: 1.6966, D2 Loss: 0.3143\n",
      "Batch 5, G1 Loss: 1.8658, G2 Loss: 3.9564, D1 Loss: 1.8064, D2 Loss: 0.0393\n",
      "Batch 6, G1 Loss: 0.6982, G2 Loss: 3.8594, D1 Loss: 2.1131, D2 Loss: 0.0300\n",
      "Batch 7, G1 Loss: 1.6222, G2 Loss: 2.8112, D1 Loss: 1.7453, D2 Loss: 0.1315\n",
      "Batch 8, G1 Loss: 0.9164, G2 Loss: 2.9284, D1 Loss: 1.6919, D2 Loss: 0.3566\n",
      "Batch 9, G1 Loss: 1.0610, G2 Loss: 1.6921, D1 Loss: 1.5057, D2 Loss: 0.3308\n",
      "Batch 10, G1 Loss: 1.2236, G2 Loss: 3.9698, D1 Loss: 1.3233, D2 Loss: 0.0540\n",
      "Batch 11, G1 Loss: 1.0661, G2 Loss: 2.1776, D1 Loss: 1.3433, D2 Loss: 0.4822\n",
      "Batch 12, G1 Loss: 1.2038, G2 Loss: 3.6576, D1 Loss: 1.2190, D2 Loss: 0.5146\n",
      "Batch 13, G1 Loss: 1.2125, G2 Loss: 1.4298, D1 Loss: 1.2521, D2 Loss: 0.8166\n",
      "Batch 14, G1 Loss: 1.1165, G2 Loss: 6.0628, D1 Loss: 1.2225, D2 Loss: 0.3400\n",
      "Batch 15, G1 Loss: 1.3081, G2 Loss: 3.4716, D1 Loss: 1.3112, D2 Loss: 0.3022\n",
      "Batch 16, G1 Loss: 0.8605, G2 Loss: 1.0331, D1 Loss: 1.4880, D2 Loss: 0.8018\n",
      "Batch 17, G1 Loss: 1.4958, G2 Loss: 10.4373, D1 Loss: 1.7963, D2 Loss: 0.9406\n",
      "Batch 18, G1 Loss: 0.5574, G2 Loss: 8.5281, D1 Loss: 2.3869, D2 Loss: 0.3801\n",
      "Batch 19, G1 Loss: 1.5651, G2 Loss: 2.9589, D1 Loss: 1.9432, D2 Loss: 0.6151\n",
      "Batch 20, G1 Loss: 0.7758, G2 Loss: 1.2580, D1 Loss: 1.5868, D2 Loss: 1.2082\n",
      "Batch 21, G1 Loss: 1.6624, G2 Loss: 7.3163, D1 Loss: 1.0741, D2 Loss: 0.6444\n",
      "Batch 22, G1 Loss: 0.8173, G2 Loss: 5.4348, D1 Loss: 1.3361, D2 Loss: 0.9626\n",
      "Batch 23, G1 Loss: 1.5861, G2 Loss: 1.4231, D1 Loss: 1.2988, D2 Loss: 0.8371\n",
      "Batch 24, G1 Loss: 0.9000, G2 Loss: 2.4027, D1 Loss: 1.4715, D2 Loss: 0.2278\n",
      "Batch 25, G1 Loss: 1.1846, G2 Loss: 3.7407, D1 Loss: 1.6094, D2 Loss: 0.2593\n",
      "Batch 26, G1 Loss: 0.8431, G2 Loss: 2.4908, D1 Loss: 1.6382, D2 Loss: 0.3538\n",
      "Batch 27, G1 Loss: 1.1943, G2 Loss: 2.2058, D1 Loss: 1.6886, D2 Loss: 0.6013\n",
      "Batch 28, G1 Loss: 0.6896, G2 Loss: 2.8174, D1 Loss: 1.7264, D2 Loss: 0.5123\n",
      "Batch 29, G1 Loss: 1.2596, G2 Loss: 1.6882, D1 Loss: 1.6943, D2 Loss: 0.6590\n",
      "Batch 30, G1 Loss: 0.6973, G2 Loss: 3.5717, D1 Loss: 1.5822, D2 Loss: 0.7255\n",
      "Batch 31, G1 Loss: 1.2699, G2 Loss: 1.0968, D1 Loss: 1.4355, D2 Loss: 0.8015\n",
      "Batch 32, G1 Loss: 0.8448, G2 Loss: 4.9894, D1 Loss: 1.5329, D2 Loss: 1.1805\n",
      "Epoch 8 Average -> G1 Loss: 1.138104796409607, G2 Loss: 3.46342134475708, D1 Loss: 1.5604841709136963, D2 Loss: 0.4907829165458679\n",
      "Epoch 9/100\n",
      "Batch 1, G1 Loss: 1.0335, G2 Loss: 1.4309, D1 Loss: 1.4148, D2 Loss: 1.1874\n",
      "Batch 2, G1 Loss: 1.1179, G2 Loss: 3.2936, D1 Loss: 1.3237, D2 Loss: 0.4042\n",
      "Batch 3, G1 Loss: 1.0749, G2 Loss: 2.2854, D1 Loss: 1.3670, D2 Loss: 0.4225\n",
      "Batch 4, G1 Loss: 1.1682, G2 Loss: 2.5430, D1 Loss: 1.3729, D2 Loss: 0.2594\n",
      "Batch 5, G1 Loss: 1.0598, G2 Loss: 2.7093, D1 Loss: 1.4443, D2 Loss: 0.5220\n",
      "Batch 6, G1 Loss: 1.0911, G2 Loss: 1.9803, D1 Loss: 1.3331, D2 Loss: 0.7420\n",
      "Batch 7, G1 Loss: 1.2332, G2 Loss: 2.6588, D1 Loss: 1.4104, D2 Loss: 0.2326\n",
      "Batch 8, G1 Loss: 0.9905, G2 Loss: 2.5620, D1 Loss: 1.4295, D2 Loss: 0.2889\n",
      "Batch 9, G1 Loss: 1.2658, G2 Loss: 2.5982, D1 Loss: 1.5251, D2 Loss: 0.2815\n",
      "Batch 10, G1 Loss: 0.9991, G2 Loss: 2.9610, D1 Loss: 1.5334, D2 Loss: 0.2553\n",
      "Batch 11, G1 Loss: 1.3264, G2 Loss: 2.9878, D1 Loss: 1.3662, D2 Loss: 0.6669\n",
      "Batch 12, G1 Loss: 1.0258, G2 Loss: 1.9541, D1 Loss: 1.3096, D2 Loss: 0.3626\n",
      "Batch 13, G1 Loss: 1.2996, G2 Loss: 4.3851, D1 Loss: 1.3084, D2 Loss: 0.5306\n",
      "Batch 14, G1 Loss: 0.8693, G2 Loss: 3.2114, D1 Loss: 1.4953, D2 Loss: 0.2755\n",
      "Batch 15, G1 Loss: 1.3034, G2 Loss: 1.3707, D1 Loss: 1.4821, D2 Loss: 0.7600\n",
      "Batch 16, G1 Loss: 0.7696, G2 Loss: 4.6783, D1 Loss: 1.6187, D2 Loss: 0.3515\n",
      "Batch 17, G1 Loss: 1.3895, G2 Loss: 3.4024, D1 Loss: 1.6545, D2 Loss: 0.1578\n",
      "Batch 18, G1 Loss: 0.7256, G2 Loss: 1.8796, D1 Loss: 1.7281, D2 Loss: 0.8154\n",
      "Batch 19, G1 Loss: 1.2853, G2 Loss: 2.3396, D1 Loss: 1.6001, D2 Loss: 0.8060\n",
      "Batch 20, G1 Loss: 0.7876, G2 Loss: 1.5447, D1 Loss: 1.5824, D2 Loss: 0.7539\n",
      "Batch 21, G1 Loss: 1.2362, G2 Loss: 2.9144, D1 Loss: 1.4929, D2 Loss: 0.4166\n",
      "Batch 22, G1 Loss: 0.9093, G2 Loss: 1.5924, D1 Loss: 1.4446, D2 Loss: 0.7878\n",
      "Batch 23, G1 Loss: 1.1008, G2 Loss: 4.1712, D1 Loss: 1.4815, D2 Loss: 1.8454\n",
      "Batch 24, G1 Loss: 0.8863, G2 Loss: 0.4987, D1 Loss: 1.4150, D2 Loss: 2.0547\n",
      "Batch 25, G1 Loss: 1.2357, G2 Loss: 11.4802, D1 Loss: 1.4737, D2 Loss: 1.3491\n",
      "Batch 26, G1 Loss: 0.8998, G2 Loss: 8.6488, D1 Loss: 1.3981, D2 Loss: 0.8417\n",
      "Batch 27, G1 Loss: 1.1169, G2 Loss: 2.2943, D1 Loss: 1.2909, D2 Loss: 0.4996\n",
      "Batch 28, G1 Loss: 1.0261, G2 Loss: 0.6915, D1 Loss: 1.3585, D2 Loss: 1.0893\n",
      "Batch 29, G1 Loss: 0.8857, G2 Loss: 8.5757, D1 Loss: 1.5909, D2 Loss: 0.5012\n",
      "Batch 30, G1 Loss: 0.9707, G2 Loss: 8.7205, D1 Loss: 1.4495, D2 Loss: 1.2686\n",
      "Batch 31, G1 Loss: 1.2199, G2 Loss: 3.2624, D1 Loss: 1.2879, D2 Loss: 0.2160\n",
      "Batch 32, G1 Loss: 0.8136, G2 Loss: 0.5028, D1 Loss: 1.3660, D2 Loss: 1.6348\n",
      "Epoch 9 Average -> G1 Loss: 1.0661619901657104, G2 Loss: 3.3165252208709717, D1 Loss: 1.448411226272583, D2 Loss: 0.7056500315666199\n",
      "Epoch 10/100\n",
      "Batch 1, G1 Loss: 1.4051, G2 Loss: 6.7028, D1 Loss: 1.3839, D2 Loss: 0.8161\n",
      "Batch 2, G1 Loss: 0.7413, G2 Loss: 5.9066, D1 Loss: 1.6180, D2 Loss: 0.7188\n",
      "Batch 3, G1 Loss: 1.4161, G2 Loss: 2.5410, D1 Loss: 1.6848, D2 Loss: 0.8276\n",
      "Batch 4, G1 Loss: 0.7286, G2 Loss: 1.3896, D1 Loss: 1.6348, D2 Loss: 0.8553\n",
      "Batch 5, G1 Loss: 1.4934, G2 Loss: 4.6677, D1 Loss: 1.6250, D2 Loss: 0.2363\n",
      "Batch 6, G1 Loss: 0.6377, G2 Loss: 3.3295, D1 Loss: 1.8085, D2 Loss: 0.1211\n",
      "Batch 7, G1 Loss: 1.5931, G2 Loss: 1.9561, D1 Loss: 1.6802, D2 Loss: 0.4914\n",
      "Batch 8, G1 Loss: 0.6595, G2 Loss: 3.6657, D1 Loss: 1.7852, D2 Loss: 0.2018\n",
      "Batch 9, G1 Loss: 1.3854, G2 Loss: 3.0350, D1 Loss: 1.7016, D2 Loss: 0.3175\n",
      "Batch 10, G1 Loss: 0.7862, G2 Loss: 3.1690, D1 Loss: 1.6117, D2 Loss: 0.1680\n",
      "Batch 11, G1 Loss: 1.2593, G2 Loss: 2.3653, D1 Loss: 1.4665, D2 Loss: 0.4277\n",
      "Batch 12, G1 Loss: 0.8870, G2 Loss: 2.0486, D1 Loss: 1.5050, D2 Loss: 0.5140\n",
      "Batch 13, G1 Loss: 1.0674, G2 Loss: 3.1035, D1 Loss: 1.4333, D2 Loss: 0.4413\n",
      "Batch 14, G1 Loss: 0.9603, G2 Loss: 1.8528, D1 Loss: 1.3808, D2 Loss: 0.5144\n",
      "Batch 15, G1 Loss: 1.2097, G2 Loss: 3.0039, D1 Loss: 1.3905, D2 Loss: 0.4290\n",
      "Batch 16, G1 Loss: 0.8955, G2 Loss: 2.8487, D1 Loss: 1.4170, D2 Loss: 0.2786\n",
      "Batch 17, G1 Loss: 1.2009, G2 Loss: 2.5911, D1 Loss: 1.3580, D2 Loss: 0.5539\n",
      "Batch 18, G1 Loss: 0.9681, G2 Loss: 2.3877, D1 Loss: 1.3110, D2 Loss: 0.6096\n",
      "Batch 19, G1 Loss: 1.2604, G2 Loss: 2.0663, D1 Loss: 1.3306, D2 Loss: 0.7185\n",
      "Batch 20, G1 Loss: 0.8976, G2 Loss: 2.3957, D1 Loss: 1.3324, D2 Loss: 0.6403\n",
      "Batch 21, G1 Loss: 1.4105, G2 Loss: 2.1540, D1 Loss: 1.3691, D2 Loss: 0.2711\n",
      "Batch 22, G1 Loss: 0.8081, G2 Loss: 3.3174, D1 Loss: 1.4792, D2 Loss: 0.4205\n",
      "Batch 23, G1 Loss: 1.2999, G2 Loss: 1.9459, D1 Loss: 1.5067, D2 Loss: 0.6524\n",
      "Batch 24, G1 Loss: 0.7882, G2 Loss: 2.5099, D1 Loss: 1.5409, D2 Loss: 0.9652\n",
      "Batch 25, G1 Loss: 1.1357, G2 Loss: 1.7421, D1 Loss: 1.5385, D2 Loss: 0.8367\n",
      "Batch 26, G1 Loss: 0.8581, G2 Loss: 2.7840, D1 Loss: 1.4813, D2 Loss: 0.4155\n",
      "Batch 27, G1 Loss: 1.3399, G2 Loss: 3.1546, D1 Loss: 1.3183, D2 Loss: 0.2417\n",
      "Batch 28, G1 Loss: 0.7383, G2 Loss: 2.5151, D1 Loss: 1.4688, D2 Loss: 0.8180\n",
      "Batch 29, G1 Loss: 1.4519, G2 Loss: 2.3532, D1 Loss: 1.4928, D2 Loss: 0.3977\n",
      "Batch 30, G1 Loss: 0.6839, G2 Loss: 3.8459, D1 Loss: 1.6219, D2 Loss: 0.1632\n",
      "Batch 31, G1 Loss: 1.4302, G2 Loss: 3.4979, D1 Loss: 1.5634, D2 Loss: 0.6235\n",
      "Batch 32, G1 Loss: 0.6476, G2 Loss: 2.3128, D1 Loss: 1.6514, D2 Loss: 0.5159\n",
      "Epoch 10 Average -> G1 Loss: 1.063901424407959, G2 Loss: 2.911233425140381, D1 Loss: 1.5153579711914062, D2 Loss: 0.506340503692627\n",
      "Epoch 11/100\n",
      "Batch 1, G1 Loss: 1.4502, G2 Loss: 3.2551, D1 Loss: 1.5779, D2 Loss: 0.9098\n",
      "Batch 2, G1 Loss: 0.6713, G2 Loss: 1.4690, D1 Loss: 1.5523, D2 Loss: 0.7076\n",
      "Batch 3, G1 Loss: 1.5026, G2 Loss: 4.0247, D1 Loss: 1.5609, D2 Loss: 0.2691\n",
      "Batch 4, G1 Loss: 0.7133, G2 Loss: 3.0215, D1 Loss: 1.5195, D2 Loss: 0.3426\n",
      "Batch 5, G1 Loss: 1.3140, G2 Loss: 1.6597, D1 Loss: 1.4217, D2 Loss: 0.8937\n",
      "Batch 6, G1 Loss: 0.7772, G2 Loss: 2.0908, D1 Loss: 1.4224, D2 Loss: 0.4596\n",
      "Batch 7, G1 Loss: 1.2466, G2 Loss: 2.6892, D1 Loss: 1.3215, D2 Loss: 0.7191\n",
      "Batch 8, G1 Loss: 0.8824, G2 Loss: 1.6740, D1 Loss: 1.3243, D2 Loss: 1.0706\n",
      "Batch 9, G1 Loss: 1.0833, G2 Loss: 4.3354, D1 Loss: 1.3167, D2 Loss: 0.5002\n",
      "Batch 10, G1 Loss: 1.0108, G2 Loss: 1.1316, D1 Loss: 1.2501, D2 Loss: 0.7743\n",
      "Batch 11, G1 Loss: 1.1691, G2 Loss: 6.1065, D1 Loss: 1.2964, D2 Loss: 0.5969\n",
      "Batch 12, G1 Loss: 0.9624, G2 Loss: 4.1844, D1 Loss: 1.4040, D2 Loss: 0.2858\n",
      "Batch 13, G1 Loss: 1.0801, G2 Loss: 1.7979, D1 Loss: 1.3475, D2 Loss: 0.4822\n",
      "Batch 14, G1 Loss: 1.0726, G2 Loss: 3.0836, D1 Loss: 1.3790, D2 Loss: 0.3951\n",
      "Batch 15, G1 Loss: 0.9293, G2 Loss: 2.6063, D1 Loss: 1.3301, D2 Loss: 0.4462\n",
      "Batch 16, G1 Loss: 1.3269, G2 Loss: 1.7114, D1 Loss: 1.1982, D2 Loss: 0.7134\n",
      "Batch 17, G1 Loss: 0.7452, G2 Loss: 3.8885, D1 Loss: 1.4853, D2 Loss: 0.5226\n",
      "Batch 18, G1 Loss: 1.4867, G2 Loss: 1.7821, D1 Loss: 1.5234, D2 Loss: 1.0387\n",
      "Batch 19, G1 Loss: 0.6598, G2 Loss: 1.9122, D1 Loss: 1.6976, D2 Loss: 0.9705\n",
      "Batch 20, G1 Loss: 1.4006, G2 Loss: 2.8088, D1 Loss: 1.4183, D2 Loss: 0.9225\n",
      "Batch 21, G1 Loss: 0.7807, G2 Loss: 2.1826, D1 Loss: 1.4472, D2 Loss: 0.5840\n",
      "Batch 22, G1 Loss: 1.4114, G2 Loss: 2.7947, D1 Loss: 1.3808, D2 Loss: 0.3830\n",
      "Batch 23, G1 Loss: 0.7519, G2 Loss: 2.1482, D1 Loss: 1.6009, D2 Loss: 0.5739\n",
      "Batch 24, G1 Loss: 1.4587, G2 Loss: 2.5011, D1 Loss: 1.6395, D2 Loss: 0.8444\n",
      "Batch 25, G1 Loss: 0.6098, G2 Loss: 1.7340, D1 Loss: 1.8198, D2 Loss: 0.7864\n",
      "Batch 26, G1 Loss: 1.6311, G2 Loss: 4.2338, D1 Loss: 1.7637, D2 Loss: 0.1856\n",
      "Batch 27, G1 Loss: 0.6475, G2 Loss: 2.3754, D1 Loss: 1.6481, D2 Loss: 0.6271\n",
      "Batch 28, G1 Loss: 1.2992, G2 Loss: 2.2298, D1 Loss: 1.5275, D2 Loss: 0.4731\n",
      "Batch 29, G1 Loss: 0.8152, G2 Loss: 4.3766, D1 Loss: 1.4710, D2 Loss: 0.2937\n",
      "Batch 30, G1 Loss: 1.1328, G2 Loss: 2.1454, D1 Loss: 1.4541, D2 Loss: 0.4436\n",
      "Batch 31, G1 Loss: 0.8913, G2 Loss: 0.6887, D1 Loss: 1.4490, D2 Loss: 1.2895\n",
      "Batch 32, G1 Loss: 1.1228, G2 Loss: 7.9815, D1 Loss: 1.4204, D2 Loss: 2.0742\n",
      "Epoch 11 Average -> G1 Loss: 1.0636451244354248, G2 Loss: 2.832008123397827, D1 Loss: 1.4677772521972656, D2 Loss: 0.6743353009223938\n",
      "Epoch 12/100\n",
      "Batch 1, G1 Loss: 0.9396, G2 Loss: 3.7800, D1 Loss: 1.4040, D2 Loss: 0.5054\n",
      "Batch 2, G1 Loss: 1.1208, G2 Loss: 0.9162, D1 Loss: 1.4839, D2 Loss: 1.2899\n",
      "Batch 3, G1 Loss: 0.9564, G2 Loss: 7.9909, D1 Loss: 1.3919, D2 Loss: 2.0262\n",
      "Batch 4, G1 Loss: 1.0672, G2 Loss: 1.5221, D1 Loss: 1.3963, D2 Loss: 0.6787\n",
      "Batch 5, G1 Loss: 0.9747, G2 Loss: 4.9392, D1 Loss: 1.3808, D2 Loss: 0.8826\n",
      "Batch 6, G1 Loss: 0.9420, G2 Loss: 1.6914, D1 Loss: 1.4287, D2 Loss: 0.4403\n",
      "Batch 7, G1 Loss: 1.2293, G2 Loss: 2.3294, D1 Loss: 1.3955, D2 Loss: 0.7951\n",
      "Batch 8, G1 Loss: 0.7050, G2 Loss: 1.4331, D1 Loss: 1.4952, D2 Loss: 0.7127\n",
      "Batch 9, G1 Loss: 1.6019, G2 Loss: 2.4950, D1 Loss: 1.5721, D2 Loss: 0.6215\n",
      "Batch 10, G1 Loss: 0.5454, G2 Loss: 1.3757, D1 Loss: 1.6826, D2 Loss: 0.6775\n",
      "Batch 11, G1 Loss: 1.7073, G2 Loss: 3.9268, D1 Loss: 1.7708, D2 Loss: 0.5983\n",
      "Batch 12, G1 Loss: 0.5615, G2 Loss: 2.0688, D1 Loss: 1.7293, D2 Loss: 0.8645\n",
      "Batch 13, G1 Loss: 1.1982, G2 Loss: 1.5066, D1 Loss: 1.5613, D2 Loss: 0.8692\n",
      "Batch 14, G1 Loss: 0.8733, G2 Loss: 4.1640, D1 Loss: 1.4190, D2 Loss: 0.7971\n",
      "Batch 15, G1 Loss: 0.9654, G2 Loss: 1.8642, D1 Loss: 1.4284, D2 Loss: 0.7545\n",
      "Batch 16, G1 Loss: 1.0440, G2 Loss: 1.8403, D1 Loss: 1.3223, D2 Loss: 0.6078\n",
      "Batch 17, G1 Loss: 0.9561, G2 Loss: 3.8588, D1 Loss: 1.2724, D2 Loss: 0.9453\n",
      "Batch 18, G1 Loss: 1.2056, G2 Loss: 0.9658, D1 Loss: 1.3949, D2 Loss: 1.2205\n",
      "Batch 19, G1 Loss: 0.7247, G2 Loss: 4.6067, D1 Loss: 1.4639, D2 Loss: 0.5130\n",
      "Batch 20, G1 Loss: 1.4628, G2 Loss: 4.9201, D1 Loss: 1.4653, D2 Loss: 0.7455\n",
      "Batch 21, G1 Loss: 0.7197, G2 Loss: 1.1950, D1 Loss: 1.4022, D2 Loss: 1.0113\n",
      "Batch 22, G1 Loss: 1.1462, G2 Loss: 3.1949, D1 Loss: 1.2296, D2 Loss: 0.4547\n",
      "Batch 23, G1 Loss: 1.1842, G2 Loss: 1.9959, D1 Loss: 1.1755, D2 Loss: 0.5603\n",
      "Batch 24, G1 Loss: 0.8963, G2 Loss: 2.7324, D1 Loss: 1.3480, D2 Loss: 0.5879\n",
      "Batch 25, G1 Loss: 1.1018, G2 Loss: 1.5050, D1 Loss: 1.3361, D2 Loss: 0.8747\n",
      "Batch 26, G1 Loss: 0.9796, G2 Loss: 2.9336, D1 Loss: 1.4021, D2 Loss: 0.6795\n",
      "Batch 27, G1 Loss: 0.8806, G2 Loss: 1.5561, D1 Loss: 1.4613, D2 Loss: 0.6394\n",
      "Batch 28, G1 Loss: 1.1024, G2 Loss: 3.5475, D1 Loss: 1.4742, D2 Loss: 0.9636\n",
      "Batch 29, G1 Loss: 0.8562, G2 Loss: 0.8886, D1 Loss: 1.4335, D2 Loss: 1.1797\n",
      "Batch 30, G1 Loss: 1.1117, G2 Loss: 4.0827, D1 Loss: 1.4091, D2 Loss: 0.5168\n",
      "Batch 31, G1 Loss: 0.9332, G2 Loss: 2.8669, D1 Loss: 1.4670, D2 Loss: 0.6173\n",
      "Batch 32, G1 Loss: 0.9421, G2 Loss: 1.8124, D1 Loss: 1.3561, D2 Loss: 0.9189\n",
      "Epoch 12 Average -> G1 Loss: 1.019852876663208, G2 Loss: 2.7033305168151855, D1 Loss: 1.4360406398773193, D2 Loss: 0.7984169125556946\n",
      "Epoch 13/100\n",
      "Batch 1, G1 Loss: 1.0630, G2 Loss: 2.7529, D1 Loss: 1.3991, D2 Loss: 0.6410\n",
      "Batch 2, G1 Loss: 0.8613, G2 Loss: 1.5915, D1 Loss: 1.4294, D2 Loss: 0.5802\n",
      "Batch 3, G1 Loss: 0.9730, G2 Loss: 2.5961, D1 Loss: 1.4016, D2 Loss: 0.8214\n",
      "Batch 4, G1 Loss: 1.0241, G2 Loss: 0.9584, D1 Loss: 1.3651, D2 Loss: 0.9724\n",
      "Batch 5, G1 Loss: 0.8248, G2 Loss: 5.4325, D1 Loss: 1.5726, D2 Loss: 0.8173\n",
      "Batch 6, G1 Loss: 0.9928, G2 Loss: 2.4578, D1 Loss: 1.4814, D2 Loss: 0.4594\n",
      "Batch 7, G1 Loss: 0.8310, G2 Loss: 0.6656, D1 Loss: 1.4966, D2 Loss: 1.2674\n",
      "Batch 8, G1 Loss: 1.0553, G2 Loss: 8.8188, D1 Loss: 1.4565, D2 Loss: 1.4222\n",
      "Batch 9, G1 Loss: 0.8043, G2 Loss: 4.6094, D1 Loss: 1.5247, D2 Loss: 0.5417\n",
      "Batch 10, G1 Loss: 1.2736, G2 Loss: 1.7692, D1 Loss: 1.4590, D2 Loss: 0.6227\n",
      "Batch 11, G1 Loss: 0.6084, G2 Loss: 1.7411, D1 Loss: 1.7784, D2 Loss: 0.8366\n",
      "Batch 12, G1 Loss: 1.5909, G2 Loss: 4.5308, D1 Loss: 1.7386, D2 Loss: 0.8947\n",
      "Batch 13, G1 Loss: 0.4784, G2 Loss: 1.6188, D1 Loss: 1.8405, D2 Loss: 0.9633\n",
      "Batch 14, G1 Loss: 1.7442, G2 Loss: 0.7197, D1 Loss: 1.6662, D2 Loss: 1.1843\n",
      "Batch 15, G1 Loss: 0.7200, G2 Loss: 5.0840, D1 Loss: 1.5144, D2 Loss: 1.1238\n",
      "Batch 16, G1 Loss: 1.0422, G2 Loss: 2.6227, D1 Loss: 1.2899, D2 Loss: 0.5348\n",
      "Batch 17, G1 Loss: 1.2338, G2 Loss: 1.2683, D1 Loss: 1.3149, D2 Loss: 0.7780\n",
      "Batch 18, G1 Loss: 0.7873, G2 Loss: 2.5322, D1 Loss: 1.3918, D2 Loss: 0.4128\n",
      "Batch 19, G1 Loss: 1.1445, G2 Loss: 3.0051, D1 Loss: 1.3726, D2 Loss: 0.6671\n",
      "Batch 20, G1 Loss: 0.8305, G2 Loss: 1.6592, D1 Loss: 1.5570, D2 Loss: 0.6965\n",
      "Batch 21, G1 Loss: 0.9289, G2 Loss: 2.6144, D1 Loss: 1.4265, D2 Loss: 0.5514\n",
      "Batch 22, G1 Loss: 1.0478, G2 Loss: 2.1768, D1 Loss: 1.4181, D2 Loss: 0.5272\n",
      "Batch 23, G1 Loss: 1.0045, G2 Loss: 1.9933, D1 Loss: 1.3272, D2 Loss: 0.7353\n",
      "Batch 24, G1 Loss: 1.0224, G2 Loss: 2.4543, D1 Loss: 1.2117, D2 Loss: 0.6383\n",
      "Batch 25, G1 Loss: 1.0920, G2 Loss: 0.8672, D1 Loss: 1.2453, D2 Loss: 0.9819\n",
      "Batch 26, G1 Loss: 0.8958, G2 Loss: 8.4281, D1 Loss: 1.4460, D2 Loss: 1.7657\n",
      "Batch 27, G1 Loss: 1.0217, G2 Loss: 4.1079, D1 Loss: 1.5646, D2 Loss: 0.2303\n",
      "Batch 28, G1 Loss: 0.7445, G2 Loss: 0.5254, D1 Loss: 1.7021, D2 Loss: 1.7704\n",
      "Batch 29, G1 Loss: 1.1033, G2 Loss: 6.1147, D1 Loss: 1.6160, D2 Loss: 1.0899\n",
      "Batch 30, G1 Loss: 0.7920, G2 Loss: 4.6781, D1 Loss: 1.6458, D2 Loss: 0.6622\n",
      "Batch 31, G1 Loss: 0.9675, G2 Loss: 1.1374, D1 Loss: 1.4971, D2 Loss: 1.0668\n",
      "Batch 32, G1 Loss: 0.9904, G2 Loss: 3.5180, D1 Loss: 1.3973, D2 Loss: 0.5588\n",
      "Epoch 13 Average -> G1 Loss: 0.9841948747634888, G2 Loss: 2.9703071117401123, D1 Loss: 1.4858787059783936, D2 Loss: 0.8379967212677002\n",
      "Epoch 14/100\n",
      "Batch 1, G1 Loss: 1.0552, G2 Loss: 3.0405, D1 Loss: 1.3745, D2 Loss: 0.3144\n",
      "Batch 2, G1 Loss: 0.8969, G2 Loss: 1.7057, D1 Loss: 1.2965, D2 Loss: 0.7811\n",
      "Batch 3, G1 Loss: 1.2348, G2 Loss: 4.2998, D1 Loss: 1.3380, D2 Loss: 0.8670\n",
      "Batch 4, G1 Loss: 0.7400, G2 Loss: 2.1152, D1 Loss: 1.4208, D2 Loss: 0.4714\n",
      "Batch 5, G1 Loss: 1.2297, G2 Loss: 1.4213, D1 Loss: 1.3097, D2 Loss: 0.8990\n",
      "Batch 6, G1 Loss: 0.8231, G2 Loss: 3.2971, D1 Loss: 1.3219, D2 Loss: 0.5395\n",
      "Batch 7, G1 Loss: 1.2215, G2 Loss: 2.0034, D1 Loss: 1.4017, D2 Loss: 0.6003\n",
      "Batch 8, G1 Loss: 0.6568, G2 Loss: 1.8386, D1 Loss: 1.4911, D2 Loss: 0.5461\n",
      "Batch 9, G1 Loss: 1.4591, G2 Loss: 3.0178, D1 Loss: 1.4611, D2 Loss: 0.9605\n",
      "Batch 10, G1 Loss: 0.5543, G2 Loss: 0.8668, D1 Loss: 1.6810, D2 Loss: 1.0998\n",
      "Batch 11, G1 Loss: 1.5495, G2 Loss: 4.2974, D1 Loss: 1.5508, D2 Loss: 1.3025\n",
      "Batch 12, G1 Loss: 0.5556, G2 Loss: 1.3919, D1 Loss: 1.6545, D2 Loss: 0.8231\n",
      "Batch 13, G1 Loss: 1.4991, G2 Loss: 1.8250, D1 Loss: 1.3916, D2 Loss: 0.6659\n",
      "Batch 14, G1 Loss: 0.6312, G2 Loss: 2.6975, D1 Loss: 1.5310, D2 Loss: 0.9787\n",
      "Batch 15, G1 Loss: 1.6336, G2 Loss: 0.4456, D1 Loss: 1.3612, D2 Loss: 1.7948\n",
      "Batch 16, G1 Loss: 0.6038, G2 Loss: 6.9888, D1 Loss: 1.6149, D2 Loss: 1.1867\n",
      "Batch 17, G1 Loss: 1.4596, G2 Loss: 4.8805, D1 Loss: 1.4415, D2 Loss: 0.8298\n",
      "Batch 18, G1 Loss: 0.6018, G2 Loss: 1.0829, D1 Loss: 1.5691, D2 Loss: 1.0493\n",
      "Batch 19, G1 Loss: 1.3285, G2 Loss: 2.6452, D1 Loss: 1.6018, D2 Loss: 0.8883\n",
      "Batch 20, G1 Loss: 0.6944, G2 Loss: 2.7836, D1 Loss: 1.5292, D2 Loss: 0.7182\n",
      "Batch 21, G1 Loss: 1.1287, G2 Loss: 2.2581, D1 Loss: 1.4365, D2 Loss: 0.7440\n",
      "Batch 22, G1 Loss: 0.8704, G2 Loss: 1.2325, D1 Loss: 1.3099, D2 Loss: 0.8011\n",
      "Batch 23, G1 Loss: 1.0833, G2 Loss: 2.9015, D1 Loss: 1.2587, D2 Loss: 0.5169\n",
      "Batch 24, G1 Loss: 0.9704, G2 Loss: 2.1008, D1 Loss: 1.3145, D2 Loss: 0.4275\n",
      "Batch 25, G1 Loss: 0.9412, G2 Loss: 1.8422, D1 Loss: 1.3112, D2 Loss: 0.5638\n",
      "Batch 26, G1 Loss: 1.1109, G2 Loss: 3.9999, D1 Loss: 1.2996, D2 Loss: 0.3630\n",
      "Batch 27, G1 Loss: 0.8393, G2 Loss: 2.7320, D1 Loss: 1.3157, D2 Loss: 0.2626\n",
      "Batch 28, G1 Loss: 1.3370, G2 Loss: 0.9666, D1 Loss: 1.3940, D2 Loss: 0.8693\n",
      "Batch 29, G1 Loss: 0.6175, G2 Loss: 5.7558, D1 Loss: 1.6745, D2 Loss: 1.2758\n",
      "Batch 30, G1 Loss: 1.6782, G2 Loss: 2.4045, D1 Loss: 1.6043, D2 Loss: 0.5175\n",
      "Batch 31, G1 Loss: 0.5738, G2 Loss: 1.6977, D1 Loss: 1.6865, D2 Loss: 0.6513\n",
      "Batch 32, G1 Loss: 1.4087, G2 Loss: 2.9487, D1 Loss: 1.3666, D2 Loss: 0.4517\n",
      "Epoch 14 Average -> G1 Loss: 1.030868411064148, G2 Loss: 2.6089015007019043, D1 Loss: 1.4473053216934204, D2 Loss: 0.773773193359375\n",
      "Epoch 15/100\n",
      "Batch 1, G1 Loss: 0.7233, G2 Loss: 2.7781, D1 Loss: 1.4900, D2 Loss: 0.4773\n",
      "Batch 2, G1 Loss: 1.0096, G2 Loss: 1.4370, D1 Loss: 1.4691, D2 Loss: 0.7412\n",
      "Batch 3, G1 Loss: 0.9379, G2 Loss: 1.6076, D1 Loss: 1.4854, D2 Loss: 0.6062\n",
      "Batch 4, G1 Loss: 0.9120, G2 Loss: 3.6844, D1 Loss: 1.5392, D2 Loss: 0.3716\n",
      "Batch 5, G1 Loss: 0.9958, G2 Loss: 2.1679, D1 Loss: 1.4367, D2 Loss: 0.7367\n",
      "Batch 6, G1 Loss: 0.9154, G2 Loss: 1.3942, D1 Loss: 1.4912, D2 Loss: 1.1555\n",
      "Batch 7, G1 Loss: 0.8522, G2 Loss: 3.6408, D1 Loss: 1.4745, D2 Loss: 1.0065\n",
      "Batch 8, G1 Loss: 0.9761, G2 Loss: 1.0243, D1 Loss: 1.4424, D2 Loss: 1.0634\n",
      "Batch 9, G1 Loss: 0.9281, G2 Loss: 4.0109, D1 Loss: 1.5171, D2 Loss: 0.8165\n",
      "Batch 10, G1 Loss: 0.9252, G2 Loss: 2.7958, D1 Loss: 1.4601, D2 Loss: 0.2594\n",
      "Batch 11, G1 Loss: 1.0300, G2 Loss: 0.9656, D1 Loss: 1.5094, D2 Loss: 0.9924\n",
      "Batch 12, G1 Loss: 0.7959, G2 Loss: 4.5675, D1 Loss: 1.3808, D2 Loss: 1.2237\n",
      "Batch 13, G1 Loss: 1.3983, G2 Loss: 2.7610, D1 Loss: 1.5489, D2 Loss: 0.2937\n",
      "Batch 14, G1 Loss: 0.6383, G2 Loss: 1.9017, D1 Loss: 1.5121, D2 Loss: 0.3907\n",
      "Batch 15, G1 Loss: 1.6125, G2 Loss: 1.1377, D1 Loss: 1.5203, D2 Loss: 0.7334\n",
      "Batch 16, G1 Loss: 0.6776, G2 Loss: 4.9199, D1 Loss: 1.4600, D2 Loss: 0.5621\n",
      "Batch 17, G1 Loss: 1.4183, G2 Loss: 3.1040, D1 Loss: 1.2931, D2 Loss: 0.7115\n",
      "Batch 18, G1 Loss: 0.8811, G2 Loss: 0.4297, D1 Loss: 1.2380, D2 Loss: 1.8144\n",
      "Batch 19, G1 Loss: 1.1401, G2 Loss: 7.2647, D1 Loss: 1.2465, D2 Loss: 1.2190\n",
      "Batch 20, G1 Loss: 0.9898, G2 Loss: 4.5313, D1 Loss: 1.2544, D2 Loss: 0.9349\n",
      "Batch 21, G1 Loss: 0.9523, G2 Loss: 0.7504, D1 Loss: 1.2832, D2 Loss: 1.5907\n",
      "Batch 22, G1 Loss: 1.0784, G2 Loss: 3.8709, D1 Loss: 1.3012, D2 Loss: 0.7561\n",
      "Batch 23, G1 Loss: 0.8892, G2 Loss: 2.5121, D1 Loss: 1.3021, D2 Loss: 0.7355\n",
      "Batch 24, G1 Loss: 0.9877, G2 Loss: 1.2381, D1 Loss: 1.3357, D2 Loss: 0.6946\n",
      "Batch 25, G1 Loss: 1.0096, G2 Loss: 2.6000, D1 Loss: 1.3449, D2 Loss: 0.4198\n",
      "Batch 26, G1 Loss: 0.8102, G2 Loss: 3.2345, D1 Loss: 1.4233, D2 Loss: 0.8031\n",
      "Batch 27, G1 Loss: 1.0900, G2 Loss: 2.4247, D1 Loss: 1.3336, D2 Loss: 0.2906\n",
      "Batch 28, G1 Loss: 0.8524, G2 Loss: 0.9743, D1 Loss: 1.3612, D2 Loss: 0.8382\n",
      "Batch 29, G1 Loss: 1.0874, G2 Loss: 6.6417, D1 Loss: 1.3480, D2 Loss: 1.2213\n",
      "Batch 30, G1 Loss: 0.7966, G2 Loss: 3.3359, D1 Loss: 1.4890, D2 Loss: 0.3779\n",
      "Batch 31, G1 Loss: 1.1757, G2 Loss: 1.5518, D1 Loss: 1.6266, D2 Loss: 0.6766\n",
      "Batch 32, G1 Loss: 0.6372, G2 Loss: 2.8145, D1 Loss: 1.6361, D2 Loss: 0.6372\n",
      "Epoch 15 Average -> G1 Loss: 0.9726263284683228, G2 Loss: 2.752284049987793, D1 Loss: 1.4235697984695435, D2 Loss: 0.7859896421432495\n",
      "Epoch 16/100\n",
      "Batch 1, G1 Loss: 1.3773, G2 Loss: 1.4605, D1 Loss: 1.5962, D2 Loss: 0.5780\n",
      "Batch 2, G1 Loss: 0.6579, G2 Loss: 2.7152, D1 Loss: 1.5578, D2 Loss: 0.4366\n",
      "Batch 3, G1 Loss: 1.2037, G2 Loss: 2.3842, D1 Loss: 1.4511, D2 Loss: 0.5729\n",
      "Batch 4, G1 Loss: 0.7882, G2 Loss: 2.0796, D1 Loss: 1.3668, D2 Loss: 0.4505\n",
      "Batch 5, G1 Loss: 1.0797, G2 Loss: 2.9222, D1 Loss: 1.3668, D2 Loss: 0.3854\n",
      "Batch 6, G1 Loss: 0.9742, G2 Loss: 1.2146, D1 Loss: 1.3752, D2 Loss: 0.7902\n",
      "Batch 7, G1 Loss: 0.7934, G2 Loss: 4.9788, D1 Loss: 1.3878, D2 Loss: 0.3139\n",
      "Batch 8, G1 Loss: 1.3784, G2 Loss: 3.6859, D1 Loss: 1.3486, D2 Loss: 0.2728\n",
      "Batch 9, G1 Loss: 0.7801, G2 Loss: 1.9331, D1 Loss: 1.3779, D2 Loss: 0.4391\n",
      "Batch 10, G1 Loss: 1.1320, G2 Loss: 3.1332, D1 Loss: 1.3658, D2 Loss: 0.6019\n",
      "Batch 11, G1 Loss: 0.8074, G2 Loss: 1.5782, D1 Loss: 1.5082, D2 Loss: 0.8536\n",
      "Batch 12, G1 Loss: 1.0330, G2 Loss: 5.6799, D1 Loss: 1.5074, D2 Loss: 0.1738\n",
      "Batch 13, G1 Loss: 0.8894, G2 Loss: 2.1861, D1 Loss: 1.4397, D2 Loss: 0.3595\n",
      "Batch 14, G1 Loss: 1.0635, G2 Loss: 2.4631, D1 Loss: 1.4286, D2 Loss: 0.5894\n",
      "Batch 15, G1 Loss: 0.9112, G2 Loss: 2.4596, D1 Loss: 1.2553, D2 Loss: 0.4358\n",
      "Batch 16, G1 Loss: 1.0687, G2 Loss: 2.2791, D1 Loss: 1.3464, D2 Loss: 0.5139\n",
      "Batch 17, G1 Loss: 0.9160, G2 Loss: 2.7982, D1 Loss: 1.4470, D2 Loss: 0.8949\n",
      "Batch 18, G1 Loss: 1.0330, G2 Loss: 0.9755, D1 Loss: 1.5046, D2 Loss: 0.9341\n",
      "Batch 19, G1 Loss: 0.8865, G2 Loss: 5.3807, D1 Loss: 1.5137, D2 Loss: 1.4203\n",
      "Batch 20, G1 Loss: 1.0879, G2 Loss: 1.3564, D1 Loss: 1.4296, D2 Loss: 0.5929\n",
      "Batch 21, G1 Loss: 0.9063, G2 Loss: 2.0701, D1 Loss: 1.3420, D2 Loss: 0.3903\n",
      "Batch 22, G1 Loss: 1.3526, G2 Loss: 3.8303, D1 Loss: 1.3094, D2 Loss: 0.9016\n",
      "Batch 23, G1 Loss: 0.7517, G2 Loss: 1.0190, D1 Loss: 1.2931, D2 Loss: 0.8785\n",
      "Batch 24, G1 Loss: 1.8276, G2 Loss: 4.0955, D1 Loss: 1.4083, D2 Loss: 0.5203\n",
      "Batch 25, G1 Loss: 0.6315, G2 Loss: 3.3300, D1 Loss: 1.5063, D2 Loss: 0.3756\n",
      "Batch 26, G1 Loss: 1.5804, G2 Loss: 3.5244, D1 Loss: 1.3340, D2 Loss: 0.1765\n",
      "Batch 27, G1 Loss: 0.8081, G2 Loss: 0.5467, D1 Loss: 1.3581, D2 Loss: 1.5054\n",
      "Batch 28, G1 Loss: 1.5049, G2 Loss: 6.4794, D1 Loss: 1.2231, D2 Loss: 0.9969\n",
      "Batch 29, G1 Loss: 0.7044, G2 Loss: 5.1884, D1 Loss: 1.4149, D2 Loss: 1.0172\n",
      "Batch 30, G1 Loss: 1.5196, G2 Loss: 1.7768, D1 Loss: 1.2630, D2 Loss: 0.6239\n",
      "Batch 31, G1 Loss: 0.7747, G2 Loss: 1.7573, D1 Loss: 1.2933, D2 Loss: 0.4682\n",
      "Batch 32, G1 Loss: 1.3094, G2 Loss: 3.3915, D1 Loss: 1.2062, D2 Loss: 0.4167\n",
      "Epoch 16 Average -> G1 Loss: 1.0478925704956055, G2 Loss: 2.833554983139038, D1 Loss: 1.3914463520050049, D2 Loss: 0.6212643384933472\n",
      "Epoch 17/100\n",
      "Batch 1, G1 Loss: 0.8871, G2 Loss: 2.4219, D1 Loss: 1.2587, D2 Loss: 0.3513\n",
      "Batch 2, G1 Loss: 1.0976, G2 Loss: 1.4078, D1 Loss: 1.3465, D2 Loss: 0.5656\n",
      "Batch 3, G1 Loss: 0.8659, G2 Loss: 3.1792, D1 Loss: 1.4844, D2 Loss: 0.6742\n",
      "Batch 4, G1 Loss: 1.0191, G2 Loss: 1.5368, D1 Loss: 1.4315, D2 Loss: 0.4765\n",
      "Batch 5, G1 Loss: 0.9548, G2 Loss: 3.1542, D1 Loss: 1.4947, D2 Loss: 0.2438\n",
      "Batch 6, G1 Loss: 0.8607, G2 Loss: 2.2924, D1 Loss: 1.5924, D2 Loss: 0.4075\n",
      "Batch 7, G1 Loss: 0.9674, G2 Loss: 1.7877, D1 Loss: 1.5459, D2 Loss: 0.5328\n",
      "Batch 8, G1 Loss: 0.8931, G2 Loss: 5.1960, D1 Loss: 1.5187, D2 Loss: 0.9079\n",
      "Batch 9, G1 Loss: 0.9187, G2 Loss: 3.0751, D1 Loss: 1.3669, D2 Loss: 0.1640\n",
      "Batch 10, G1 Loss: 1.3886, G2 Loss: 0.4809, D1 Loss: 1.3093, D2 Loss: 1.5726\n",
      "Batch 11, G1 Loss: 0.4919, G2 Loss: 10.2601, D1 Loss: 1.9377, D2 Loss: 1.6331\n",
      "Batch 12, G1 Loss: 1.9908, G2 Loss: 7.5076, D1 Loss: 2.1426, D2 Loss: 0.7375\n",
      "Batch 13, G1 Loss: 0.4851, G2 Loss: 2.6434, D1 Loss: 1.9527, D2 Loss: 0.2911\n",
      "Batch 14, G1 Loss: 1.2713, G2 Loss: 2.0836, D1 Loss: 1.5941, D2 Loss: 0.3882\n",
      "Batch 15, G1 Loss: 0.8832, G2 Loss: 3.0565, D1 Loss: 1.4929, D2 Loss: 0.4745\n",
      "Batch 16, G1 Loss: 0.8990, G2 Loss: 1.5313, D1 Loss: 1.3889, D2 Loss: 0.7838\n",
      "Batch 17, G1 Loss: 1.1500, G2 Loss: 1.6237, D1 Loss: 1.3471, D2 Loss: 0.6161\n",
      "Batch 18, G1 Loss: 0.9700, G2 Loss: 4.2060, D1 Loss: 1.3331, D2 Loss: 0.3831\n",
      "Batch 19, G1 Loss: 1.0824, G2 Loss: 3.4371, D1 Loss: 1.3348, D2 Loss: 0.4029\n",
      "Batch 20, G1 Loss: 0.9866, G2 Loss: 1.9189, D1 Loss: 1.2540, D2 Loss: 0.4293\n",
      "Batch 21, G1 Loss: 1.1528, G2 Loss: 4.9293, D1 Loss: 1.4118, D2 Loss: 0.2888\n",
      "Batch 22, G1 Loss: 0.7762, G2 Loss: 2.4469, D1 Loss: 1.3366, D2 Loss: 0.5466\n",
      "Batch 23, G1 Loss: 1.3101, G2 Loss: 1.5482, D1 Loss: 1.2920, D2 Loss: 0.6933\n",
      "Batch 24, G1 Loss: 0.8110, G2 Loss: 4.4822, D1 Loss: 1.3548, D2 Loss: 0.6961\n",
      "Batch 25, G1 Loss: 1.2416, G2 Loss: 2.2793, D1 Loss: 1.3390, D2 Loss: 0.7557\n",
      "Batch 26, G1 Loss: 0.9042, G2 Loss: 2.7422, D1 Loss: 1.3197, D2 Loss: 0.3032\n",
      "Batch 27, G1 Loss: 1.3291, G2 Loss: 2.7956, D1 Loss: 0.9850, D2 Loss: 0.6046\n",
      "Batch 28, G1 Loss: 1.1066, G2 Loss: 1.3800, D1 Loss: 1.0928, D2 Loss: 0.6607\n",
      "Batch 29, G1 Loss: 0.9243, G2 Loss: 4.7481, D1 Loss: 1.3359, D2 Loss: 0.8961\n",
      "Batch 30, G1 Loss: 1.0788, G2 Loss: 1.2519, D1 Loss: 1.3876, D2 Loss: 0.6311\n",
      "Batch 31, G1 Loss: 0.8197, G2 Loss: 4.9423, D1 Loss: 1.6627, D2 Loss: 0.6122\n",
      "Batch 32, G1 Loss: 0.9211, G2 Loss: 1.7506, D1 Loss: 1.5328, D2 Loss: 0.4791\n",
      "Epoch 17 Average -> G1 Loss: 1.0137053728103638, G2 Loss: 3.065523624420166, D1 Loss: 1.443047046661377, D2 Loss: 0.6001118421554565\n",
      "Epoch 18/100\n",
      "Batch 1, G1 Loss: 0.9887, G2 Loss: 1.9442, D1 Loss: 1.4764, D2 Loss: 0.4019\n",
      "Batch 2, G1 Loss: 0.8911, G2 Loss: 3.7772, D1 Loss: 1.3705, D2 Loss: 0.5292\n",
      "Batch 3, G1 Loss: 1.1219, G2 Loss: 5.6790, D1 Loss: 1.2706, D2 Loss: 0.1033\n",
      "Batch 4, G1 Loss: 0.9856, G2 Loss: 0.5851, D1 Loss: 1.2450, D2 Loss: 1.2233\n",
      "Batch 5, G1 Loss: 1.0293, G2 Loss: 9.0303, D1 Loss: 1.2329, D2 Loss: 1.8513\n",
      "Batch 6, G1 Loss: 1.0636, G2 Loss: 3.6665, D1 Loss: 1.3660, D2 Loss: 0.2689\n",
      "Batch 7, G1 Loss: 0.9194, G2 Loss: 2.2045, D1 Loss: 1.4791, D2 Loss: 0.2925\n",
      "Batch 8, G1 Loss: 1.0553, G2 Loss: 2.7718, D1 Loss: 1.2358, D2 Loss: 0.2333\n",
      "Batch 9, G1 Loss: 1.1158, G2 Loss: 2.4370, D1 Loss: 1.2406, D2 Loss: 0.4197\n",
      "Batch 10, G1 Loss: 0.8720, G2 Loss: 2.0982, D1 Loss: 1.3504, D2 Loss: 0.4200\n",
      "Batch 11, G1 Loss: 1.4085, G2 Loss: 3.6470, D1 Loss: 1.4542, D2 Loss: 0.8159\n",
      "Batch 12, G1 Loss: 0.5129, G2 Loss: 1.4944, D1 Loss: 1.7919, D2 Loss: 0.6906\n",
      "Batch 13, G1 Loss: 1.8587, G2 Loss: 2.8751, D1 Loss: 1.9908, D2 Loss: 0.1866\n",
      "Batch 14, G1 Loss: 0.4854, G2 Loss: 3.4673, D1 Loss: 2.0227, D2 Loss: 0.5224\n",
      "Batch 15, G1 Loss: 1.7720, G2 Loss: 1.7590, D1 Loss: 1.8077, D2 Loss: 0.3641\n",
      "Batch 16, G1 Loss: 0.6661, G2 Loss: 2.3709, D1 Loss: 1.5707, D2 Loss: 0.3869\n",
      "Batch 17, G1 Loss: 1.2816, G2 Loss: 3.8123, D1 Loss: 1.2421, D2 Loss: 0.5301\n",
      "Batch 18, G1 Loss: 1.1494, G2 Loss: 2.3486, D1 Loss: 1.2446, D2 Loss: 0.3747\n",
      "Batch 19, G1 Loss: 0.8604, G2 Loss: 1.8057, D1 Loss: 1.2203, D2 Loss: 0.5857\n",
      "Batch 20, G1 Loss: 1.4720, G2 Loss: 3.4339, D1 Loss: 1.3191, D2 Loss: 0.5974\n",
      "Batch 21, G1 Loss: 0.7143, G2 Loss: 1.8525, D1 Loss: 1.4017, D2 Loss: 0.3769\n",
      "Batch 22, G1 Loss: 1.5571, G2 Loss: 2.5426, D1 Loss: 1.4427, D2 Loss: 0.6303\n",
      "Batch 23, G1 Loss: 0.6021, G2 Loss: 2.2601, D1 Loss: 1.6742, D2 Loss: 0.4930\n",
      "Batch 24, G1 Loss: 1.4382, G2 Loss: 1.4356, D1 Loss: 1.3648, D2 Loss: 0.6314\n",
      "Batch 25, G1 Loss: 0.7260, G2 Loss: 4.2711, D1 Loss: 1.3908, D2 Loss: 1.0183\n",
      "Batch 26, G1 Loss: 1.4826, G2 Loss: 1.7851, D1 Loss: 1.2553, D2 Loss: 0.5791\n",
      "Batch 27, G1 Loss: 0.8525, G2 Loss: 2.9057, D1 Loss: 1.2608, D2 Loss: 0.2851\n",
      "Batch 28, G1 Loss: 1.2138, G2 Loss: 2.6139, D1 Loss: 1.3243, D2 Loss: 0.5298\n",
      "Batch 29, G1 Loss: 0.7781, G2 Loss: 2.1040, D1 Loss: 1.4282, D2 Loss: 0.4546\n",
      "Batch 30, G1 Loss: 1.3822, G2 Loss: 2.6025, D1 Loss: 1.5582, D2 Loss: 0.4752\n",
      "Batch 31, G1 Loss: 0.5524, G2 Loss: 2.1322, D1 Loss: 1.7280, D2 Loss: 0.5665\n",
      "Batch 32, G1 Loss: 1.6775, G2 Loss: 2.0309, D1 Loss: 1.6607, D2 Loss: 0.6191\n",
      "Epoch 18 Average -> G1 Loss: 1.0777068138122559, G2 Loss: 2.804511308670044, D1 Loss: 1.4506614208221436, D2 Loss: 0.5455243587493896\n",
      "Epoch 19/100\n",
      "Batch 1, G1 Loss: 0.6022, G2 Loss: 3.4637, D1 Loss: 1.7061, D2 Loss: 0.3009\n",
      "Batch 2, G1 Loss: 1.3947, G2 Loss: 2.3538, D1 Loss: 1.3423, D2 Loss: 0.5683\n",
      "Batch 3, G1 Loss: 0.8642, G2 Loss: 1.7756, D1 Loss: 1.3293, D2 Loss: 0.4937\n",
      "Batch 4, G1 Loss: 1.3035, G2 Loss: 5.9306, D1 Loss: 1.3160, D2 Loss: 0.5148\n",
      "Batch 5, G1 Loss: 0.8361, G2 Loss: 4.0101, D1 Loss: 1.3480, D2 Loss: 0.0648\n",
      "Batch 6, G1 Loss: 1.4229, G2 Loss: 0.9421, D1 Loss: 1.1599, D2 Loss: 0.8444\n",
      "Batch 7, G1 Loss: 0.8976, G2 Loss: 7.2038, D1 Loss: 1.2695, D2 Loss: 0.8034\n",
      "Batch 8, G1 Loss: 1.2057, G2 Loss: 4.2363, D1 Loss: 1.1834, D2 Loss: 0.1665\n",
      "Batch 9, G1 Loss: 0.9803, G2 Loss: 1.7954, D1 Loss: 1.2036, D2 Loss: 0.3414\n",
      "Batch 10, G1 Loss: 1.2502, G2 Loss: 2.9746, D1 Loss: 1.2409, D2 Loss: 0.3128\n",
      "Batch 11, G1 Loss: 0.6887, G2 Loss: 2.8087, D1 Loss: 1.4598, D2 Loss: 0.2684\n",
      "Batch 12, G1 Loss: 2.0365, G2 Loss: 2.2415, D1 Loss: 1.7784, D2 Loss: 0.3586\n",
      "Batch 13, G1 Loss: 0.3517, G2 Loss: 2.8745, D1 Loss: 2.5361, D2 Loss: 0.2931\n",
      "Batch 14, G1 Loss: 2.1421, G2 Loss: 2.2553, D1 Loss: 1.6487, D2 Loss: 0.4163\n",
      "Batch 15, G1 Loss: 0.5846, G2 Loss: 2.1018, D1 Loss: 1.6938, D2 Loss: 0.3842\n",
      "Batch 16, G1 Loss: 1.0365, G2 Loss: 2.3580, D1 Loss: 1.5253, D2 Loss: 0.5567\n",
      "Batch 17, G1 Loss: 1.0149, G2 Loss: 1.4296, D1 Loss: 1.6973, D2 Loss: 0.5637\n",
      "Batch 18, G1 Loss: 0.6213, G2 Loss: 4.0765, D1 Loss: 1.8054, D2 Loss: 0.5926\n",
      "Batch 19, G1 Loss: 1.5357, G2 Loss: 4.7826, D1 Loss: 1.5334, D2 Loss: 0.2326\n",
      "Batch 20, G1 Loss: 0.5890, G2 Loss: 0.7517, D1 Loss: 1.7069, D2 Loss: 1.1979\n",
      "Batch 21, G1 Loss: 1.5617, G2 Loss: 8.0893, D1 Loss: 1.3172, D2 Loss: 0.7835\n",
      "Batch 22, G1 Loss: 0.7458, G2 Loss: 6.5100, D1 Loss: 1.4058, D2 Loss: 0.3280\n",
      "Batch 23, G1 Loss: 1.2091, G2 Loss: 3.7906, D1 Loss: 1.3298, D2 Loss: 0.4545\n",
      "Batch 24, G1 Loss: 0.9182, G2 Loss: 1.6566, D1 Loss: 1.3566, D2 Loss: 0.7493\n",
      "Batch 25, G1 Loss: 0.9377, G2 Loss: 3.2888, D1 Loss: 1.3317, D2 Loss: 0.4036\n",
      "Batch 26, G1 Loss: 1.1195, G2 Loss: 2.1896, D1 Loss: 1.2809, D2 Loss: 0.3634\n",
      "Batch 27, G1 Loss: 0.9018, G2 Loss: 1.8803, D1 Loss: 1.4152, D2 Loss: 0.5622\n",
      "Batch 28, G1 Loss: 0.9680, G2 Loss: 2.4553, D1 Loss: 1.3179, D2 Loss: 0.6804\n",
      "Batch 29, G1 Loss: 1.2321, G2 Loss: 2.0732, D1 Loss: 1.4018, D2 Loss: 0.3845\n",
      "Batch 30, G1 Loss: 0.7745, G2 Loss: 6.1610, D1 Loss: 1.4386, D2 Loss: 0.1822\n",
      "Batch 31, G1 Loss: 1.3694, G2 Loss: 1.3701, D1 Loss: 1.3352, D2 Loss: 0.8499\n",
      "Batch 32, G1 Loss: 0.8196, G2 Loss: 4.5145, D1 Loss: 1.3446, D2 Loss: 0.6107\n",
      "Epoch 19 Average -> G1 Loss: 1.0598742961883545, G2 Loss: 3.2607924938201904, D1 Loss: 1.461223840713501, D2 Loss: 0.4883507788181305\n",
      "Epoch 20/100\n",
      "Batch 1, G1 Loss: 1.1943, G2 Loss: 1.9142, D1 Loss: 1.2806, D2 Loss: 0.4833\n",
      "Batch 2, G1 Loss: 0.9341, G2 Loss: 2.7216, D1 Loss: 1.3461, D2 Loss: 0.4619\n",
      "Batch 3, G1 Loss: 0.9665, G2 Loss: 2.2797, D1 Loss: 1.2381, D2 Loss: 0.2259\n",
      "Batch 4, G1 Loss: 1.3106, G2 Loss: 2.4547, D1 Loss: 1.2935, D2 Loss: 0.3273\n",
      "Batch 5, G1 Loss: 0.7298, G2 Loss: 1.9650, D1 Loss: 1.3834, D2 Loss: 0.4417\n",
      "Batch 6, G1 Loss: 1.7799, G2 Loss: 2.5453, D1 Loss: 1.1924, D2 Loss: 0.5885\n",
      "Batch 7, G1 Loss: 0.5539, G2 Loss: 2.4429, D1 Loss: 1.6151, D2 Loss: 0.2186\n",
      "Batch 8, G1 Loss: 1.6714, G2 Loss: 4.5235, D1 Loss: 1.4907, D2 Loss: 0.0954\n",
      "Batch 9, G1 Loss: 0.6989, G2 Loss: 2.4617, D1 Loss: 1.4929, D2 Loss: 0.2278\n",
      "Batch 10, G1 Loss: 1.3171, G2 Loss: 3.3216, D1 Loss: 1.3841, D2 Loss: 0.2555\n",
      "Batch 11, G1 Loss: 0.8468, G2 Loss: 2.4524, D1 Loss: 1.3888, D2 Loss: 0.3320\n",
      "Batch 12, G1 Loss: 1.1978, G2 Loss: 2.0556, D1 Loss: 1.3661, D2 Loss: 0.6280\n",
      "Batch 13, G1 Loss: 0.8957, G2 Loss: 3.8475, D1 Loss: 1.3287, D2 Loss: 0.5694\n",
      "Batch 14, G1 Loss: 1.3337, G2 Loss: 1.6598, D1 Loss: 1.2002, D2 Loss: 0.7265\n",
      "Batch 15, G1 Loss: 0.8286, G2 Loss: 4.3966, D1 Loss: 1.2785, D2 Loss: 0.5125\n",
      "Batch 16, G1 Loss: 1.3775, G2 Loss: 2.5751, D1 Loss: 1.2598, D2 Loss: 0.3960\n",
      "Batch 17, G1 Loss: 0.7219, G2 Loss: 1.7614, D1 Loss: 1.4073, D2 Loss: 0.4910\n",
      "Batch 18, G1 Loss: 1.4508, G2 Loss: 4.4655, D1 Loss: 1.3764, D2 Loss: 0.6705\n",
      "Batch 19, G1 Loss: 0.6497, G2 Loss: 1.0862, D1 Loss: 1.5893, D2 Loss: 0.6316\n",
      "Batch 20, G1 Loss: 1.7895, G2 Loss: 6.1975, D1 Loss: 1.4948, D2 Loss: 0.3445\n",
      "Batch 21, G1 Loss: 0.5168, G2 Loss: 8.6955, D1 Loss: 1.8255, D2 Loss: 0.3786\n",
      "Batch 22, G1 Loss: 1.8761, G2 Loss: 1.4930, D1 Loss: 1.5500, D2 Loss: 0.5576\n",
      "Batch 23, G1 Loss: 0.5790, G2 Loss: 4.7904, D1 Loss: 1.6539, D2 Loss: 0.2300\n",
      "Batch 24, G1 Loss: 1.6033, G2 Loss: 2.9168, D1 Loss: 1.5366, D2 Loss: 0.3753\n",
      "Batch 25, G1 Loss: 0.6468, G2 Loss: 1.5537, D1 Loss: 1.5559, D2 Loss: 0.5032\n",
      "Batch 26, G1 Loss: 1.6934, G2 Loss: 4.6742, D1 Loss: 1.3639, D2 Loss: 0.3156\n",
      "Batch 27, G1 Loss: 0.7043, G2 Loss: 3.3316, D1 Loss: 1.3456, D2 Loss: 0.2680\n",
      "Batch 28, G1 Loss: 1.5211, G2 Loss: 1.6200, D1 Loss: 1.3044, D2 Loss: 0.4236\n",
      "Batch 29, G1 Loss: 0.7301, G2 Loss: 3.6249, D1 Loss: 1.3810, D2 Loss: 0.3622\n",
      "Batch 30, G1 Loss: 1.2447, G2 Loss: 2.2422, D1 Loss: 1.3760, D2 Loss: 0.4172\n",
      "Batch 31, G1 Loss: 0.7853, G2 Loss: 1.6625, D1 Loss: 1.4070, D2 Loss: 0.6595\n",
      "Batch 32, G1 Loss: 1.2988, G2 Loss: 3.6578, D1 Loss: 1.3054, D2 Loss: 0.2006\n",
      "Epoch 20 Average -> G1 Loss: 1.1077462434768677, G2 Loss: 3.0434486865997314, D1 Loss: 1.4066280126571655, D2 Loss: 0.4162271022796631\n",
      "Epoch 21/100\n",
      "Batch 1, G1 Loss: 0.7758, G2 Loss: 3.1146, D1 Loss: 1.4324, D2 Loss: 0.2597\n",
      "Batch 2, G1 Loss: 1.2504, G2 Loss: 1.8876, D1 Loss: 1.4383, D2 Loss: 0.4321\n",
      "Batch 3, G1 Loss: 0.7081, G2 Loss: 4.3363, D1 Loss: 1.5680, D2 Loss: 0.3977\n",
      "Batch 4, G1 Loss: 1.3709, G2 Loss: 1.9929, D1 Loss: 1.3415, D2 Loss: 0.4426\n",
      "Batch 5, G1 Loss: 0.7986, G2 Loss: 3.0950, D1 Loss: 1.2754, D2 Loss: 0.4579\n",
      "Batch 6, G1 Loss: 1.3756, G2 Loss: 2.2176, D1 Loss: 1.2075, D2 Loss: 0.4023\n",
      "Batch 7, G1 Loss: 0.8335, G2 Loss: 2.2597, D1 Loss: 1.3910, D2 Loss: 0.6243\n",
      "Batch 8, G1 Loss: 1.1129, G2 Loss: 2.8122, D1 Loss: 1.2810, D2 Loss: 0.2930\n",
      "Batch 9, G1 Loss: 1.0189, G2 Loss: 4.2766, D1 Loss: 1.3418, D2 Loss: 0.5160\n",
      "Batch 10, G1 Loss: 0.8755, G2 Loss: 1.6568, D1 Loss: 1.3407, D2 Loss: 0.4572\n",
      "Batch 11, G1 Loss: 1.3714, G2 Loss: 2.5727, D1 Loss: 1.2245, D2 Loss: 0.3389\n",
      "Batch 12, G1 Loss: 0.8525, G2 Loss: 4.4471, D1 Loss: 1.2673, D2 Loss: 0.1160\n",
      "Batch 13, G1 Loss: 1.5840, G2 Loss: 3.2567, D1 Loss: 1.3481, D2 Loss: 0.4017\n",
      "Batch 14, G1 Loss: 0.7035, G2 Loss: 1.3766, D1 Loss: 1.3866, D2 Loss: 0.6333\n",
      "Batch 15, G1 Loss: 1.3373, G2 Loss: 6.0277, D1 Loss: 1.4658, D2 Loss: 0.6596\n",
      "Batch 16, G1 Loss: 0.8053, G2 Loss: 4.8081, D1 Loss: 1.4338, D2 Loss: 0.0859\n",
      "Batch 17, G1 Loss: 1.0926, G2 Loss: 3.7020, D1 Loss: 1.4655, D2 Loss: 0.0961\n",
      "Batch 18, G1 Loss: 1.0077, G2 Loss: 1.5554, D1 Loss: 1.4124, D2 Loss: 0.5939\n",
      "Batch 19, G1 Loss: 1.0574, G2 Loss: 4.8214, D1 Loss: 1.3597, D2 Loss: 0.3528\n",
      "Batch 20, G1 Loss: 1.0192, G2 Loss: 3.3031, D1 Loss: 1.3742, D2 Loss: 0.2823\n",
      "Batch 21, G1 Loss: 0.8279, G2 Loss: 2.6816, D1 Loss: 1.4324, D2 Loss: 0.3094\n",
      "Batch 22, G1 Loss: 1.3078, G2 Loss: 2.6022, D1 Loss: 1.3832, D2 Loss: 0.3781\n",
      "Batch 23, G1 Loss: 0.7457, G2 Loss: 2.2858, D1 Loss: 1.5096, D2 Loss: 0.7210\n",
      "Batch 24, G1 Loss: 1.3237, G2 Loss: 2.2608, D1 Loss: 1.5415, D2 Loss: 0.3052\n",
      "Batch 25, G1 Loss: 0.7712, G2 Loss: 5.0445, D1 Loss: 1.5105, D2 Loss: 0.8565\n",
      "Batch 26, G1 Loss: 1.1401, G2 Loss: 3.0355, D1 Loss: 1.2825, D2 Loss: 0.2494\n",
      "Batch 27, G1 Loss: 1.1352, G2 Loss: 1.8189, D1 Loss: 1.2963, D2 Loss: 0.5112\n",
      "Batch 28, G1 Loss: 0.9297, G2 Loss: 6.1919, D1 Loss: 1.2354, D2 Loss: 1.2427\n",
      "Batch 29, G1 Loss: 1.2696, G2 Loss: 0.6990, D1 Loss: 1.2354, D2 Loss: 1.3150\n",
      "Batch 30, G1 Loss: 0.8590, G2 Loss: 8.5856, D1 Loss: 1.2600, D2 Loss: 1.4407\n",
      "Batch 31, G1 Loss: 1.4824, G2 Loss: 4.2035, D1 Loss: 1.0616, D2 Loss: 0.3678\n",
      "Batch 32, G1 Loss: 0.9184, G2 Loss: 1.8466, D1 Loss: 1.2282, D2 Loss: 0.6209\n",
      "Epoch 21 Average -> G1 Loss: 1.0519323348999023, G2 Loss: 3.2742483615875244, D1 Loss: 1.354122519493103, D2 Loss: 0.5050381422042847\n",
      "Epoch 22/100\n",
      "Batch 1, G1 Loss: 1.1425, G2 Loss: 2.6417, D1 Loss: 1.1509, D2 Loss: 0.2844\n",
      "Batch 2, G1 Loss: 1.1545, G2 Loss: 4.0029, D1 Loss: 1.1754, D2 Loss: 0.3133\n",
      "Batch 3, G1 Loss: 0.9488, G2 Loss: 3.0268, D1 Loss: 1.2956, D2 Loss: 0.2257\n",
      "Batch 4, G1 Loss: 1.2708, G2 Loss: 2.1756, D1 Loss: 1.2672, D2 Loss: 0.3458\n",
      "Batch 5, G1 Loss: 0.8054, G2 Loss: 2.2348, D1 Loss: 1.5473, D2 Loss: 0.5118\n",
      "Batch 6, G1 Loss: 1.6963, G2 Loss: 2.4550, D1 Loss: 1.5817, D2 Loss: 0.3365\n",
      "Batch 7, G1 Loss: 0.5032, G2 Loss: 2.7123, D1 Loss: 1.9389, D2 Loss: 0.3435\n",
      "Batch 8, G1 Loss: 2.2497, G2 Loss: 2.1533, D1 Loss: 1.8120, D2 Loss: 0.3161\n",
      "Batch 9, G1 Loss: 0.5893, G2 Loss: 3.1978, D1 Loss: 1.7272, D2 Loss: 0.6617\n",
      "Batch 10, G1 Loss: 1.3048, G2 Loss: 2.8769, D1 Loss: 1.4503, D2 Loss: 0.1483\n",
      "Batch 11, G1 Loss: 1.0119, G2 Loss: 6.7908, D1 Loss: 1.3597, D2 Loss: 0.0727\n",
      "Batch 12, G1 Loss: 0.9679, G2 Loss: 1.9190, D1 Loss: 1.3208, D2 Loss: 0.4565\n",
      "Batch 13, G1 Loss: 1.0875, G2 Loss: 4.9943, D1 Loss: 1.5144, D2 Loss: 0.1559\n",
      "Batch 14, G1 Loss: 0.9000, G2 Loss: 4.7706, D1 Loss: 1.4346, D2 Loss: 0.5935\n",
      "Batch 15, G1 Loss: 1.2217, G2 Loss: 2.5584, D1 Loss: 1.3589, D2 Loss: 0.6027\n",
      "Batch 16, G1 Loss: 0.6973, G2 Loss: 2.1340, D1 Loss: 1.4954, D2 Loss: 0.3929\n",
      "Batch 17, G1 Loss: 1.7731, G2 Loss: 5.5269, D1 Loss: 1.4459, D2 Loss: 0.6398\n",
      "Batch 18, G1 Loss: 0.5166, G2 Loss: 1.6745, D1 Loss: 1.7531, D2 Loss: 0.4114\n",
      "Batch 19, G1 Loss: 1.4924, G2 Loss: 4.3254, D1 Loss: 1.7305, D2 Loss: 0.2027\n",
      "Batch 20, G1 Loss: 0.6095, G2 Loss: 3.0351, D1 Loss: 1.6341, D2 Loss: 0.3236\n",
      "Batch 21, G1 Loss: 1.4556, G2 Loss: 1.0939, D1 Loss: 1.6254, D2 Loss: 0.7309\n",
      "Batch 22, G1 Loss: 0.6902, G2 Loss: 7.4608, D1 Loss: 1.5806, D2 Loss: 0.5631\n",
      "Batch 23, G1 Loss: 1.1881, G2 Loss: 5.2175, D1 Loss: 1.4489, D2 Loss: 0.4181\n",
      "Batch 24, G1 Loss: 0.8547, G2 Loss: 6.6529, D1 Loss: 1.4449, D2 Loss: 0.0673\n",
      "Batch 25, G1 Loss: 1.1180, G2 Loss: 1.5382, D1 Loss: 1.3551, D2 Loss: 0.5156\n",
      "Batch 26, G1 Loss: 0.9957, G2 Loss: 1.7734, D1 Loss: 1.2736, D2 Loss: 0.3673\n",
      "Batch 27, G1 Loss: 1.0125, G2 Loss: 5.0615, D1 Loss: 1.2230, D2 Loss: 0.3097\n",
      "Batch 28, G1 Loss: 1.2342, G2 Loss: 4.4118, D1 Loss: 1.2381, D2 Loss: 0.3303\n",
      "Batch 29, G1 Loss: 0.8183, G2 Loss: 2.1085, D1 Loss: 1.3244, D2 Loss: 0.3947\n",
      "Batch 30, G1 Loss: 1.3055, G2 Loss: 2.6410, D1 Loss: 1.2685, D2 Loss: 0.1942\n",
      "Batch 31, G1 Loss: 0.8761, G2 Loss: 4.2641, D1 Loss: 1.3111, D2 Loss: 0.3655\n",
      "Batch 32, G1 Loss: 1.1169, G2 Loss: 1.9769, D1 Loss: 1.3291, D2 Loss: 0.4150\n",
      "Epoch 22 Average -> G1 Loss: 1.0815324783325195, G2 Loss: 3.4189577102661133, D1 Loss: 1.4505105018615723, D2 Loss: 0.3753214180469513\n",
      "Epoch 23/100\n",
      "Batch 1, G1 Loss: 0.9317, G2 Loss: 3.7442, D1 Loss: 1.2856, D2 Loss: 0.1411\n",
      "Batch 2, G1 Loss: 1.2494, G2 Loss: 3.5710, D1 Loss: 1.2892, D2 Loss: 0.2857\n",
      "Batch 3, G1 Loss: 0.8127, G2 Loss: 1.8505, D1 Loss: 1.4294, D2 Loss: 0.3000\n",
      "Batch 4, G1 Loss: 1.3750, G2 Loss: 3.0186, D1 Loss: 1.2429, D2 Loss: 0.1532\n",
      "Batch 5, G1 Loss: 0.8651, G2 Loss: 2.5391, D1 Loss: 1.3414, D2 Loss: 0.7194\n",
      "Batch 6, G1 Loss: 1.0683, G2 Loss: 1.7592, D1 Loss: 1.2166, D2 Loss: 0.3332\n",
      "Batch 7, G1 Loss: 1.3220, G2 Loss: 6.6783, D1 Loss: 1.3639, D2 Loss: 0.3433\n",
      "Batch 8, G1 Loss: 0.7262, G2 Loss: 3.2616, D1 Loss: 1.4708, D2 Loss: 0.1879\n",
      "Batch 9, G1 Loss: 1.3514, G2 Loss: 1.2741, D1 Loss: 1.2402, D2 Loss: 0.7747\n",
      "Batch 10, G1 Loss: 0.9113, G2 Loss: 7.9404, D1 Loss: 1.3518, D2 Loss: 0.7732\n",
      "Batch 11, G1 Loss: 0.8781, G2 Loss: 5.5427, D1 Loss: 1.4570, D2 Loss: 0.2394\n",
      "Batch 12, G1 Loss: 1.2017, G2 Loss: 3.4989, D1 Loss: 1.3844, D2 Loss: 0.1770\n",
      "Batch 13, G1 Loss: 0.7984, G2 Loss: 1.4425, D1 Loss: 1.5657, D2 Loss: 0.5538\n",
      "Batch 14, G1 Loss: 1.2158, G2 Loss: 4.4450, D1 Loss: 1.3119, D2 Loss: 0.2111\n",
      "Batch 15, G1 Loss: 0.7917, G2 Loss: 3.5838, D1 Loss: 1.3896, D2 Loss: 0.4584\n",
      "Batch 16, G1 Loss: 1.5643, G2 Loss: 2.0351, D1 Loss: 1.3265, D2 Loss: 0.3457\n",
      "Batch 17, G1 Loss: 0.5072, G2 Loss: 2.9540, D1 Loss: 1.7927, D2 Loss: 0.4075\n",
      "Batch 18, G1 Loss: 2.2011, G2 Loss: 2.0636, D1 Loss: 1.8443, D2 Loss: 0.3244\n",
      "Batch 19, G1 Loss: 0.6658, G2 Loss: 3.3380, D1 Loss: 1.6200, D2 Loss: 0.2347\n",
      "Batch 20, G1 Loss: 1.4574, G2 Loss: 3.4852, D1 Loss: 1.5467, D2 Loss: 0.4553\n",
      "Batch 21, G1 Loss: 0.8192, G2 Loss: 2.2599, D1 Loss: 1.5584, D2 Loss: 0.3648\n",
      "Batch 22, G1 Loss: 0.9910, G2 Loss: 5.2827, D1 Loss: 1.4281, D2 Loss: 0.2915\n",
      "Batch 23, G1 Loss: 1.0817, G2 Loss: 2.3307, D1 Loss: 1.4605, D2 Loss: 0.4161\n",
      "Batch 24, G1 Loss: 0.9944, G2 Loss: 3.6227, D1 Loss: 1.4677, D2 Loss: 0.2347\n",
      "Batch 25, G1 Loss: 1.0138, G2 Loss: 3.1169, D1 Loss: 1.2667, D2 Loss: 0.3573\n",
      "Batch 26, G1 Loss: 1.5074, G2 Loss: 2.9899, D1 Loss: 1.1738, D2 Loss: 0.3094\n",
      "Batch 27, G1 Loss: 0.7155, G2 Loss: 3.6143, D1 Loss: 1.3624, D2 Loss: 0.2870\n",
      "Batch 28, G1 Loss: 2.1393, G2 Loss: 2.6725, D1 Loss: 1.4337, D2 Loss: 0.4381\n",
      "Batch 29, G1 Loss: 0.5051, G2 Loss: 4.9329, D1 Loss: 1.8561, D2 Loss: 0.4061\n",
      "Batch 30, G1 Loss: 1.5304, G2 Loss: 1.9441, D1 Loss: 1.6613, D2 Loss: 0.4232\n",
      "Batch 31, G1 Loss: 0.6857, G2 Loss: 3.8229, D1 Loss: 1.5973, D2 Loss: 0.2655\n",
      "Batch 32, G1 Loss: 1.1970, G2 Loss: 2.9872, D1 Loss: 1.4842, D2 Loss: 0.2615\n",
      "Epoch 23 Average -> G1 Loss: 1.0960921049118042, G2 Loss: 3.3625760078430176, D1 Loss: 1.444400429725647, D2 Loss: 0.35857465863227844\n",
      "Epoch 24/100\n",
      "Batch 1, G1 Loss: 0.8708, G2 Loss: 2.1445, D1 Loss: 1.4358, D2 Loss: 0.3138\n",
      "Batch 2, G1 Loss: 1.1354, G2 Loss: 4.2509, D1 Loss: 1.4500, D2 Loss: 0.4667\n",
      "Batch 3, G1 Loss: 0.7975, G2 Loss: 1.2039, D1 Loss: 1.4960, D2 Loss: 0.7271\n",
      "Batch 4, G1 Loss: 1.3141, G2 Loss: 8.0492, D1 Loss: 1.4144, D2 Loss: 1.1848\n",
      "Batch 5, G1 Loss: 0.8105, G2 Loss: 3.5232, D1 Loss: 1.2920, D2 Loss: 0.1041\n",
      "Batch 6, G1 Loss: 1.4920, G2 Loss: 1.5333, D1 Loss: 1.2044, D2 Loss: 0.4648\n",
      "Batch 7, G1 Loss: 0.9508, G2 Loss: 4.6240, D1 Loss: 1.2100, D2 Loss: 0.5201\n",
      "Batch 8, G1 Loss: 1.3711, G2 Loss: 3.7186, D1 Loss: 1.2056, D2 Loss: 0.2410\n",
      "Batch 9, G1 Loss: 0.8809, G2 Loss: 2.0304, D1 Loss: 1.3470, D2 Loss: 0.2876\n",
      "Batch 10, G1 Loss: 1.2444, G2 Loss: 2.6082, D1 Loss: 1.2718, D2 Loss: 0.2422\n",
      "Batch 11, G1 Loss: 0.8742, G2 Loss: 3.6199, D1 Loss: 1.3421, D2 Loss: 0.2019\n",
      "Batch 12, G1 Loss: 1.3530, G2 Loss: 2.4492, D1 Loss: 1.2324, D2 Loss: 0.3549\n",
      "Batch 13, G1 Loss: 0.8922, G2 Loss: 2.5518, D1 Loss: 1.3823, D2 Loss: 0.4123\n",
      "Batch 14, G1 Loss: 1.0271, G2 Loss: 2.3476, D1 Loss: 1.3049, D2 Loss: 0.1974\n",
      "Batch 15, G1 Loss: 1.1936, G2 Loss: 3.5808, D1 Loss: 1.5008, D2 Loss: 0.3853\n",
      "Batch 16, G1 Loss: 0.7562, G2 Loss: 4.1371, D1 Loss: 1.4447, D2 Loss: 0.5338\n",
      "Batch 17, G1 Loss: 1.3816, G2 Loss: 0.8788, D1 Loss: 1.1360, D2 Loss: 1.0373\n",
      "Batch 18, G1 Loss: 0.9274, G2 Loss: 8.9511, D1 Loss: 1.3990, D2 Loss: 0.5501\n",
      "Batch 19, G1 Loss: 1.0246, G2 Loss: 8.5541, D1 Loss: 1.4987, D2 Loss: 0.4225\n",
      "Batch 20, G1 Loss: 0.8993, G2 Loss: 3.9451, D1 Loss: 1.4698, D2 Loss: 0.2704\n",
      "Batch 21, G1 Loss: 1.0830, G2 Loss: 3.2073, D1 Loss: 1.4017, D2 Loss: 0.2311\n",
      "Batch 22, G1 Loss: 0.9131, G2 Loss: 3.0165, D1 Loss: 1.3203, D2 Loss: 0.3836\n",
      "Batch 23, G1 Loss: 1.2459, G2 Loss: 3.0226, D1 Loss: 1.2601, D2 Loss: 0.2348\n",
      "Batch 24, G1 Loss: 0.8229, G2 Loss: 3.4969, D1 Loss: 1.5699, D2 Loss: 0.2398\n",
      "Batch 25, G1 Loss: 1.0594, G2 Loss: 2.5441, D1 Loss: 1.6071, D2 Loss: 0.4339\n",
      "Batch 26, G1 Loss: 0.8793, G2 Loss: 1.6622, D1 Loss: 1.4590, D2 Loss: 0.4732\n",
      "Batch 27, G1 Loss: 1.2062, G2 Loss: 5.0053, D1 Loss: 1.4032, D2 Loss: 0.7263\n",
      "Batch 28, G1 Loss: 0.8396, G2 Loss: 2.1594, D1 Loss: 1.3389, D2 Loss: 0.5320\n",
      "Batch 29, G1 Loss: 1.4075, G2 Loss: 2.3113, D1 Loss: 1.3193, D2 Loss: 0.3182\n",
      "Batch 30, G1 Loss: 0.7991, G2 Loss: 3.8334, D1 Loss: 1.2705, D2 Loss: 0.2375\n",
      "Batch 31, G1 Loss: 1.7353, G2 Loss: 3.3477, D1 Loss: 1.1743, D2 Loss: 0.5049\n",
      "Batch 32, G1 Loss: 0.8411, G2 Loss: 2.2226, D1 Loss: 1.2637, D2 Loss: 0.5528\n",
      "Epoch 24 Average -> G1 Loss: 1.0634064674377441, G2 Loss: 3.454092025756836, D1 Loss: 1.357057809829712, D2 Loss: 0.43082085251808167\n",
      "Epoch 25/100\n",
      "Batch 1, G1 Loss: 1.2867, G2 Loss: 2.0906, D1 Loss: 1.0324, D2 Loss: 0.3826\n",
      "Batch 2, G1 Loss: 1.3001, G2 Loss: 6.9910, D1 Loss: 1.1419, D2 Loss: 0.9341\n",
      "Batch 3, G1 Loss: 1.0261, G2 Loss: 2.4183, D1 Loss: 1.1343, D2 Loss: 0.2012\n",
      "Batch 4, G1 Loss: 1.1287, G2 Loss: 1.8656, D1 Loss: 1.1233, D2 Loss: 0.3791\n",
      "Batch 5, G1 Loss: 1.1272, G2 Loss: 5.4885, D1 Loss: 1.1614, D2 Loss: 0.6371\n",
      "Batch 6, G1 Loss: 0.9237, G2 Loss: 1.6230, D1 Loss: 1.5349, D2 Loss: 0.4494\n",
      "Batch 7, G1 Loss: 0.8852, G2 Loss: 4.4667, D1 Loss: 1.6675, D2 Loss: 0.3423\n",
      "Batch 8, G1 Loss: 1.1203, G2 Loss: 3.8880, D1 Loss: 1.5802, D2 Loss: 0.5462\n",
      "Batch 9, G1 Loss: 0.7365, G2 Loss: 5.5284, D1 Loss: 1.4523, D2 Loss: 0.0965\n",
      "Batch 10, G1 Loss: 2.2693, G2 Loss: 0.5671, D1 Loss: 1.3465, D2 Loss: 1.7667\n",
      "Batch 11, G1 Loss: 0.3270, G2 Loss: 8.4475, D1 Loss: 2.8845, D2 Loss: 1.1795\n",
      "Batch 12, G1 Loss: 1.9115, G2 Loss: 6.2173, D1 Loss: 1.7274, D2 Loss: 0.6233\n",
      "Batch 13, G1 Loss: 0.7798, G2 Loss: 2.7903, D1 Loss: 1.6148, D2 Loss: 0.3001\n",
      "Batch 14, G1 Loss: 1.0547, G2 Loss: 2.0353, D1 Loss: 1.5644, D2 Loss: 0.4557\n",
      "Batch 15, G1 Loss: 1.2142, G2 Loss: 5.0457, D1 Loss: 1.3965, D2 Loss: 0.4505\n",
      "Batch 16, G1 Loss: 0.8694, G2 Loss: 2.9685, D1 Loss: 1.3482, D2 Loss: 0.1791\n",
      "Batch 17, G1 Loss: 1.4426, G2 Loss: 2.3470, D1 Loss: 1.2399, D2 Loss: 0.3298\n",
      "Batch 18, G1 Loss: 0.7777, G2 Loss: 2.8457, D1 Loss: 1.3261, D2 Loss: 0.5986\n",
      "Batch 19, G1 Loss: 1.2807, G2 Loss: 1.9194, D1 Loss: 1.3223, D2 Loss: 0.4188\n",
      "Batch 20, G1 Loss: 0.9742, G2 Loss: 2.3655, D1 Loss: 1.4007, D2 Loss: 0.4336\n",
      "Batch 21, G1 Loss: 0.8502, G2 Loss: 4.1586, D1 Loss: 1.4455, D2 Loss: 0.2938\n",
      "Batch 22, G1 Loss: 1.4929, G2 Loss: 3.2338, D1 Loss: 1.4936, D2 Loss: 0.1986\n",
      "Batch 23, G1 Loss: 0.7333, G2 Loss: 2.2641, D1 Loss: 1.6108, D2 Loss: 0.3826\n",
      "Batch 24, G1 Loss: 1.1959, G2 Loss: 4.9412, D1 Loss: 1.3253, D2 Loss: 0.3154\n",
      "Batch 25, G1 Loss: 0.9925, G2 Loss: 3.3385, D1 Loss: 1.3197, D2 Loss: 0.2936\n",
      "Batch 26, G1 Loss: 1.0212, G2 Loss: 2.2415, D1 Loss: 1.3506, D2 Loss: 0.2718\n",
      "Batch 27, G1 Loss: 0.9829, G2 Loss: 3.1131, D1 Loss: 1.3350, D2 Loss: 0.2786\n",
      "Batch 28, G1 Loss: 1.0610, G2 Loss: 3.1067, D1 Loss: 1.3610, D2 Loss: 0.2007\n",
      "Batch 29, G1 Loss: 0.9564, G2 Loss: 2.8407, D1 Loss: 1.3364, D2 Loss: 0.1600\n",
      "Batch 30, G1 Loss: 1.1025, G2 Loss: 4.5453, D1 Loss: 1.3393, D2 Loss: 0.2575\n",
      "Batch 31, G1 Loss: 0.9381, G2 Loss: 1.5728, D1 Loss: 1.3706, D2 Loss: 0.5670\n",
      "Batch 32, G1 Loss: 1.0635, G2 Loss: 4.1499, D1 Loss: 1.2914, D2 Loss: 0.6286\n",
      "Epoch 25 Average -> G1 Loss: 1.0883206129074097, G2 Loss: 3.481739044189453, D1 Loss: 1.4243288040161133, D2 Loss: 0.4547678828239441\n",
      "Epoch 26/100\n",
      "Batch 1, G1 Loss: 1.0042, G2 Loss: 2.9837, D1 Loss: 1.3103, D2 Loss: 0.3981\n",
      "Batch 2, G1 Loss: 0.9835, G2 Loss: 2.0077, D1 Loss: 1.3758, D2 Loss: 0.3650\n",
      "Batch 3, G1 Loss: 0.9691, G2 Loss: 5.4904, D1 Loss: 1.3598, D2 Loss: 0.5378\n",
      "Batch 4, G1 Loss: 1.1742, G2 Loss: 4.3821, D1 Loss: 1.4252, D2 Loss: 0.2367\n",
      "Batch 5, G1 Loss: 0.8048, G2 Loss: 1.7211, D1 Loss: 1.4942, D2 Loss: 0.4062\n",
      "Batch 6, G1 Loss: 1.1828, G2 Loss: 4.8957, D1 Loss: 1.4650, D2 Loss: 0.1078\n",
      "Batch 7, G1 Loss: 0.8751, G2 Loss: 4.6368, D1 Loss: 1.3806, D2 Loss: 0.2956\n",
      "Batch 8, G1 Loss: 1.3181, G2 Loss: 6.6479, D1 Loss: 1.3667, D2 Loss: 0.2276\n",
      "Batch 9, G1 Loss: 0.7607, G2 Loss: 0.7407, D1 Loss: 1.5145, D2 Loss: 1.2636\n",
      "Batch 10, G1 Loss: 1.7250, G2 Loss: 11.3116, D1 Loss: 1.1377, D2 Loss: 1.1188\n",
      "Batch 11, G1 Loss: 0.6729, G2 Loss: 7.7861, D1 Loss: 1.3557, D2 Loss: 0.2397\n",
      "Batch 12, G1 Loss: 1.9043, G2 Loss: 3.7649, D1 Loss: 1.1617, D2 Loss: 0.1742\n",
      "Batch 13, G1 Loss: 0.6825, G2 Loss: 1.7928, D1 Loss: 1.4864, D2 Loss: 0.4413\n",
      "Batch 14, G1 Loss: 1.3657, G2 Loss: 3.5552, D1 Loss: 1.6176, D2 Loss: 0.3841\n",
      "Batch 15, G1 Loss: 0.6967, G2 Loss: 2.8182, D1 Loss: 1.5557, D2 Loss: 0.4139\n",
      "Batch 16, G1 Loss: 1.4225, G2 Loss: 1.6301, D1 Loss: 1.4812, D2 Loss: 0.5294\n",
      "Batch 17, G1 Loss: 0.7125, G2 Loss: 4.0309, D1 Loss: 1.5146, D2 Loss: 0.2947\n",
      "Batch 18, G1 Loss: 1.5341, G2 Loss: 5.5326, D1 Loss: 1.4469, D2 Loss: 0.1949\n",
      "Batch 19, G1 Loss: 0.6758, G2 Loss: 2.4802, D1 Loss: 1.7368, D2 Loss: 0.3237\n",
      "Batch 20, G1 Loss: 1.3698, G2 Loss: 2.2588, D1 Loss: 1.3486, D2 Loss: 0.4253\n",
      "Batch 21, G1 Loss: 0.8690, G2 Loss: 5.1514, D1 Loss: 1.4066, D2 Loss: 0.5491\n",
      "Batch 22, G1 Loss: 1.0395, G2 Loss: 2.8712, D1 Loss: 1.2857, D2 Loss: 0.3371\n",
      "Batch 23, G1 Loss: 1.1553, G2 Loss: 2.0164, D1 Loss: 1.2564, D2 Loss: 0.4097\n",
      "Batch 24, G1 Loss: 0.7078, G2 Loss: 3.6386, D1 Loss: 1.4583, D2 Loss: 0.3100\n",
      "Batch 25, G1 Loss: 1.6677, G2 Loss: 2.4984, D1 Loss: 1.4978, D2 Loss: 0.3946\n",
      "Batch 26, G1 Loss: 0.5290, G2 Loss: 1.5580, D1 Loss: 1.6916, D2 Loss: 0.4724\n",
      "Batch 27, G1 Loss: 1.5445, G2 Loss: 7.2098, D1 Loss: 1.4661, D2 Loss: 0.4560\n",
      "Batch 28, G1 Loss: 0.7238, G2 Loss: 5.6052, D1 Loss: 1.4774, D2 Loss: 0.1460\n",
      "Batch 29, G1 Loss: 1.3032, G2 Loss: 0.8747, D1 Loss: 1.2122, D2 Loss: 1.1124\n",
      "Batch 30, G1 Loss: 1.0073, G2 Loss: 11.2078, D1 Loss: 1.2539, D2 Loss: 2.1321\n",
      "Batch 31, G1 Loss: 0.9681, G2 Loss: 6.6869, D1 Loss: 1.3130, D2 Loss: 0.4766\n",
      "Batch 32, G1 Loss: 1.1848, G2 Loss: 3.0697, D1 Loss: 1.3398, D2 Loss: 0.5283\n",
      "Epoch 26 Average -> G1 Loss: 1.0791913270950317, G2 Loss: 4.151731014251709, D1 Loss: 1.4122987985610962, D2 Loss: 0.4907059073448181\n",
      "Epoch 27/100\n",
      "Batch 1, G1 Loss: 0.8810, G2 Loss: 3.0556, D1 Loss: 1.2879, D2 Loss: 0.2380\n",
      "Batch 2, G1 Loss: 1.1991, G2 Loss: 3.4884, D1 Loss: 1.3440, D2 Loss: 0.2603\n",
      "Batch 3, G1 Loss: 0.9396, G2 Loss: 2.3029, D1 Loss: 1.2808, D2 Loss: 0.3359\n",
      "Batch 4, G1 Loss: 0.9813, G2 Loss: 3.3118, D1 Loss: 1.2196, D2 Loss: 0.3857\n",
      "Batch 5, G1 Loss: 1.2653, G2 Loss: 2.5873, D1 Loss: 1.2694, D2 Loss: 0.2847\n",
      "Batch 6, G1 Loss: 0.8448, G2 Loss: 2.7183, D1 Loss: 1.3372, D2 Loss: 0.2816\n",
      "Batch 7, G1 Loss: 1.2339, G2 Loss: 2.1472, D1 Loss: 1.2460, D2 Loss: 0.4559\n",
      "Batch 8, G1 Loss: 0.9508, G2 Loss: 4.9946, D1 Loss: 1.3112, D2 Loss: 0.3402\n",
      "Batch 9, G1 Loss: 1.1120, G2 Loss: 4.9489, D1 Loss: 1.3131, D2 Loss: 0.0570\n",
      "Batch 10, G1 Loss: 1.0503, G2 Loss: 1.5140, D1 Loss: 1.4017, D2 Loss: 0.5848\n",
      "Batch 11, G1 Loss: 0.8021, G2 Loss: 4.1847, D1 Loss: 1.4258, D2 Loss: 0.1499\n",
      "Batch 12, G1 Loss: 1.4455, G2 Loss: 3.7761, D1 Loss: 1.3776, D2 Loss: 0.3585\n",
      "Batch 13, G1 Loss: 0.5782, G2 Loss: 3.1842, D1 Loss: 1.7646, D2 Loss: 0.4013\n",
      "Batch 14, G1 Loss: 1.6215, G2 Loss: 1.7029, D1 Loss: 1.5133, D2 Loss: 0.6218\n",
      "Batch 15, G1 Loss: 0.6244, G2 Loss: 5.2720, D1 Loss: 1.6597, D2 Loss: 0.7070\n",
      "Batch 16, G1 Loss: 1.6746, G2 Loss: 1.7745, D1 Loss: 1.4924, D2 Loss: 0.4044\n",
      "Batch 17, G1 Loss: 0.6815, G2 Loss: 2.4292, D1 Loss: 1.4842, D2 Loss: 0.3093\n",
      "Batch 18, G1 Loss: 1.4489, G2 Loss: 3.3617, D1 Loss: 1.2877, D2 Loss: 0.1886\n",
      "Batch 19, G1 Loss: 0.8959, G2 Loss: 4.5057, D1 Loss: 1.2522, D2 Loss: 0.1162\n",
      "Batch 20, G1 Loss: 1.1831, G2 Loss: 1.9931, D1 Loss: 1.2581, D2 Loss: 0.4230\n",
      "Batch 21, G1 Loss: 0.9121, G2 Loss: 4.0748, D1 Loss: 1.3371, D2 Loss: 0.1829\n",
      "Batch 22, G1 Loss: 1.3614, G2 Loss: 3.8716, D1 Loss: 1.3427, D2 Loss: 0.3304\n",
      "Batch 23, G1 Loss: 0.8578, G2 Loss: 1.7472, D1 Loss: 1.2501, D2 Loss: 0.4334\n",
      "Batch 24, G1 Loss: 1.5539, G2 Loss: 4.9238, D1 Loss: 1.1441, D2 Loss: 0.7017\n",
      "Batch 25, G1 Loss: 0.6791, G2 Loss: 1.7434, D1 Loss: 1.4239, D2 Loss: 0.5252\n",
      "Batch 26, G1 Loss: 1.9513, G2 Loss: 4.6290, D1 Loss: 1.3167, D2 Loss: 0.1578\n",
      "Batch 27, G1 Loss: 0.5296, G2 Loss: 4.6668, D1 Loss: 1.7596, D2 Loss: 0.5141\n",
      "Batch 28, G1 Loss: 1.8529, G2 Loss: 1.7495, D1 Loss: 1.3860, D2 Loss: 0.5358\n",
      "Batch 29, G1 Loss: 0.5163, G2 Loss: 2.6810, D1 Loss: 1.8751, D2 Loss: 0.3626\n",
      "Batch 30, G1 Loss: 1.5762, G2 Loss: 3.0021, D1 Loss: 1.6888, D2 Loss: 0.5258\n",
      "Batch 31, G1 Loss: 0.6639, G2 Loss: 1.4611, D1 Loss: 1.6018, D2 Loss: 0.5078\n",
      "Batch 32, G1 Loss: 1.1318, G2 Loss: 6.5333, D1 Loss: 1.3651, D2 Loss: 0.3414\n",
      "Epoch 27 Average -> G1 Loss: 1.0937552452087402, G2 Loss: 3.260516881942749, D1 Loss: 1.4067968130111694, D2 Loss: 0.37571981549263\n",
      "Epoch 28/100\n",
      "Batch 1, G1 Loss: 0.9860, G2 Loss: 6.3342, D1 Loss: 1.3471, D2 Loss: 0.1145\n",
      "Batch 2, G1 Loss: 0.9796, G2 Loss: 2.2606, D1 Loss: 1.3099, D2 Loss: 0.2198\n",
      "Batch 3, G1 Loss: 0.9971, G2 Loss: 3.3349, D1 Loss: 1.3510, D2 Loss: 0.1028\n",
      "Batch 4, G1 Loss: 1.0395, G2 Loss: 4.0081, D1 Loss: 1.2226, D2 Loss: 0.2765\n",
      "Batch 5, G1 Loss: 1.0370, G2 Loss: 1.9785, D1 Loss: 1.1823, D2 Loss: 0.3338\n",
      "Batch 6, G1 Loss: 1.1140, G2 Loss: 4.4244, D1 Loss: 1.2822, D2 Loss: 0.1787\n",
      "Batch 7, G1 Loss: 0.8906, G2 Loss: 3.1227, D1 Loss: 1.3486, D2 Loss: 0.3501\n",
      "Batch 8, G1 Loss: 1.1557, G2 Loss: 1.2772, D1 Loss: 1.2814, D2 Loss: 0.7749\n",
      "Batch 9, G1 Loss: 0.9232, G2 Loss: 9.8330, D1 Loss: 1.2768, D2 Loss: 1.4274\n",
      "Batch 10, G1 Loss: 1.2108, G2 Loss: 4.4946, D1 Loss: 1.3683, D2 Loss: 0.2398\n",
      "Batch 11, G1 Loss: 0.9299, G2 Loss: 2.7184, D1 Loss: 1.3173, D2 Loss: 0.2419\n",
      "Batch 12, G1 Loss: 1.1891, G2 Loss: 2.8420, D1 Loss: 1.2164, D2 Loss: 0.2058\n",
      "Batch 13, G1 Loss: 1.0536, G2 Loss: 0.8204, D1 Loss: 1.1903, D2 Loss: 1.1742\n",
      "Batch 14, G1 Loss: 1.1813, G2 Loss: 10.3443, D1 Loss: 1.2314, D2 Loss: 2.2203\n",
      "Batch 15, G1 Loss: 0.9935, G2 Loss: 5.2250, D1 Loss: 1.2350, D2 Loss: 0.4926\n",
      "Batch 16, G1 Loss: 1.1912, G2 Loss: 3.5773, D1 Loss: 1.2923, D2 Loss: 0.3264\n",
      "Batch 17, G1 Loss: 1.0003, G2 Loss: 3.1867, D1 Loss: 1.2434, D2 Loss: 0.3687\n",
      "Batch 18, G1 Loss: 1.0354, G2 Loss: 3.1979, D1 Loss: 1.3468, D2 Loss: 0.1511\n",
      "Batch 19, G1 Loss: 1.0638, G2 Loss: 3.2615, D1 Loss: 1.3631, D2 Loss: 0.4404\n",
      "Batch 20, G1 Loss: 1.0017, G2 Loss: 1.9638, D1 Loss: 1.3825, D2 Loss: 0.4594\n",
      "Batch 21, G1 Loss: 1.0480, G2 Loss: 3.3996, D1 Loss: 1.3115, D2 Loss: 0.3654\n",
      "Batch 22, G1 Loss: 1.2062, G2 Loss: 3.0117, D1 Loss: 1.3145, D2 Loss: 0.1854\n",
      "Batch 23, G1 Loss: 0.9661, G2 Loss: 4.4846, D1 Loss: 1.2191, D2 Loss: 0.1375\n",
      "Batch 24, G1 Loss: 1.5281, G2 Loss: 3.4969, D1 Loss: 1.2301, D2 Loss: 0.1163\n",
      "Batch 25, G1 Loss: 0.6003, G2 Loss: 1.6705, D1 Loss: 1.8273, D2 Loss: 0.6061\n",
      "Batch 26, G1 Loss: 2.0113, G2 Loss: 5.5058, D1 Loss: 1.9715, D2 Loss: 0.7984\n",
      "Batch 27, G1 Loss: 0.5414, G2 Loss: 2.8845, D1 Loss: 1.8902, D2 Loss: 0.2078\n",
      "Batch 28, G1 Loss: 1.6582, G2 Loss: 2.7737, D1 Loss: 1.5604, D2 Loss: 0.3897\n",
      "Batch 29, G1 Loss: 0.8697, G2 Loss: 1.9441, D1 Loss: 1.3808, D2 Loss: 0.4359\n",
      "Batch 30, G1 Loss: 1.0989, G2 Loss: 3.8035, D1 Loss: 1.4077, D2 Loss: 0.4778\n",
      "Batch 31, G1 Loss: 0.9471, G2 Loss: 2.7379, D1 Loss: 1.3864, D2 Loss: 0.2503\n",
      "Batch 32, G1 Loss: 1.1525, G2 Loss: 1.9227, D1 Loss: 1.2422, D2 Loss: 0.4539\n",
      "Epoch 28 Average -> G1 Loss: 1.081282377243042, G2 Loss: 3.620025396347046, D1 Loss: 1.3603260517120361, D2 Loss: 0.45385825634002686\n",
      "Epoch 29/100\n",
      "Batch 1, G1 Loss: 0.9324, G2 Loss: 3.6105, D1 Loss: 1.3012, D2 Loss: 0.2950\n",
      "Batch 2, G1 Loss: 1.2841, G2 Loss: 2.6116, D1 Loss: 1.1827, D2 Loss: 0.3278\n",
      "Batch 3, G1 Loss: 0.9378, G2 Loss: 1.8960, D1 Loss: 1.1725, D2 Loss: 0.4447\n",
      "Batch 4, G1 Loss: 1.2596, G2 Loss: 4.8955, D1 Loss: 1.1934, D2 Loss: 0.1509\n",
      "Batch 5, G1 Loss: 0.9891, G2 Loss: 8.9008, D1 Loss: 1.2034, D2 Loss: 0.2287\n",
      "Batch 6, G1 Loss: 1.2445, G2 Loss: 7.1579, D1 Loss: 1.2147, D2 Loss: 0.0455\n",
      "Batch 7, G1 Loss: 0.8831, G2 Loss: 4.8631, D1 Loss: 1.2115, D2 Loss: 0.0296\n",
      "Batch 8, G1 Loss: 1.6235, G2 Loss: 2.9256, D1 Loss: 1.3692, D2 Loss: 0.1415\n",
      "Batch 9, G1 Loss: 0.6491, G2 Loss: 2.0932, D1 Loss: 1.5207, D2 Loss: 0.2871\n",
      "Batch 10, G1 Loss: 1.9883, G2 Loss: 4.6010, D1 Loss: 1.5511, D2 Loss: 0.0613\n",
      "Batch 11, G1 Loss: 0.5783, G2 Loss: 4.3470, D1 Loss: 1.6240, D2 Loss: 0.1410\n",
      "Batch 12, G1 Loss: 1.6670, G2 Loss: 2.4948, D1 Loss: 1.3052, D2 Loss: 0.2684\n",
      "Batch 13, G1 Loss: 0.8687, G2 Loss: 2.9863, D1 Loss: 1.2557, D2 Loss: 0.1619\n",
      "Batch 14, G1 Loss: 1.3496, G2 Loss: 3.5810, D1 Loss: 1.2703, D2 Loss: 0.2473\n",
      "Batch 15, G1 Loss: 0.8221, G2 Loss: 2.6868, D1 Loss: 1.3514, D2 Loss: 0.2966\n",
      "Batch 16, G1 Loss: 1.3407, G2 Loss: 3.3984, D1 Loss: 1.3322, D2 Loss: 0.3134\n",
      "Batch 17, G1 Loss: 0.7930, G2 Loss: 2.8192, D1 Loss: 1.7375, D2 Loss: 0.1308\n",
      "Batch 18, G1 Loss: 1.0796, G2 Loss: 3.7354, D1 Loss: 1.4461, D2 Loss: 0.3500\n",
      "Batch 19, G1 Loss: 0.8432, G2 Loss: 2.5683, D1 Loss: 1.4523, D2 Loss: 0.1917\n",
      "Batch 20, G1 Loss: 1.3991, G2 Loss: 3.6938, D1 Loss: 1.4686, D2 Loss: 0.1457\n",
      "Batch 21, G1 Loss: 0.5959, G2 Loss: 3.3433, D1 Loss: 1.7868, D2 Loss: 0.1234\n",
      "Batch 22, G1 Loss: 1.8113, G2 Loss: 3.5048, D1 Loss: 1.7788, D2 Loss: 0.2341\n",
      "Batch 23, G1 Loss: 0.5461, G2 Loss: 2.7720, D1 Loss: 2.0562, D2 Loss: 0.1650\n",
      "Batch 24, G1 Loss: 1.4428, G2 Loss: 3.5296, D1 Loss: 1.5822, D2 Loss: 0.3418\n",
      "Batch 25, G1 Loss: 0.8677, G2 Loss: 2.5788, D1 Loss: 1.5513, D2 Loss: 0.2758\n",
      "Batch 26, G1 Loss: 1.0059, G2 Loss: 4.1862, D1 Loss: 1.4828, D2 Loss: 0.2064\n",
      "Batch 27, G1 Loss: 1.0819, G2 Loss: 3.6484, D1 Loss: 1.4432, D2 Loss: 0.1296\n",
      "Batch 28, G1 Loss: 1.0014, G2 Loss: 3.8231, D1 Loss: 1.4068, D2 Loss: 0.0812\n",
      "Batch 29, G1 Loss: 1.0810, G2 Loss: 2.9988, D1 Loss: 1.3432, D2 Loss: 0.1259\n",
      "Batch 30, G1 Loss: 1.1006, G2 Loss: 3.0710, D1 Loss: 1.3512, D2 Loss: 0.2068\n",
      "Batch 31, G1 Loss: 1.0386, G2 Loss: 3.6975, D1 Loss: 1.3496, D2 Loss: 0.1646\n",
      "Batch 32, G1 Loss: 1.1764, G2 Loss: 3.3240, D1 Loss: 1.2302, D2 Loss: 0.3643\n",
      "Epoch 29 Average -> G1 Loss: 1.1025751829147339, G2 Loss: 3.6357505321502686, D1 Loss: 1.4226911067962646, D2 Loss: 0.20868341624736786\n",
      "Epoch 30/100\n",
      "Batch 1, G1 Loss: 1.0186, G2 Loss: 1.7914, D1 Loss: 1.1607, D2 Loss: 0.3821\n",
      "Batch 2, G1 Loss: 1.3572, G2 Loss: 6.2023, D1 Loss: 1.1812, D2 Loss: 0.2846\n",
      "Batch 3, G1 Loss: 0.9022, G2 Loss: 3.5708, D1 Loss: 1.2333, D2 Loss: 0.1762\n",
      "Batch 4, G1 Loss: 1.4468, G2 Loss: 1.5674, D1 Loss: 1.2214, D2 Loss: 0.5019\n",
      "Batch 5, G1 Loss: 0.7461, G2 Loss: 4.0532, D1 Loss: 1.2894, D2 Loss: 0.3398\n",
      "Batch 6, G1 Loss: 1.4759, G2 Loss: 2.5370, D1 Loss: 1.1673, D2 Loss: 0.2802\n",
      "Batch 7, G1 Loss: 0.9408, G2 Loss: 2.5922, D1 Loss: 1.3506, D2 Loss: 0.3319\n",
      "Batch 8, G1 Loss: 0.8672, G2 Loss: 3.2479, D1 Loss: 1.4028, D2 Loss: 0.1140\n",
      "Batch 9, G1 Loss: 1.3549, G2 Loss: 3.5678, D1 Loss: 1.4186, D2 Loss: 0.1298\n",
      "Batch 10, G1 Loss: 0.6898, G2 Loss: 7.0904, D1 Loss: 1.5524, D2 Loss: 0.0694\n",
      "Batch 11, G1 Loss: 1.4985, G2 Loss: 2.2585, D1 Loss: 1.4698, D2 Loss: 0.2167\n",
      "Batch 12, G1 Loss: 0.6329, G2 Loss: 3.2133, D1 Loss: 1.4805, D2 Loss: 0.1526\n",
      "Batch 13, G1 Loss: 1.7547, G2 Loss: 3.6268, D1 Loss: 1.4173, D2 Loss: 0.1975\n",
      "Batch 14, G1 Loss: 0.5910, G2 Loss: 1.8274, D1 Loss: 1.7020, D2 Loss: 0.4572\n",
      "Batch 15, G1 Loss: 1.6503, G2 Loss: 5.8627, D1 Loss: 1.6139, D2 Loss: 0.4244\n",
      "Batch 16, G1 Loss: 0.6063, G2 Loss: 3.8573, D1 Loss: 1.6437, D2 Loss: 0.1773\n",
      "Batch 17, G1 Loss: 1.6843, G2 Loss: 1.7039, D1 Loss: 1.5820, D2 Loss: 0.4578\n",
      "Batch 18, G1 Loss: 0.6913, G2 Loss: 7.8321, D1 Loss: 1.6285, D2 Loss: 0.4256\n",
      "Batch 19, G1 Loss: 1.2686, G2 Loss: 6.2221, D1 Loss: 1.3889, D2 Loss: 0.0941\n",
      "Batch 20, G1 Loss: 0.9812, G2 Loss: 4.1740, D1 Loss: 1.3448, D2 Loss: 0.0943\n",
      "Batch 21, G1 Loss: 1.0529, G2 Loss: 3.2087, D1 Loss: 1.2553, D2 Loss: 0.2534\n",
      "Batch 22, G1 Loss: 1.1053, G2 Loss: 2.0449, D1 Loss: 1.3767, D2 Loss: 0.4062\n",
      "Batch 23, G1 Loss: 0.8506, G2 Loss: 7.3457, D1 Loss: 1.4716, D2 Loss: 0.7444\n",
      "Batch 24, G1 Loss: 1.3647, G2 Loss: 3.8212, D1 Loss: 1.4015, D2 Loss: 0.2960\n",
      "Batch 25, G1 Loss: 0.7540, G2 Loss: 1.8686, D1 Loss: 1.5251, D2 Loss: 0.4455\n",
      "Batch 26, G1 Loss: 1.3414, G2 Loss: 5.5584, D1 Loss: 1.4262, D2 Loss: 0.5397\n",
      "Batch 27, G1 Loss: 0.8096, G2 Loss: 3.8386, D1 Loss: 1.5697, D2 Loss: 0.1532\n",
      "Batch 28, G1 Loss: 1.0308, G2 Loss: 3.9226, D1 Loss: 1.4729, D2 Loss: 0.1325\n",
      "Batch 29, G1 Loss: 1.0699, G2 Loss: 1.7253, D1 Loss: 1.4716, D2 Loss: 0.3772\n",
      "Batch 30, G1 Loss: 0.8957, G2 Loss: 6.1119, D1 Loss: 1.4019, D2 Loss: 0.2461\n",
      "Batch 31, G1 Loss: 1.2242, G2 Loss: 4.7961, D1 Loss: 1.2367, D2 Loss: 0.2181\n",
      "Batch 32, G1 Loss: 1.0364, G2 Loss: 2.1187, D1 Loss: 1.2787, D2 Loss: 0.4183\n",
      "Epoch 30 Average -> G1 Loss: 1.084193468093872, G2 Loss: 3.8487203121185303, D1 Loss: 1.410531997680664, D2 Loss: 0.29806429147720337\n",
      "Epoch 31/100\n",
      "Batch 1, G1 Loss: 1.0273, G2 Loss: 3.4735, D1 Loss: 1.1519, D2 Loss: 0.1370\n",
      "Batch 2, G1 Loss: 1.4131, G2 Loss: 3.6443, D1 Loss: 1.0926, D2 Loss: 0.1620\n",
      "Batch 3, G1 Loss: 0.9317, G2 Loss: 2.5338, D1 Loss: 1.1322, D2 Loss: 0.2840\n",
      "Batch 4, G1 Loss: 1.4422, G2 Loss: 9.0725, D1 Loss: 1.2926, D2 Loss: 0.3072\n",
      "Batch 5, G1 Loss: 0.7109, G2 Loss: 2.3240, D1 Loss: 1.4793, D2 Loss: 0.3261\n",
      "Batch 6, G1 Loss: 1.3048, G2 Loss: 7.4192, D1 Loss: 1.5394, D2 Loss: 0.0419\n",
      "Batch 7, G1 Loss: 0.7502, G2 Loss: 4.0521, D1 Loss: 1.5923, D2 Loss: 0.1585\n",
      "Batch 8, G1 Loss: 1.1123, G2 Loss: 2.0256, D1 Loss: 1.5595, D2 Loss: 0.3562\n",
      "Batch 9, G1 Loss: 1.0067, G2 Loss: 5.0557, D1 Loss: 1.3468, D2 Loss: 0.1221\n",
      "Batch 10, G1 Loss: 0.9454, G2 Loss: 4.5878, D1 Loss: 1.5950, D2 Loss: 0.5937\n",
      "Batch 11, G1 Loss: 0.9523, G2 Loss: 0.7613, D1 Loss: 1.6639, D2 Loss: 1.0792\n",
      "Batch 12, G1 Loss: 0.7467, G2 Loss: 13.1394, D1 Loss: 1.6591, D2 Loss: 3.2381\n",
      "Batch 13, G1 Loss: 1.1540, G2 Loss: 6.9282, D1 Loss: 1.5671, D2 Loss: 0.2584\n",
      "Batch 14, G1 Loss: 0.7775, G2 Loss: 2.4240, D1 Loss: 1.4636, D2 Loss: 0.2821\n",
      "Batch 15, G1 Loss: 1.3134, G2 Loss: 1.9077, D1 Loss: 1.4261, D2 Loss: 0.3793\n",
      "Batch 16, G1 Loss: 0.7455, G2 Loss: 5.0285, D1 Loss: 1.4344, D2 Loss: 0.4286\n",
      "Batch 17, G1 Loss: 1.5632, G2 Loss: 3.0637, D1 Loss: 1.2445, D2 Loss: 0.3519\n",
      "Batch 18, G1 Loss: 0.9720, G2 Loss: 1.2503, D1 Loss: 1.2506, D2 Loss: 0.8162\n",
      "Batch 19, G1 Loss: 1.0553, G2 Loss: 6.6439, D1 Loss: 1.1053, D2 Loss: 0.2646\n",
      "Batch 20, G1 Loss: 1.4680, G2 Loss: 6.0489, D1 Loss: 1.3607, D2 Loss: 0.1540\n",
      "Batch 21, G1 Loss: 0.7601, G2 Loss: 5.0690, D1 Loss: 1.3970, D2 Loss: 0.0504\n",
      "Batch 22, G1 Loss: 1.2327, G2 Loss: 2.0279, D1 Loss: 1.1640, D2 Loss: 0.2916\n",
      "Batch 23, G1 Loss: 1.1728, G2 Loss: 3.0174, D1 Loss: 1.2394, D2 Loss: 0.2387\n",
      "Batch 24, G1 Loss: 0.8871, G2 Loss: 2.6407, D1 Loss: 1.3966, D2 Loss: 0.2939\n",
      "Batch 25, G1 Loss: 1.4345, G2 Loss: 3.3888, D1 Loss: 1.3294, D2 Loss: 0.3762\n",
      "Batch 26, G1 Loss: 0.6814, G2 Loss: 1.8313, D1 Loss: 1.5774, D2 Loss: 0.5380\n",
      "Batch 27, G1 Loss: 1.1875, G2 Loss: 3.4892, D1 Loss: 1.3416, D2 Loss: 0.4103\n",
      "Batch 28, G1 Loss: 1.0189, G2 Loss: 3.0440, D1 Loss: 1.4684, D2 Loss: 0.2028\n",
      "Batch 29, G1 Loss: 0.8148, G2 Loss: 3.0117, D1 Loss: 1.6273, D2 Loss: 0.1828\n",
      "Batch 30, G1 Loss: 1.1308, G2 Loss: 4.6036, D1 Loss: 1.6647, D2 Loss: 0.2824\n",
      "Batch 31, G1 Loss: 0.7030, G2 Loss: 6.2812, D1 Loss: 1.7101, D2 Loss: 0.0965\n",
      "Batch 32, G1 Loss: 1.3340, G2 Loss: 1.8878, D1 Loss: 1.7282, D2 Loss: 0.4881\n",
      "Epoch 31 Average -> G1 Loss: 1.0546952486038208, G2 Loss: 4.114902019500732, D1 Loss: 1.425026297569275, D2 Loss: 0.41227754950523376\n",
      "Epoch 32/100\n",
      "Batch 1, G1 Loss: 0.6704, G2 Loss: 5.7538, D1 Loss: 1.6242, D2 Loss: 0.4605\n",
      "Batch 2, G1 Loss: 1.5066, G2 Loss: 4.2633, D1 Loss: 1.5705, D2 Loss: 0.3029\n",
      "Batch 3, G1 Loss: 0.6772, G2 Loss: 2.7311, D1 Loss: 1.7494, D2 Loss: 0.3189\n",
      "Batch 4, G1 Loss: 1.4327, G2 Loss: 2.6839, D1 Loss: 1.6574, D2 Loss: 0.2549\n",
      "Batch 5, G1 Loss: 0.7089, G2 Loss: 4.5750, D1 Loss: 1.6500, D2 Loss: 0.2753\n",
      "Batch 6, G1 Loss: 1.4210, G2 Loss: 2.4505, D1 Loss: 1.3929, D2 Loss: 0.3100\n",
      "Batch 7, G1 Loss: 0.8514, G2 Loss: 2.9878, D1 Loss: 1.3720, D2 Loss: 0.2522\n",
      "Batch 8, G1 Loss: 1.1596, G2 Loss: 1.9963, D1 Loss: 1.3130, D2 Loss: 0.3562\n",
      "Batch 9, G1 Loss: 1.0605, G2 Loss: 3.5445, D1 Loss: 1.2683, D2 Loss: 0.4550\n",
      "Batch 10, G1 Loss: 1.0850, G2 Loss: 1.8959, D1 Loss: 1.1228, D2 Loss: 0.4069\n",
      "Batch 11, G1 Loss: 1.3426, G2 Loss: 5.0582, D1 Loss: 1.1186, D2 Loss: 0.2075\n",
      "Batch 12, G1 Loss: 0.9681, G2 Loss: 4.5755, D1 Loss: 1.2190, D2 Loss: 0.2117\n",
      "Batch 13, G1 Loss: 1.4408, G2 Loss: 1.8775, D1 Loss: 1.0174, D2 Loss: 0.3867\n",
      "Batch 14, G1 Loss: 1.1386, G2 Loss: 5.1499, D1 Loss: 1.1297, D2 Loss: 0.4502\n",
      "Batch 15, G1 Loss: 1.1320, G2 Loss: 5.8812, D1 Loss: 1.2028, D2 Loss: 0.1604\n",
      "Batch 16, G1 Loss: 1.1092, G2 Loss: 1.8252, D1 Loss: 1.1779, D2 Loss: 0.4005\n",
      "Batch 17, G1 Loss: 1.2787, G2 Loss: 4.3784, D1 Loss: 1.1338, D2 Loss: 0.2488\n",
      "Batch 18, G1 Loss: 1.0615, G2 Loss: 4.7002, D1 Loss: 1.2091, D2 Loss: 0.1138\n",
      "Batch 19, G1 Loss: 1.2828, G2 Loss: 4.8226, D1 Loss: 1.1457, D2 Loss: 0.0969\n",
      "Batch 20, G1 Loss: 1.0364, G2 Loss: 1.5420, D1 Loss: 1.0914, D2 Loss: 0.5313\n",
      "Batch 21, G1 Loss: 1.3830, G2 Loss: 8.0204, D1 Loss: 1.3047, D2 Loss: 0.6855\n",
      "Batch 22, G1 Loss: 0.5974, G2 Loss: 5.8897, D1 Loss: 1.9255, D2 Loss: 0.1050\n",
      "Batch 23, G1 Loss: 1.8250, G2 Loss: 2.6436, D1 Loss: 2.0146, D2 Loss: 0.3270\n",
      "Batch 24, G1 Loss: 0.3776, G2 Loss: 4.6277, D1 Loss: 2.8327, D2 Loss: 0.3425\n",
      "Batch 25, G1 Loss: 1.6041, G2 Loss: 2.4466, D1 Loss: 1.9629, D2 Loss: 0.3377\n",
      "Batch 26, G1 Loss: 0.7080, G2 Loss: 5.6977, D1 Loss: 1.6491, D2 Loss: 0.2863\n",
      "Batch 27, G1 Loss: 1.1455, G2 Loss: 3.4582, D1 Loss: 1.4116, D2 Loss: 0.1068\n",
      "Batch 28, G1 Loss: 0.9600, G2 Loss: 2.2304, D1 Loss: 1.3157, D2 Loss: 0.2799\n",
      "Batch 29, G1 Loss: 1.1512, G2 Loss: 4.4518, D1 Loss: 1.5869, D2 Loss: 0.3596\n",
      "Batch 30, G1 Loss: 0.7626, G2 Loss: 2.3545, D1 Loss: 1.6557, D2 Loss: 0.2878\n",
      "Batch 31, G1 Loss: 1.2287, G2 Loss: 4.3188, D1 Loss: 1.4965, D2 Loss: 0.2441\n",
      "Batch 32, G1 Loss: 0.8874, G2 Loss: 5.0323, D1 Loss: 1.5203, D2 Loss: 0.0453\n",
      "Epoch 32 Average -> G1 Loss: 1.0935858488082886, G2 Loss: 3.87076735496521, D1 Loss: 1.4638111591339111, D2 Loss: 0.3002510070800781\n",
      "Epoch 33/100\n",
      "Batch 1, G1 Loss: 1.0720, G2 Loss: 1.5597, D1 Loss: 1.3037, D2 Loss: 0.5148\n",
      "Batch 2, G1 Loss: 1.2162, G2 Loss: 6.3968, D1 Loss: 1.2019, D2 Loss: 0.3410\n",
      "Batch 3, G1 Loss: 1.1070, G2 Loss: 5.1422, D1 Loss: 1.1719, D2 Loss: 0.2035\n",
      "Batch 4, G1 Loss: 1.1107, G2 Loss: 3.2467, D1 Loss: 1.3161, D2 Loss: 0.3707\n",
      "Batch 5, G1 Loss: 0.9180, G2 Loss: 2.7272, D1 Loss: 1.3374, D2 Loss: 0.2134\n",
      "Batch 6, G1 Loss: 1.2255, G2 Loss: 3.0486, D1 Loss: 1.1762, D2 Loss: 0.2428\n",
      "Batch 7, G1 Loss: 1.0135, G2 Loss: 3.7330, D1 Loss: 1.3097, D2 Loss: 0.2303\n",
      "Batch 8, G1 Loss: 0.9630, G2 Loss: 2.9102, D1 Loss: 1.2408, D2 Loss: 0.3136\n",
      "Batch 9, G1 Loss: 1.2701, G2 Loss: 2.8439, D1 Loss: 1.2622, D2 Loss: 0.2667\n",
      "Batch 10, G1 Loss: 0.9592, G2 Loss: 3.1003, D1 Loss: 1.2931, D2 Loss: 0.1836\n",
      "Batch 11, G1 Loss: 1.2533, G2 Loss: 2.5431, D1 Loss: 1.1416, D2 Loss: 0.3296\n",
      "Batch 12, G1 Loss: 1.0511, G2 Loss: 4.4249, D1 Loss: 1.1450, D2 Loss: 0.3896\n",
      "Batch 13, G1 Loss: 1.3168, G2 Loss: 2.2944, D1 Loss: 1.1540, D2 Loss: 0.2160\n",
      "Batch 14, G1 Loss: 0.9037, G2 Loss: 5.2057, D1 Loss: 1.3163, D2 Loss: 0.4224\n",
      "Batch 15, G1 Loss: 1.2680, G2 Loss: 2.3498, D1 Loss: 1.5108, D2 Loss: 0.3516\n",
      "Batch 16, G1 Loss: 0.7819, G2 Loss: 3.5701, D1 Loss: 1.5496, D2 Loss: 0.3935\n",
      "Batch 17, G1 Loss: 1.4484, G2 Loss: 3.2556, D1 Loss: 1.5913, D2 Loss: 0.1728\n",
      "Batch 18, G1 Loss: 0.7820, G2 Loss: 2.6947, D1 Loss: 1.4798, D2 Loss: 0.2075\n",
      "Batch 19, G1 Loss: 1.4777, G2 Loss: 3.7780, D1 Loss: 1.5278, D2 Loss: 0.1432\n",
      "Batch 20, G1 Loss: 0.7487, G2 Loss: 3.7936, D1 Loss: 1.4612, D2 Loss: 0.3806\n",
      "Batch 21, G1 Loss: 1.4258, G2 Loss: 2.2618, D1 Loss: 1.4005, D2 Loss: 0.3507\n",
      "Batch 22, G1 Loss: 0.7378, G2 Loss: 3.6019, D1 Loss: 1.4695, D2 Loss: 0.1463\n",
      "Batch 23, G1 Loss: 1.2048, G2 Loss: 3.2784, D1 Loss: 1.4569, D2 Loss: 0.2421\n",
      "Batch 24, G1 Loss: 0.7454, G2 Loss: 1.9263, D1 Loss: 1.4682, D2 Loss: 0.3887\n",
      "Batch 25, G1 Loss: 1.4139, G2 Loss: 4.3142, D1 Loss: 1.3933, D2 Loss: 0.5388\n",
      "Batch 26, G1 Loss: 0.7179, G2 Loss: 1.6217, D1 Loss: 1.3407, D2 Loss: 0.4053\n",
      "Batch 27, G1 Loss: 1.5500, G2 Loss: 6.6635, D1 Loss: 1.2646, D2 Loss: 0.8358\n",
      "Batch 28, G1 Loss: 0.9036, G2 Loss: 2.1419, D1 Loss: 1.2503, D2 Loss: 0.2526\n",
      "Batch 29, G1 Loss: 1.1041, G2 Loss: 3.0257, D1 Loss: 1.2449, D2 Loss: 0.1572\n",
      "Batch 30, G1 Loss: 1.0756, G2 Loss: 3.5593, D1 Loss: 1.3012, D2 Loss: 0.2102\n",
      "Batch 31, G1 Loss: 0.9038, G2 Loss: 2.5880, D1 Loss: 1.3594, D2 Loss: 0.1666\n",
      "Batch 32, G1 Loss: 1.1306, G2 Loss: 4.6981, D1 Loss: 1.5760, D2 Loss: 0.0992\n",
      "Epoch 33 Average -> G1 Loss: 1.0874987840652466, G2 Loss: 3.384354829788208, D1 Loss: 1.3442518711090088, D2 Loss: 0.3025248348712921\n",
      "Epoch 34/100\n",
      "Batch 1, G1 Loss: 0.7205, G2 Loss: 1.3741, D1 Loss: 1.6453, D2 Loss: 0.4943\n",
      "Batch 2, G1 Loss: 1.1555, G2 Loss: 9.5246, D1 Loss: 1.3274, D2 Loss: 1.0879\n",
      "Batch 3, G1 Loss: 1.0240, G2 Loss: 5.8187, D1 Loss: 1.3586, D2 Loss: 0.0852\n",
      "Batch 4, G1 Loss: 0.9510, G2 Loss: 2.6713, D1 Loss: 1.2802, D2 Loss: 0.2197\n",
      "Batch 5, G1 Loss: 1.2571, G2 Loss: 1.9323, D1 Loss: 1.2984, D2 Loss: 0.2865\n",
      "Batch 6, G1 Loss: 0.9258, G2 Loss: 4.6807, D1 Loss: 1.3411, D2 Loss: 0.3623\n",
      "Batch 7, G1 Loss: 1.0960, G2 Loss: 1.9282, D1 Loss: 1.1437, D2 Loss: 0.3457\n",
      "Batch 8, G1 Loss: 1.2662, G2 Loss: 9.5373, D1 Loss: 1.3002, D2 Loss: 0.1020\n",
      "Batch 9, G1 Loss: 0.8598, G2 Loss: 8.2094, D1 Loss: 1.2373, D2 Loss: 0.0448\n",
      "Batch 10, G1 Loss: 1.3156, G2 Loss: 5.6814, D1 Loss: 1.0434, D2 Loss: 0.2437\n",
      "Batch 11, G1 Loss: 1.3394, G2 Loss: 1.7484, D1 Loss: 1.1613, D2 Loss: 0.4528\n",
      "Batch 12, G1 Loss: 0.8883, G2 Loss: 5.0889, D1 Loss: 1.2870, D2 Loss: 0.3272\n",
      "Batch 13, G1 Loss: 1.3310, G2 Loss: 4.6645, D1 Loss: 1.3326, D2 Loss: 0.3009\n",
      "Batch 14, G1 Loss: 0.9796, G2 Loss: 2.0525, D1 Loss: 1.2735, D2 Loss: 0.3956\n",
      "Batch 15, G1 Loss: 1.1127, G2 Loss: 4.1972, D1 Loss: 1.2710, D2 Loss: 0.1018\n",
      "Batch 16, G1 Loss: 1.0636, G2 Loss: 5.2630, D1 Loss: 1.3369, D2 Loss: 0.3987\n",
      "Batch 17, G1 Loss: 1.1582, G2 Loss: 3.7293, D1 Loss: 1.1413, D2 Loss: 0.2591\n",
      "Batch 18, G1 Loss: 0.9910, G2 Loss: 1.6696, D1 Loss: 1.2034, D2 Loss: 0.4139\n",
      "Batch 19, G1 Loss: 1.2556, G2 Loss: 5.9764, D1 Loss: 1.3511, D2 Loss: 0.1642\n",
      "Batch 20, G1 Loss: 0.6748, G2 Loss: 6.1415, D1 Loss: 1.6480, D2 Loss: 0.2231\n",
      "Batch 21, G1 Loss: 1.4426, G2 Loss: 3.6721, D1 Loss: 1.6500, D2 Loss: 0.2065\n",
      "Batch 22, G1 Loss: 0.5877, G2 Loss: 3.1989, D1 Loss: 1.9339, D2 Loss: 0.1936\n",
      "Batch 23, G1 Loss: 1.2941, G2 Loss: 3.2164, D1 Loss: 1.6463, D2 Loss: 0.1905\n",
      "Batch 24, G1 Loss: 0.7980, G2 Loss: 3.3106, D1 Loss: 1.6581, D2 Loss: 0.3111\n",
      "Batch 25, G1 Loss: 0.9637, G2 Loss: 3.9181, D1 Loss: 1.3601, D2 Loss: 0.1840\n",
      "Batch 26, G1 Loss: 1.2542, G2 Loss: 1.3825, D1 Loss: 1.3358, D2 Loss: 0.6443\n",
      "Batch 27, G1 Loss: 0.8963, G2 Loss: 7.8304, D1 Loss: 1.3456, D2 Loss: 0.5730\n",
      "Batch 28, G1 Loss: 1.1196, G2 Loss: 6.0399, D1 Loss: 1.3382, D2 Loss: 0.2734\n",
      "Batch 29, G1 Loss: 0.9744, G2 Loss: 2.6099, D1 Loss: 1.3136, D2 Loss: 0.3043\n",
      "Batch 30, G1 Loss: 1.0593, G2 Loss: 2.0356, D1 Loss: 1.3139, D2 Loss: 0.3511\n",
      "Batch 31, G1 Loss: 1.0047, G2 Loss: 5.5629, D1 Loss: 1.1718, D2 Loss: 0.4810\n",
      "Batch 32, G1 Loss: 1.2826, G2 Loss: 3.3115, D1 Loss: 1.1835, D2 Loss: 0.1521\n",
      "Epoch 34 Average -> G1 Loss: 1.0638339519500732, G2 Loss: 4.311812400817871, D1 Loss: 1.3510143756866455, D2 Loss: 0.31794288754463196\n",
      "Epoch 35/100\n",
      "Batch 1, G1 Loss: 1.1016, G2 Loss: 4.1548, D1 Loss: 1.0450, D2 Loss: 0.0629\n",
      "Batch 2, G1 Loss: 1.1913, G2 Loss: 2.5536, D1 Loss: 1.1268, D2 Loss: 0.2824\n",
      "Batch 3, G1 Loss: 1.0730, G2 Loss: 2.9189, D1 Loss: 1.1235, D2 Loss: 0.3107\n",
      "Batch 4, G1 Loss: 1.1096, G2 Loss: 3.2582, D1 Loss: 1.1877, D2 Loss: 0.2351\n",
      "Batch 5, G1 Loss: 0.9501, G2 Loss: 3.2046, D1 Loss: 1.2303, D2 Loss: 0.1980\n",
      "Batch 6, G1 Loss: 1.4875, G2 Loss: 3.9820, D1 Loss: 1.2263, D2 Loss: 0.2747\n",
      "Batch 7, G1 Loss: 0.7064, G2 Loss: 2.0545, D1 Loss: 1.3628, D2 Loss: 0.3834\n",
      "Batch 8, G1 Loss: 1.6087, G2 Loss: 6.3177, D1 Loss: 1.3956, D2 Loss: 0.1322\n",
      "Batch 9, G1 Loss: 0.5350, G2 Loss: 6.3795, D1 Loss: 1.7832, D2 Loss: 0.2528\n",
      "Batch 10, G1 Loss: 1.4997, G2 Loss: 5.3861, D1 Loss: 1.6096, D2 Loss: 0.1637\n",
      "Batch 11, G1 Loss: 0.5142, G2 Loss: 6.7472, D1 Loss: 1.8015, D2 Loss: 0.0076\n",
      "Batch 12, G1 Loss: 1.9299, G2 Loss: 0.5477, D1 Loss: 1.6891, D2 Loss: 1.6468\n",
      "Batch 13, G1 Loss: 0.5006, G2 Loss: 13.6421, D1 Loss: 1.8204, D2 Loss: 1.7229\n",
      "Batch 14, G1 Loss: 1.7733, G2 Loss: 10.6934, D1 Loss: 1.3653, D2 Loss: 1.1789\n",
      "Batch 15, G1 Loss: 0.7581, G2 Loss: 4.2973, D1 Loss: 1.4069, D2 Loss: 0.1280\n",
      "Batch 16, G1 Loss: 1.0794, G2 Loss: 1.7920, D1 Loss: 1.4436, D2 Loss: 0.4231\n",
      "Batch 17, G1 Loss: 0.9469, G2 Loss: 5.4715, D1 Loss: 1.3730, D2 Loss: 0.4440\n",
      "Batch 18, G1 Loss: 1.0537, G2 Loss: 2.5736, D1 Loss: 1.3733, D2 Loss: 0.3620\n",
      "Batch 19, G1 Loss: 0.9245, G2 Loss: 2.2035, D1 Loss: 1.3355, D2 Loss: 0.3482\n",
      "Batch 20, G1 Loss: 1.1112, G2 Loss: 4.3697, D1 Loss: 1.2052, D2 Loss: 0.2886\n",
      "Batch 21, G1 Loss: 1.0999, G2 Loss: 2.8959, D1 Loss: 1.2005, D2 Loss: 0.2342\n",
      "Batch 22, G1 Loss: 1.0583, G2 Loss: 1.8057, D1 Loss: 1.2496, D2 Loss: 0.3014\n",
      "Batch 23, G1 Loss: 1.0220, G2 Loss: 4.4839, D1 Loss: 1.1959, D2 Loss: 0.2658\n",
      "Batch 24, G1 Loss: 1.1562, G2 Loss: 3.3407, D1 Loss: 1.1151, D2 Loss: 0.1659\n",
      "Batch 25, G1 Loss: 1.2318, G2 Loss: 2.5627, D1 Loss: 1.2101, D2 Loss: 0.3606\n",
      "Batch 26, G1 Loss: 0.9578, G2 Loss: 1.4398, D1 Loss: 1.2269, D2 Loss: 0.5486\n",
      "Batch 27, G1 Loss: 1.3001, G2 Loss: 4.5574, D1 Loss: 1.1428, D2 Loss: 0.3660\n",
      "Batch 28, G1 Loss: 1.1079, G2 Loss: 4.5171, D1 Loss: 1.2005, D2 Loss: 0.5541\n",
      "Batch 29, G1 Loss: 1.2916, G2 Loss: 6.6545, D1 Loss: 1.1610, D2 Loss: 0.0583\n",
      "Batch 30, G1 Loss: 0.9609, G2 Loss: 0.6102, D1 Loss: 1.3396, D2 Loss: 1.7166\n",
      "Batch 31, G1 Loss: 1.1821, G2 Loss: 9.4709, D1 Loss: 1.2770, D2 Loss: 0.7882\n",
      "Batch 32, G1 Loss: 1.1858, G2 Loss: 8.7632, D1 Loss: 1.3340, D2 Loss: 0.7504\n",
      "Epoch 35 Average -> G1 Loss: 1.1065324544906616, G2 Loss: 4.489052772521973, D1 Loss: 1.3299263715744019, D2 Loss: 0.46737292408943176\n",
      "Epoch 36/100\n",
      "Batch 1, G1 Loss: 0.9099, G2 Loss: 3.9525, D1 Loss: 1.1936, D2 Loss: 0.2979\n",
      "Batch 2, G1 Loss: 1.6586, G2 Loss: 2.3125, D1 Loss: 1.1470, D2 Loss: 0.5806\n",
      "Batch 3, G1 Loss: 0.8557, G2 Loss: 3.4177, D1 Loss: 1.2341, D2 Loss: 0.2969\n",
      "Batch 4, G1 Loss: 1.3415, G2 Loss: 3.7663, D1 Loss: 1.3079, D2 Loss: 0.2434\n",
      "Batch 5, G1 Loss: 0.7998, G2 Loss: 3.6932, D1 Loss: 1.5461, D2 Loss: 0.3577\n",
      "Batch 6, G1 Loss: 1.3036, G2 Loss: 1.9457, D1 Loss: 1.3366, D2 Loss: 0.3431\n",
      "Batch 7, G1 Loss: 0.8516, G2 Loss: 2.8367, D1 Loss: 1.2975, D2 Loss: 0.2683\n",
      "Batch 8, G1 Loss: 1.3452, G2 Loss: 5.1317, D1 Loss: 1.3864, D2 Loss: 0.4298\n",
      "Batch 9, G1 Loss: 0.8296, G2 Loss: 2.0836, D1 Loss: 1.3555, D2 Loss: 0.5542\n",
      "Batch 10, G1 Loss: 1.3912, G2 Loss: 6.5006, D1 Loss: 1.2065, D2 Loss: 0.2176\n",
      "Batch 11, G1 Loss: 1.1565, G2 Loss: 5.5737, D1 Loss: 1.1302, D2 Loss: 0.2946\n",
      "Batch 12, G1 Loss: 0.9833, G2 Loss: 1.4859, D1 Loss: 1.1391, D2 Loss: 0.5544\n",
      "Batch 13, G1 Loss: 2.0067, G2 Loss: 5.4779, D1 Loss: 1.2613, D2 Loss: 0.1723\n",
      "Batch 14, G1 Loss: 0.5578, G2 Loss: 4.6428, D1 Loss: 1.6598, D2 Loss: 0.4269\n",
      "Batch 15, G1 Loss: 1.8257, G2 Loss: 1.7366, D1 Loss: 1.2368, D2 Loss: 0.6143\n",
      "Batch 16, G1 Loss: 0.8871, G2 Loss: 4.5206, D1 Loss: 1.2897, D2 Loss: 0.2192\n",
      "Batch 17, G1 Loss: 1.2135, G2 Loss: 3.8219, D1 Loss: 1.2933, D2 Loss: 0.1876\n",
      "Batch 18, G1 Loss: 1.0716, G2 Loss: 3.4035, D1 Loss: 1.2152, D2 Loss: 0.1645\n",
      "Batch 19, G1 Loss: 1.1319, G2 Loss: 1.9688, D1 Loss: 1.2647, D2 Loss: 0.3687\n",
      "Batch 20, G1 Loss: 1.1287, G2 Loss: 3.2369, D1 Loss: 1.1036, D2 Loss: 0.3649\n",
      "Batch 21, G1 Loss: 1.2575, G2 Loss: 3.4844, D1 Loss: 1.2910, D2 Loss: 0.1322\n",
      "Batch 22, G1 Loss: 0.9384, G2 Loss: 2.3804, D1 Loss: 1.4038, D2 Loss: 0.3572\n",
      "Batch 23, G1 Loss: 1.4832, G2 Loss: 3.4443, D1 Loss: 1.2547, D2 Loss: 0.2483\n",
      "Batch 24, G1 Loss: 0.9156, G2 Loss: 2.8147, D1 Loss: 1.2993, D2 Loss: 0.2734\n",
      "Batch 25, G1 Loss: 1.3110, G2 Loss: 4.0283, D1 Loss: 1.1168, D2 Loss: 0.2644\n",
      "Batch 26, G1 Loss: 1.1740, G2 Loss: 3.0431, D1 Loss: 1.1547, D2 Loss: 0.2683\n",
      "Batch 27, G1 Loss: 1.0669, G2 Loss: 5.6820, D1 Loss: 1.0820, D2 Loss: 0.0231\n",
      "Batch 28, G1 Loss: 1.5667, G2 Loss: 4.1724, D1 Loss: 1.2052, D2 Loss: 0.1037\n",
      "Batch 29, G1 Loss: 0.6353, G2 Loss: 1.9520, D1 Loss: 1.5854, D2 Loss: 0.5924\n",
      "Batch 30, G1 Loss: 1.7153, G2 Loss: 7.9904, D1 Loss: 1.6432, D2 Loss: 0.3943\n",
      "Batch 31, G1 Loss: 0.5847, G2 Loss: 5.7039, D1 Loss: 1.7173, D2 Loss: 0.4220\n",
      "Batch 32, G1 Loss: 1.6605, G2 Loss: 2.0078, D1 Loss: 1.5260, D2 Loss: 0.5422\n",
      "Epoch 36 Average -> G1 Loss: 1.1737104654312134, G2 Loss: 3.694145679473877, D1 Loss: 1.3088937997817993, D2 Loss: 0.3305773437023163\n",
      "Epoch 37/100\n",
      "Batch 1, G1 Loss: 0.8562, G2 Loss: 4.4574, D1 Loss: 1.2416, D2 Loss: 0.0636\n",
      "Batch 2, G1 Loss: 1.2959, G2 Loss: 4.9790, D1 Loss: 1.4143, D2 Loss: 0.1819\n",
      "Batch 3, G1 Loss: 0.8254, G2 Loss: 2.5132, D1 Loss: 1.3850, D2 Loss: 0.1982\n",
      "Batch 4, G1 Loss: 1.3519, G2 Loss: 2.3787, D1 Loss: 1.2954, D2 Loss: 0.2975\n",
      "Batch 5, G1 Loss: 0.8458, G2 Loss: 4.4424, D1 Loss: 1.4164, D2 Loss: 0.2054\n",
      "Batch 6, G1 Loss: 1.2717, G2 Loss: 3.2262, D1 Loss: 1.2900, D2 Loss: 0.1511\n",
      "Batch 7, G1 Loss: 0.7948, G2 Loss: 2.0977, D1 Loss: 1.3767, D2 Loss: 0.4240\n",
      "Batch 8, G1 Loss: 1.3480, G2 Loss: 3.2075, D1 Loss: 1.4209, D2 Loss: 0.2215\n",
      "Batch 9, G1 Loss: 0.7422, G2 Loss: 8.0232, D1 Loss: 1.5364, D2 Loss: 0.4308\n",
      "Batch 10, G1 Loss: 1.2045, G2 Loss: 4.0294, D1 Loss: 1.4171, D2 Loss: 0.1042\n",
      "Batch 11, G1 Loss: 1.0935, G2 Loss: 1.0825, D1 Loss: 1.3556, D2 Loss: 0.9147\n",
      "Batch 12, G1 Loss: 0.8968, G2 Loss: 10.5058, D1 Loss: 1.3410, D2 Loss: 1.3278\n",
      "Batch 13, G1 Loss: 1.5711, G2 Loss: 7.6531, D1 Loss: 1.2349, D2 Loss: 0.2309\n",
      "Batch 14, G1 Loss: 0.8205, G2 Loss: 5.8571, D1 Loss: 1.2445, D2 Loss: 0.1570\n",
      "Batch 15, G1 Loss: 1.6821, G2 Loss: 2.6601, D1 Loss: 1.1725, D2 Loss: 0.4849\n",
      "Batch 16, G1 Loss: 0.9501, G2 Loss: 6.0894, D1 Loss: 1.1305, D2 Loss: 0.2370\n",
      "Batch 17, G1 Loss: 1.4551, G2 Loss: 3.2540, D1 Loss: 0.9898, D2 Loss: 0.1352\n",
      "Batch 18, G1 Loss: 1.1549, G2 Loss: 2.8870, D1 Loss: 1.0512, D2 Loss: 0.2155\n",
      "Batch 19, G1 Loss: 1.2238, G2 Loss: 4.0538, D1 Loss: 1.0343, D2 Loss: 0.2657\n",
      "Batch 20, G1 Loss: 1.2947, G2 Loss: 2.6604, D1 Loss: 1.1115, D2 Loss: 0.3115\n",
      "Batch 21, G1 Loss: 0.9382, G2 Loss: 2.2616, D1 Loss: 1.2387, D2 Loss: 0.2849\n",
      "Batch 22, G1 Loss: 1.4592, G2 Loss: 3.2910, D1 Loss: 1.2259, D2 Loss: 0.4863\n",
      "Batch 23, G1 Loss: 0.7400, G2 Loss: 2.0337, D1 Loss: 1.4128, D2 Loss: 0.2595\n",
      "Batch 24, G1 Loss: 1.8492, G2 Loss: 4.8717, D1 Loss: 1.4903, D2 Loss: 0.2447\n",
      "Batch 25, G1 Loss: 0.6365, G2 Loss: 7.2490, D1 Loss: 1.5274, D2 Loss: 0.0960\n",
      "Batch 26, G1 Loss: 2.4584, G2 Loss: 1.4765, D1 Loss: 1.6945, D2 Loss: 0.5308\n",
      "Batch 27, G1 Loss: 0.4092, G2 Loss: 6.5281, D1 Loss: 2.5783, D2 Loss: 0.0879\n",
      "Batch 28, G1 Loss: 2.7805, G2 Loss: 6.4846, D1 Loss: 1.7486, D2 Loss: 0.5034\n",
      "Batch 29, G1 Loss: 0.6926, G2 Loss: 3.4728, D1 Loss: 1.5574, D2 Loss: 0.2021\n",
      "Batch 30, G1 Loss: 1.2708, G2 Loss: 2.4375, D1 Loss: 1.0404, D2 Loss: 0.2379\n",
      "Batch 31, G1 Loss: 1.2349, G2 Loss: 3.5074, D1 Loss: 1.3443, D2 Loss: 0.1636\n",
      "Batch 32, G1 Loss: 0.9166, G2 Loss: 4.8158, D1 Loss: 1.2969, D2 Loss: 0.2505\n",
      "Epoch 37 Average -> G1 Loss: 1.1895338296890259, G2 Loss: 4.202736854553223, D1 Loss: 1.3629764318466187, D2 Loss: 0.30956292152404785\n",
      "Epoch 38/100\n",
      "Batch 1, G1 Loss: 1.2279, G2 Loss: 3.1740, D1 Loss: 1.3379, D2 Loss: 0.1309\n",
      "Batch 2, G1 Loss: 0.9987, G2 Loss: 2.4569, D1 Loss: 1.3327, D2 Loss: 0.3554\n",
      "Batch 3, G1 Loss: 0.9693, G2 Loss: 4.1417, D1 Loss: 1.2882, D2 Loss: 0.3826\n",
      "Batch 4, G1 Loss: 1.2586, G2 Loss: 2.7989, D1 Loss: 1.4075, D2 Loss: 0.2474\n",
      "Batch 5, G1 Loss: 0.7847, G2 Loss: 2.8235, D1 Loss: 1.2810, D2 Loss: 0.3427\n",
      "Batch 6, G1 Loss: 1.7920, G2 Loss: 2.8941, D1 Loss: 1.2356, D2 Loss: 0.2975\n",
      "Batch 7, G1 Loss: 0.8039, G2 Loss: 3.6977, D1 Loss: 1.3901, D2 Loss: 0.1687\n",
      "Batch 8, G1 Loss: 1.3026, G2 Loss: 3.3198, D1 Loss: 1.2253, D2 Loss: 0.1940\n",
      "Batch 9, G1 Loss: 1.0326, G2 Loss: 4.5900, D1 Loss: 1.2008, D2 Loss: 0.0938\n",
      "Batch 10, G1 Loss: 1.4472, G2 Loss: 2.6230, D1 Loss: 1.2004, D2 Loss: 0.2994\n",
      "Batch 11, G1 Loss: 1.0109, G2 Loss: 5.1803, D1 Loss: 1.0554, D2 Loss: 0.4417\n",
      "Batch 12, G1 Loss: 1.3772, G2 Loss: 3.2155, D1 Loss: 1.1339, D2 Loss: 0.1330\n",
      "Batch 13, G1 Loss: 0.9812, G2 Loss: 2.2082, D1 Loss: 1.2930, D2 Loss: 0.2249\n",
      "Batch 14, G1 Loss: 1.2813, G2 Loss: 4.6199, D1 Loss: 1.3753, D2 Loss: 0.6050\n",
      "Batch 15, G1 Loss: 0.7452, G2 Loss: 1.2969, D1 Loss: 1.4513, D2 Loss: 0.5684\n",
      "Batch 16, G1 Loss: 1.9664, G2 Loss: 8.9474, D1 Loss: 1.4803, D2 Loss: 0.5572\n",
      "Batch 17, G1 Loss: 0.4843, G2 Loss: 6.6054, D1 Loss: 1.7916, D2 Loss: 0.2072\n",
      "Batch 18, G1 Loss: 1.9017, G2 Loss: 2.7061, D1 Loss: 1.4764, D2 Loss: 0.2399\n",
      "Batch 19, G1 Loss: 0.7176, G2 Loss: 2.6908, D1 Loss: 1.4552, D2 Loss: 0.2293\n",
      "Batch 20, G1 Loss: 1.2488, G2 Loss: 6.5970, D1 Loss: 1.3573, D2 Loss: 0.1452\n",
      "Batch 21, G1 Loss: 0.7990, G2 Loss: 5.7780, D1 Loss: 1.4221, D2 Loss: 0.2007\n",
      "Batch 22, G1 Loss: 1.4299, G2 Loss: 1.1071, D1 Loss: 1.1516, D2 Loss: 0.7531\n",
      "Batch 23, G1 Loss: 0.8265, G2 Loss: 6.5095, D1 Loss: 1.3464, D2 Loss: 0.8225\n",
      "Batch 24, G1 Loss: 1.2378, G2 Loss: 5.0628, D1 Loss: 1.1846, D2 Loss: 0.0546\n",
      "Batch 25, G1 Loss: 0.9992, G2 Loss: 3.9745, D1 Loss: 1.2553, D2 Loss: 0.1473\n",
      "Batch 26, G1 Loss: 0.8780, G2 Loss: 2.7164, D1 Loss: 1.4108, D2 Loss: 0.1651\n",
      "Batch 27, G1 Loss: 1.1778, G2 Loss: 3.4500, D1 Loss: 1.3128, D2 Loss: 0.3829\n",
      "Batch 28, G1 Loss: 0.9424, G2 Loss: 1.4584, D1 Loss: 1.3494, D2 Loss: 0.5430\n",
      "Batch 29, G1 Loss: 1.1055, G2 Loss: 5.8342, D1 Loss: 1.1902, D2 Loss: 0.7456\n",
      "Batch 30, G1 Loss: 1.2756, G2 Loss: 6.9537, D1 Loss: 1.0487, D2 Loss: 0.0482\n",
      "Batch 31, G1 Loss: 1.1888, G2 Loss: 5.0241, D1 Loss: 1.1420, D2 Loss: 0.0570\n",
      "Batch 32, G1 Loss: 1.0760, G2 Loss: 2.4336, D1 Loss: 1.1706, D2 Loss: 0.2707\n",
      "Epoch 38 Average -> G1 Loss: 1.1333928108215332, G2 Loss: 3.9652905464172363, D1 Loss: 1.3047972917556763, D2 Loss: 0.3142177164554596\n",
      "Epoch 39/100\n",
      "Batch 1, G1 Loss: 1.1906, G2 Loss: 3.4946, D1 Loss: 1.0634, D2 Loss: 0.1134\n",
      "Batch 2, G1 Loss: 1.1545, G2 Loss: 3.3138, D1 Loss: 1.1726, D2 Loss: 0.1992\n",
      "Batch 3, G1 Loss: 1.4950, G2 Loss: 3.3159, D1 Loss: 1.0391, D2 Loss: 0.2314\n",
      "Batch 4, G1 Loss: 0.9340, G2 Loss: 2.9163, D1 Loss: 1.1819, D2 Loss: 0.2987\n",
      "Batch 5, G1 Loss: 1.2754, G2 Loss: 1.6649, D1 Loss: 1.0439, D2 Loss: 0.5942\n",
      "Batch 6, G1 Loss: 1.3384, G2 Loss: 6.1528, D1 Loss: 1.1898, D2 Loss: 0.4668\n",
      "Batch 7, G1 Loss: 0.8808, G2 Loss: 8.9654, D1 Loss: 1.3350, D2 Loss: 0.2361\n",
      "Batch 8, G1 Loss: 1.5056, G2 Loss: 2.4985, D1 Loss: 1.2589, D2 Loss: 0.2706\n",
      "Batch 9, G1 Loss: 0.8226, G2 Loss: 1.9943, D1 Loss: 1.3993, D2 Loss: 0.3398\n",
      "Batch 10, G1 Loss: 1.4075, G2 Loss: 6.4448, D1 Loss: 1.3122, D2 Loss: 0.2822\n",
      "Batch 11, G1 Loss: 0.7145, G2 Loss: 5.2173, D1 Loss: 1.4963, D2 Loss: 0.6378\n",
      "Batch 12, G1 Loss: 2.0320, G2 Loss: 1.1400, D1 Loss: 1.5713, D2 Loss: 0.7249\n",
      "Batch 13, G1 Loss: 0.5144, G2 Loss: 9.0403, D1 Loss: 1.9594, D2 Loss: 0.8251\n",
      "Batch 14, G1 Loss: 2.0566, G2 Loss: 7.2345, D1 Loss: 1.3376, D2 Loss: 0.0891\n",
      "Batch 15, G1 Loss: 0.9561, G2 Loss: 4.6153, D1 Loss: 1.1988, D2 Loss: 0.0702\n",
      "Batch 16, G1 Loss: 1.2202, G2 Loss: 2.6155, D1 Loss: 1.1611, D2 Loss: 0.2557\n",
      "Batch 17, G1 Loss: 1.2936, G2 Loss: 2.9211, D1 Loss: 1.2685, D2 Loss: 0.1858\n",
      "Batch 18, G1 Loss: 0.8374, G2 Loss: 3.0300, D1 Loss: 1.3093, D2 Loss: 0.1685\n",
      "Batch 19, G1 Loss: 1.7452, G2 Loss: 4.0638, D1 Loss: 1.3874, D2 Loss: 0.1361\n",
      "Batch 20, G1 Loss: 0.7849, G2 Loss: 3.6356, D1 Loss: 1.3421, D2 Loss: 0.1058\n",
      "Batch 21, G1 Loss: 1.6712, G2 Loss: 6.1757, D1 Loss: 1.0391, D2 Loss: 0.0973\n",
      "Batch 22, G1 Loss: 0.8887, G2 Loss: 4.6518, D1 Loss: 1.1833, D2 Loss: 0.0962\n",
      "Batch 23, G1 Loss: 1.6309, G2 Loss: 1.3708, D1 Loss: 1.5859, D2 Loss: 0.6684\n",
      "Batch 24, G1 Loss: 0.5138, G2 Loss: 6.5648, D1 Loss: 1.8741, D2 Loss: 0.7618\n",
      "Batch 25, G1 Loss: 1.7074, G2 Loss: 5.1276, D1 Loss: 1.4762, D2 Loss: 0.2094\n",
      "Batch 26, G1 Loss: 0.9148, G2 Loss: 2.5850, D1 Loss: 1.2247, D2 Loss: 0.2213\n",
      "Batch 27, G1 Loss: 1.2196, G2 Loss: 2.5390, D1 Loss: 1.1831, D2 Loss: 0.2471\n",
      "Batch 28, G1 Loss: 1.2279, G2 Loss: 3.4586, D1 Loss: 1.1489, D2 Loss: 0.1509\n",
      "Batch 29, G1 Loss: 1.0774, G2 Loss: 4.6497, D1 Loss: 1.0995, D2 Loss: 0.1787\n",
      "Batch 30, G1 Loss: 1.3611, G2 Loss: 5.8111, D1 Loss: 1.0763, D2 Loss: 0.1672\n",
      "Batch 31, G1 Loss: 0.9965, G2 Loss: 2.1603, D1 Loss: 1.2304, D2 Loss: 0.2348\n",
      "Batch 32, G1 Loss: 1.1337, G2 Loss: 3.3486, D1 Loss: 1.3076, D2 Loss: 0.2352\n",
      "Epoch 39 Average -> G1 Loss: 1.2031983137130737, G2 Loss: 4.147424221038818, D1 Loss: 1.2955347299575806, D2 Loss: 0.2968730330467224\n",
      "Epoch 40/100\n",
      "Batch 1, G1 Loss: 0.8748, G2 Loss: 3.4880, D1 Loss: 1.5724, D2 Loss: 0.2651\n",
      "Batch 2, G1 Loss: 1.2761, G2 Loss: 3.0495, D1 Loss: 1.3431, D2 Loss: 0.2464\n",
      "Batch 3, G1 Loss: 0.6741, G2 Loss: 6.5164, D1 Loss: 1.6325, D2 Loss: 0.1171\n",
      "Batch 4, G1 Loss: 2.2917, G2 Loss: 1.7828, D1 Loss: 1.3910, D2 Loss: 0.3391\n",
      "Batch 5, G1 Loss: 0.5026, G2 Loss: 6.2223, D1 Loss: 1.8633, D2 Loss: 0.1538\n",
      "Batch 6, G1 Loss: 2.4194, G2 Loss: 6.0296, D1 Loss: 1.5908, D2 Loss: 0.3003\n",
      "Batch 7, G1 Loss: 0.4722, G2 Loss: 2.7179, D1 Loss: 1.9424, D2 Loss: 0.1636\n",
      "Batch 8, G1 Loss: 1.7584, G2 Loss: 2.7118, D1 Loss: 1.4834, D2 Loss: 0.1602\n",
      "Batch 9, G1 Loss: 0.7884, G2 Loss: 4.0528, D1 Loss: 1.4443, D2 Loss: 0.1414\n",
      "Batch 10, G1 Loss: 1.3506, G2 Loss: 4.6209, D1 Loss: 1.2017, D2 Loss: 0.2079\n",
      "Batch 11, G1 Loss: 1.1135, G2 Loss: 2.6807, D1 Loss: 1.1530, D2 Loss: 0.2923\n",
      "Batch 12, G1 Loss: 1.1219, G2 Loss: 2.5819, D1 Loss: 1.2264, D2 Loss: 0.2508\n",
      "Batch 13, G1 Loss: 1.0751, G2 Loss: 4.5087, D1 Loss: 1.1081, D2 Loss: 0.2755\n",
      "Batch 14, G1 Loss: 1.3770, G2 Loss: 4.2705, D1 Loss: 1.0414, D2 Loss: 0.1016\n",
      "Batch 15, G1 Loss: 1.1426, G2 Loss: 2.4916, D1 Loss: 1.0288, D2 Loss: 0.2433\n",
      "Batch 16, G1 Loss: 1.2203, G2 Loss: 2.7233, D1 Loss: 1.0794, D2 Loss: 0.1471\n",
      "Batch 17, G1 Loss: 1.1133, G2 Loss: 4.2248, D1 Loss: 1.2132, D2 Loss: 0.3915\n",
      "Batch 18, G1 Loss: 1.0128, G2 Loss: 2.7876, D1 Loss: 1.2982, D2 Loss: 0.1649\n",
      "Batch 19, G1 Loss: 1.2816, G2 Loss: 3.2341, D1 Loss: 1.3158, D2 Loss: 0.2671\n",
      "Batch 20, G1 Loss: 0.7114, G2 Loss: 3.4670, D1 Loss: 1.5107, D2 Loss: 0.0995\n",
      "Batch 21, G1 Loss: 1.7269, G2 Loss: 6.6268, D1 Loss: 1.2530, D2 Loss: 0.0328\n",
      "Batch 22, G1 Loss: 1.0086, G2 Loss: 2.4618, D1 Loss: 1.2714, D2 Loss: 0.2465\n",
      "Batch 23, G1 Loss: 1.1604, G2 Loss: 5.0561, D1 Loss: 1.0605, D2 Loss: 0.1241\n",
      "Batch 24, G1 Loss: 1.4535, G2 Loss: 3.6986, D1 Loss: 0.9126, D2 Loss: 0.1731\n",
      "Batch 25, G1 Loss: 1.2703, G2 Loss: 2.9444, D1 Loss: 1.0141, D2 Loss: 0.2859\n",
      "Batch 26, G1 Loss: 1.1848, G2 Loss: 2.2854, D1 Loss: 1.1645, D2 Loss: 0.2316\n",
      "Batch 27, G1 Loss: 1.3509, G2 Loss: 4.4786, D1 Loss: 1.2428, D2 Loss: 0.2003\n",
      "Batch 28, G1 Loss: 0.9342, G2 Loss: 3.0935, D1 Loss: 1.3473, D2 Loss: 0.3678\n",
      "Batch 29, G1 Loss: 1.5032, G2 Loss: 4.8546, D1 Loss: 1.3504, D2 Loss: 0.2007\n",
      "Batch 30, G1 Loss: 0.7369, G2 Loss: 7.8854, D1 Loss: 1.5072, D2 Loss: 0.0102\n",
      "Batch 31, G1 Loss: 2.0581, G2 Loss: 1.8519, D1 Loss: 1.5853, D2 Loss: 0.3872\n",
      "Batch 32, G1 Loss: 0.6031, G2 Loss: 5.5464, D1 Loss: 1.7237, D2 Loss: 0.0343\n",
      "Epoch 40 Average -> G1 Loss: 1.2052733898162842, G2 Loss: 3.9045603275299072, D1 Loss: 1.3397715091705322, D2 Loss: 0.20696483552455902\n",
      "Epoch 41/100\n",
      "Batch 1, G1 Loss: 1.8668, G2 Loss: 6.0248, D1 Loss: 1.3417, D2 Loss: 0.0874\n",
      "Batch 2, G1 Loss: 0.7138, G2 Loss: 4.0012, D1 Loss: 1.5443, D2 Loss: 0.2786\n",
      "Batch 3, G1 Loss: 1.4989, G2 Loss: 2.5535, D1 Loss: 1.2210, D2 Loss: 0.2404\n",
      "Batch 4, G1 Loss: 0.7979, G2 Loss: 3.6358, D1 Loss: 1.5987, D2 Loss: 0.3512\n",
      "Batch 5, G1 Loss: 1.5491, G2 Loss: 2.5767, D1 Loss: 1.3571, D2 Loss: 0.2587\n",
      "Batch 6, G1 Loss: 0.7819, G2 Loss: 6.1459, D1 Loss: 1.4715, D2 Loss: 0.1355\n",
      "Batch 7, G1 Loss: 1.7338, G2 Loss: 6.5094, D1 Loss: 1.3904, D2 Loss: 0.0470\n",
      "Batch 8, G1 Loss: 0.6388, G2 Loss: 4.0213, D1 Loss: 1.6284, D2 Loss: 0.0790\n",
      "Batch 9, G1 Loss: 1.9211, G2 Loss: 2.6742, D1 Loss: 1.2254, D2 Loss: 0.2422\n",
      "Batch 10, G1 Loss: 0.8773, G2 Loss: 4.7476, D1 Loss: 1.2666, D2 Loss: 0.3047\n",
      "Batch 11, G1 Loss: 1.1992, G2 Loss: 2.5048, D1 Loss: 1.1903, D2 Loss: 0.4444\n",
      "Batch 12, G1 Loss: 1.2059, G2 Loss: 3.9840, D1 Loss: 1.1920, D2 Loss: 0.1061\n",
      "Batch 13, G1 Loss: 0.8105, G2 Loss: 3.4650, D1 Loss: 1.3634, D2 Loss: 0.2347\n",
      "Batch 14, G1 Loss: 1.4562, G2 Loss: 1.9805, D1 Loss: 1.4478, D2 Loss: 0.4652\n",
      "Batch 15, G1 Loss: 0.7652, G2 Loss: 6.4019, D1 Loss: 1.3934, D2 Loss: 0.3205\n",
      "Batch 16, G1 Loss: 1.4666, G2 Loss: 3.5574, D1 Loss: 1.2883, D2 Loss: 0.2100\n",
      "Batch 17, G1 Loss: 0.9920, G2 Loss: 1.0809, D1 Loss: 1.1948, D2 Loss: 0.8463\n",
      "Batch 18, G1 Loss: 1.1799, G2 Loss: 10.8920, D1 Loss: 1.0405, D2 Loss: 0.4216\n",
      "Batch 19, G1 Loss: 1.3811, G2 Loss: 9.2499, D1 Loss: 1.1912, D2 Loss: 0.2667\n",
      "Batch 20, G1 Loss: 0.7450, G2 Loss: 3.9384, D1 Loss: 1.3606, D2 Loss: 0.0938\n",
      "Batch 21, G1 Loss: 1.8346, G2 Loss: 1.5756, D1 Loss: 1.4742, D2 Loss: 0.5779\n",
      "Batch 22, G1 Loss: 0.7248, G2 Loss: 6.7994, D1 Loss: 1.5001, D2 Loss: 0.3483\n",
      "Batch 23, G1 Loss: 1.5581, G2 Loss: 5.3037, D1 Loss: 1.1415, D2 Loss: 0.3544\n",
      "Batch 24, G1 Loss: 1.0131, G2 Loss: 1.2800, D1 Loss: 1.3232, D2 Loss: 0.7266\n",
      "Batch 25, G1 Loss: 1.0178, G2 Loss: 9.8433, D1 Loss: 1.2558, D2 Loss: 0.5929\n",
      "Batch 26, G1 Loss: 1.3772, G2 Loss: 7.8455, D1 Loss: 1.1441, D2 Loss: 0.1366\n",
      "Batch 27, G1 Loss: 0.9313, G2 Loss: 2.5018, D1 Loss: 1.2135, D2 Loss: 0.2065\n",
      "Batch 28, G1 Loss: 1.4041, G2 Loss: 1.8170, D1 Loss: 1.2202, D2 Loss: 0.2937\n",
      "Batch 29, G1 Loss: 0.7562, G2 Loss: 6.6075, D1 Loss: 1.3744, D2 Loss: 0.4439\n",
      "Batch 30, G1 Loss: 1.5191, G2 Loss: 3.3750, D1 Loss: 1.3494, D2 Loss: 0.2540\n",
      "Batch 31, G1 Loss: 0.7275, G2 Loss: 2.1990, D1 Loss: 1.6074, D2 Loss: 0.2623\n",
      "Batch 32, G1 Loss: 1.3409, G2 Loss: 3.6928, D1 Loss: 1.1638, D2 Loss: 0.3418\n",
      "Epoch 41 Average -> G1 Loss: 1.1808017492294312, G2 Loss: 4.462067604064941, D1 Loss: 1.3273437023162842, D2 Loss: 0.31165140867233276\n",
      "Epoch 42/100\n",
      "Batch 1, G1 Loss: 1.1677, G2 Loss: 2.6552, D1 Loss: 1.3042, D2 Loss: 0.3178\n",
      "Batch 2, G1 Loss: 0.9055, G2 Loss: 5.0277, D1 Loss: 1.1385, D2 Loss: 0.0525\n",
      "Batch 3, G1 Loss: 1.5095, G2 Loss: 6.3366, D1 Loss: 1.2217, D2 Loss: 0.0232\n",
      "Batch 4, G1 Loss: 0.5765, G2 Loss: 2.4974, D1 Loss: 1.6857, D2 Loss: 0.2092\n",
      "Batch 5, G1 Loss: 1.3733, G2 Loss: 5.5046, D1 Loss: 1.5178, D2 Loss: 0.1515\n",
      "Batch 6, G1 Loss: 0.9196, G2 Loss: 3.0283, D1 Loss: 1.2515, D2 Loss: 0.1503\n",
      "Batch 7, G1 Loss: 1.3335, G2 Loss: 3.5275, D1 Loss: 1.1022, D2 Loss: 0.1216\n",
      "Batch 8, G1 Loss: 1.0487, G2 Loss: 4.9507, D1 Loss: 1.1257, D2 Loss: 0.0637\n",
      "Batch 9, G1 Loss: 1.1286, G2 Loss: 0.4476, D1 Loss: 1.2165, D2 Loss: 2.1132\n",
      "Batch 10, G1 Loss: 1.0493, G2 Loss: 23.2172, D1 Loss: 1.1974, D2 Loss: 2.1965\n",
      "Batch 11, G1 Loss: 1.0360, G2 Loss: 19.0625, D1 Loss: 1.4723, D2 Loss: 0.3824\n",
      "Batch 12, G1 Loss: 1.1269, G2 Loss: 12.1131, D1 Loss: 1.2586, D2 Loss: 0.2739\n",
      "Batch 13, G1 Loss: 0.8579, G2 Loss: 4.5919, D1 Loss: 1.3847, D2 Loss: 0.1865\n",
      "Batch 14, G1 Loss: 1.4534, G2 Loss: 1.4576, D1 Loss: 1.3837, D2 Loss: 0.5682\n",
      "Batch 15, G1 Loss: 0.7506, G2 Loss: 9.6644, D1 Loss: 1.3966, D2 Loss: 0.1896\n",
      "Batch 16, G1 Loss: 1.4886, G2 Loss: 9.2800, D1 Loss: 1.2111, D2 Loss: 0.2276\n",
      "Batch 17, G1 Loss: 0.8547, G2 Loss: 5.6691, D1 Loss: 1.3427, D2 Loss: 0.0498\n",
      "Batch 18, G1 Loss: 1.2789, G2 Loss: 4.7595, D1 Loss: 1.2442, D2 Loss: 0.0169\n",
      "Batch 19, G1 Loss: 0.7639, G2 Loss: 1.3941, D1 Loss: 1.5193, D2 Loss: 0.4939\n",
      "Batch 20, G1 Loss: 1.4873, G2 Loss: 6.8529, D1 Loss: 1.5478, D2 Loss: 0.0162\n",
      "Batch 21, G1 Loss: 0.6139, G2 Loss: 6.8377, D1 Loss: 1.5397, D2 Loss: 0.2930\n",
      "Batch 22, G1 Loss: 2.2604, G2 Loss: 4.2842, D1 Loss: 1.6245, D2 Loss: 0.1641\n",
      "Batch 23, G1 Loss: 0.7723, G2 Loss: 1.2629, D1 Loss: 1.3523, D2 Loss: 0.7020\n",
      "Batch 24, G1 Loss: 1.3809, G2 Loss: 6.8780, D1 Loss: 1.2521, D2 Loss: 0.3344\n",
      "Batch 25, G1 Loss: 1.1502, G2 Loss: 5.6064, D1 Loss: 1.3625, D2 Loss: 0.5220\n",
      "Batch 26, G1 Loss: 0.8208, G2 Loss: 5.7745, D1 Loss: 1.7062, D2 Loss: 0.2411\n",
      "Batch 27, G1 Loss: 1.1350, G2 Loss: 4.7808, D1 Loss: 1.8327, D2 Loss: 0.0435\n",
      "Batch 28, G1 Loss: 1.2797, G2 Loss: 3.8557, D1 Loss: 1.5456, D2 Loss: 0.0730\n",
      "Batch 29, G1 Loss: 1.1104, G2 Loss: 0.1548, D1 Loss: 1.2976, D2 Loss: 3.8320\n",
      "Batch 30, G1 Loss: 1.3268, G2 Loss: 22.8672, D1 Loss: 1.1169, D2 Loss: 1.4847\n",
      "Batch 31, G1 Loss: 1.8627, G2 Loss: 20.3216, D1 Loss: 0.9241, D2 Loss: 1.5217\n",
      "Batch 32, G1 Loss: 0.8625, G2 Loss: 14.0321, D1 Loss: 1.3652, D2 Loss: 0.1461\n",
      "Epoch 42 Average -> G1 Loss: 1.1464484930038452, G2 Loss: 7.146683692932129, D1 Loss: 1.3575507402420044, D2 Loss: 0.5363088846206665\n",
      "Epoch 43/100\n",
      "Batch 1, G1 Loss: 2.2296, G2 Loss: 9.8215, D1 Loss: 1.4659, D2 Loss: 0.0026\n",
      "Batch 2, G1 Loss: 0.4717, G2 Loss: 7.3325, D1 Loss: 2.4556, D2 Loss: 0.0045\n",
      "Batch 3, G1 Loss: 1.6167, G2 Loss: 5.1984, D1 Loss: 1.5474, D2 Loss: 0.0118\n",
      "Batch 4, G1 Loss: 0.9254, G2 Loss: 3.2398, D1 Loss: 1.7182, D2 Loss: 0.0631\n",
      "Batch 5, G1 Loss: 0.8530, G2 Loss: 2.8798, D1 Loss: 1.4606, D2 Loss: 0.0919\n",
      "Batch 6, G1 Loss: 1.4018, G2 Loss: 4.2167, D1 Loss: 1.3100, D2 Loss: 0.0296\n",
      "Batch 7, G1 Loss: 1.1253, G2 Loss: 4.0759, D1 Loss: 1.1500, D2 Loss: 0.0713\n",
      "Batch 8, G1 Loss: 1.2675, G2 Loss: 1.2331, D1 Loss: 1.0007, D2 Loss: 0.5442\n",
      "Batch 9, G1 Loss: 1.4896, G2 Loss: 8.3444, D1 Loss: 1.0068, D2 Loss: 0.1376\n",
      "Batch 10, G1 Loss: 1.1601, G2 Loss: 3.3735, D1 Loss: 1.0207, D2 Loss: 0.2841\n",
      "Batch 11, G1 Loss: 1.2750, G2 Loss: 1.1840, D1 Loss: 0.9873, D2 Loss: 0.7903\n",
      "Batch 12, G1 Loss: 1.4479, G2 Loss: 13.2182, D1 Loss: 0.9211, D2 Loss: 1.0013\n",
      "Batch 13, G1 Loss: 1.2750, G2 Loss: 9.6856, D1 Loss: 1.1339, D2 Loss: 0.3219\n",
      "Batch 14, G1 Loss: 1.0592, G2 Loss: 2.7226, D1 Loss: 1.0965, D2 Loss: 0.1604\n",
      "Batch 15, G1 Loss: 1.7368, G2 Loss: 1.2066, D1 Loss: 1.0725, D2 Loss: 0.8965\n",
      "Batch 16, G1 Loss: 0.8276, G2 Loss: 8.0446, D1 Loss: 1.2368, D2 Loss: 0.6139\n",
      "Batch 17, G1 Loss: 1.9354, G2 Loss: 6.1273, D1 Loss: 1.0789, D2 Loss: 0.3144\n",
      "Batch 18, G1 Loss: 0.9023, G2 Loss: 4.9154, D1 Loss: 1.1597, D2 Loss: 0.0954\n",
      "Batch 19, G1 Loss: 1.5204, G2 Loss: 2.9079, D1 Loss: 0.8744, D2 Loss: 0.1711\n",
      "Batch 20, G1 Loss: 1.3037, G2 Loss: 2.3799, D1 Loss: 1.0738, D2 Loss: 0.3517\n",
      "Batch 21, G1 Loss: 1.1875, G2 Loss: 5.9634, D1 Loss: 1.0963, D2 Loss: 0.3241\n",
      "Batch 22, G1 Loss: 1.2092, G2 Loss: 3.4767, D1 Loss: 1.1617, D2 Loss: 0.1720\n",
      "Batch 23, G1 Loss: 1.2243, G2 Loss: 1.9681, D1 Loss: 1.2738, D2 Loss: 0.3064\n",
      "Batch 24, G1 Loss: 0.9524, G2 Loss: 3.9380, D1 Loss: 1.2903, D2 Loss: 0.2113\n",
      "Batch 25, G1 Loss: 1.5053, G2 Loss: 3.2422, D1 Loss: 1.5195, D2 Loss: 0.1407\n",
      "Batch 26, G1 Loss: 0.6645, G2 Loss: 2.3981, D1 Loss: 1.6667, D2 Loss: 0.1919\n",
      "Batch 27, G1 Loss: 2.3639, G2 Loss: 3.8116, D1 Loss: 1.8506, D2 Loss: 0.4051\n",
      "Batch 28, G1 Loss: 0.5128, G2 Loss: 11.0620, D1 Loss: 1.8991, D2 Loss: 0.1726\n",
      "Batch 29, G1 Loss: 2.0262, G2 Loss: 3.3282, D1 Loss: 1.1735, D2 Loss: 0.0961\n",
      "Batch 30, G1 Loss: 1.1131, G2 Loss: 1.5438, D1 Loss: 1.3347, D2 Loss: 0.4268\n",
      "Batch 31, G1 Loss: 0.9333, G2 Loss: 5.4703, D1 Loss: 1.3257, D2 Loss: 0.0904\n",
      "Batch 32, G1 Loss: 1.5339, G2 Loss: 5.5200, D1 Loss: 1.2064, D2 Loss: 0.1174\n",
      "Epoch 43 Average -> G1 Loss: 1.282831072807312, G2 Loss: 4.8071746826171875, D1 Loss: 1.2990411520004272, D2 Loss: 0.2691415250301361\n",
      "Epoch 44/100\n",
      "Batch 1, G1 Loss: 1.1259, G2 Loss: 3.8279, D1 Loss: 1.2660, D2 Loss: 0.1667\n",
      "Batch 2, G1 Loss: 1.0364, G2 Loss: 2.5187, D1 Loss: 1.4655, D2 Loss: 0.4397\n",
      "Batch 3, G1 Loss: 1.0548, G2 Loss: 4.0892, D1 Loss: 1.5783, D2 Loss: 0.4280\n",
      "Batch 4, G1 Loss: 1.1197, G2 Loss: 2.7309, D1 Loss: 1.4975, D2 Loss: 0.2259\n",
      "Batch 5, G1 Loss: 0.9797, G2 Loss: 3.5298, D1 Loss: 1.4948, D2 Loss: 0.3286\n",
      "Batch 6, G1 Loss: 1.1057, G2 Loss: 2.1137, D1 Loss: 1.3549, D2 Loss: 0.4576\n",
      "Batch 7, G1 Loss: 1.4326, G2 Loss: 4.4409, D1 Loss: 1.5785, D2 Loss: 0.2873\n",
      "Batch 8, G1 Loss: 0.7210, G2 Loss: 3.9955, D1 Loss: 1.4673, D2 Loss: 0.1608\n",
      "Batch 9, G1 Loss: 1.4275, G2 Loss: 3.4501, D1 Loss: 1.2967, D2 Loss: 0.1968\n",
      "Batch 10, G1 Loss: 1.0385, G2 Loss: 1.6543, D1 Loss: 1.4229, D2 Loss: 0.3822\n",
      "Batch 11, G1 Loss: 0.9189, G2 Loss: 5.2971, D1 Loss: 1.3930, D2 Loss: 0.3584\n",
      "Batch 12, G1 Loss: 1.2423, G2 Loss: 3.6138, D1 Loss: 1.4617, D2 Loss: 0.5183\n",
      "Batch 13, G1 Loss: 0.9078, G2 Loss: 2.3296, D1 Loss: 1.4507, D2 Loss: 0.3483\n",
      "Batch 14, G1 Loss: 1.1314, G2 Loss: 5.0157, D1 Loss: 1.1987, D2 Loss: 0.1555\n",
      "Batch 15, G1 Loss: 1.2383, G2 Loss: 2.5989, D1 Loss: 1.2222, D2 Loss: 0.3306\n",
      "Batch 16, G1 Loss: 1.1494, G2 Loss: 2.9370, D1 Loss: 1.2616, D2 Loss: 0.3250\n",
      "Batch 17, G1 Loss: 1.1521, G2 Loss: 5.1178, D1 Loss: 1.4317, D2 Loss: 0.2517\n",
      "Batch 18, G1 Loss: 0.9657, G2 Loss: 3.2924, D1 Loss: 1.2597, D2 Loss: 0.2271\n",
      "Batch 19, G1 Loss: 1.4733, G2 Loss: 3.2520, D1 Loss: 1.2763, D2 Loss: 0.1754\n",
      "Batch 20, G1 Loss: 0.7950, G2 Loss: 4.4205, D1 Loss: 1.3774, D2 Loss: 0.1794\n",
      "Batch 21, G1 Loss: 1.7968, G2 Loss: 4.1965, D1 Loss: 1.4321, D2 Loss: 0.1778\n",
      "Batch 22, G1 Loss: 0.6179, G2 Loss: 1.3295, D1 Loss: 1.5778, D2 Loss: 0.7923\n",
      "Batch 23, G1 Loss: 1.7466, G2 Loss: 9.1570, D1 Loss: 1.3186, D2 Loss: 1.4322\n",
      "Batch 24, G1 Loss: 0.9667, G2 Loss: 5.6447, D1 Loss: 1.2180, D2 Loss: 0.3012\n",
      "Batch 25, G1 Loss: 1.1530, G2 Loss: 2.3960, D1 Loss: 1.2955, D2 Loss: 0.3491\n",
      "Batch 26, G1 Loss: 1.1001, G2 Loss: 4.1080, D1 Loss: 1.2169, D2 Loss: 0.1797\n",
      "Batch 27, G1 Loss: 1.1382, G2 Loss: 3.1258, D1 Loss: 1.2617, D2 Loss: 0.2676\n",
      "Batch 28, G1 Loss: 0.8919, G2 Loss: 4.0029, D1 Loss: 1.2805, D2 Loss: 0.1715\n",
      "Batch 29, G1 Loss: 1.5574, G2 Loss: 2.9579, D1 Loss: 1.1182, D2 Loss: 0.1956\n",
      "Batch 30, G1 Loss: 0.8200, G2 Loss: 3.2170, D1 Loss: 1.2308, D2 Loss: 0.1247\n",
      "Batch 31, G1 Loss: 1.6087, G2 Loss: 4.2542, D1 Loss: 1.1434, D2 Loss: 0.1481\n",
      "Batch 32, G1 Loss: 1.0593, G2 Loss: 3.2524, D1 Loss: 1.0192, D2 Loss: 0.1660\n",
      "Epoch 44 Average -> G1 Loss: 1.1397682428359985, G2 Loss: 3.6833667755126953, D1 Loss: 1.3396278619766235, D2 Loss: 0.3202737867832184\n",
      "Epoch 45/100\n",
      "Batch 1, G1 Loss: 1.3355, G2 Loss: 3.7356, D1 Loss: 0.8516, D2 Loss: 0.1676\n",
      "Batch 2, G1 Loss: 1.6782, G2 Loss: 3.6121, D1 Loss: 0.9533, D2 Loss: 0.1446\n",
      "Batch 3, G1 Loss: 0.9400, G2 Loss: 2.6534, D1 Loss: 1.0984, D2 Loss: 0.2700\n",
      "Batch 4, G1 Loss: 1.5242, G2 Loss: 3.9677, D1 Loss: 1.0430, D2 Loss: 0.3020\n",
      "Batch 5, G1 Loss: 1.0581, G2 Loss: 3.8927, D1 Loss: 1.0676, D2 Loss: 0.2193\n",
      "Batch 6, G1 Loss: 1.3968, G2 Loss: 7.2139, D1 Loss: 1.0022, D2 Loss: 0.0281\n",
      "Batch 7, G1 Loss: 1.3616, G2 Loss: 3.5418, D1 Loss: 0.9695, D2 Loss: 0.1169\n",
      "Batch 8, G1 Loss: 1.4178, G2 Loss: 1.5496, D1 Loss: 0.9281, D2 Loss: 0.6519\n",
      "Batch 9, G1 Loss: 1.5375, G2 Loss: 9.0572, D1 Loss: 1.2132, D2 Loss: 0.9241\n",
      "Batch 10, G1 Loss: 1.0621, G2 Loss: 6.1243, D1 Loss: 1.0147, D2 Loss: 0.1624\n",
      "Batch 11, G1 Loss: 1.4864, G2 Loss: 4.1218, D1 Loss: 1.0414, D2 Loss: 0.1036\n",
      "Batch 12, G1 Loss: 1.3630, G2 Loss: 2.4923, D1 Loss: 1.0285, D2 Loss: 0.2626\n",
      "Batch 13, G1 Loss: 1.2312, G2 Loss: 3.3692, D1 Loss: 0.9525, D2 Loss: 0.3884\n",
      "Batch 14, G1 Loss: 1.5235, G2 Loss: 2.2735, D1 Loss: 1.1119, D2 Loss: 0.4644\n",
      "Batch 15, G1 Loss: 0.9572, G2 Loss: 4.2420, D1 Loss: 1.4334, D2 Loss: 0.2229\n",
      "Batch 16, G1 Loss: 1.4455, G2 Loss: 4.5096, D1 Loss: 1.3275, D2 Loss: 0.1320\n",
      "Batch 17, G1 Loss: 0.8216, G2 Loss: 5.5173, D1 Loss: 1.6708, D2 Loss: 0.0448\n",
      "Batch 18, G1 Loss: 1.8997, G2 Loss: 1.3971, D1 Loss: 1.2637, D2 Loss: 0.5386\n",
      "Batch 19, G1 Loss: 0.7958, G2 Loss: 8.6711, D1 Loss: 1.5405, D2 Loss: 0.6488\n",
      "Batch 20, G1 Loss: 2.5131, G2 Loss: 5.9093, D1 Loss: 1.2555, D2 Loss: 0.2008\n",
      "Batch 21, G1 Loss: 0.6686, G2 Loss: 2.7302, D1 Loss: 1.5003, D2 Loss: 0.1648\n",
      "Batch 22, G1 Loss: 2.2082, G2 Loss: 3.5113, D1 Loss: 1.1944, D2 Loss: 0.1209\n",
      "Batch 23, G1 Loss: 0.8025, G2 Loss: 4.2383, D1 Loss: 1.3991, D2 Loss: 0.2051\n",
      "Batch 24, G1 Loss: 1.3764, G2 Loss: 2.4080, D1 Loss: 1.1125, D2 Loss: 0.3042\n",
      "Batch 25, G1 Loss: 0.9244, G2 Loss: 3.2891, D1 Loss: 1.4769, D2 Loss: 0.1791\n",
      "Batch 26, G1 Loss: 1.1797, G2 Loss: 3.0847, D1 Loss: 1.3577, D2 Loss: 0.1011\n",
      "Batch 27, G1 Loss: 1.4381, G2 Loss: 3.6286, D1 Loss: 1.1683, D2 Loss: 0.5439\n",
      "Batch 28, G1 Loss: 0.8319, G2 Loss: 2.3871, D1 Loss: 1.1754, D2 Loss: 0.4309\n",
      "Batch 29, G1 Loss: 2.0450, G2 Loss: 6.2153, D1 Loss: 1.1373, D2 Loss: 0.2410\n",
      "Batch 30, G1 Loss: 0.9772, G2 Loss: 5.5390, D1 Loss: 1.0138, D2 Loss: 0.0608\n",
      "Batch 31, G1 Loss: 1.7805, G2 Loss: 2.0501, D1 Loss: 0.8456, D2 Loss: 0.3459\n",
      "Batch 32, G1 Loss: 1.6643, G2 Loss: 7.2690, D1 Loss: 0.8152, D2 Loss: 0.5034\n",
      "Epoch 45 Average -> G1 Loss: 1.3514149188995361, G2 Loss: 4.193817615509033, D1 Loss: 1.1551185846328735, D2 Loss: 0.2873404622077942\n",
      "Epoch 46/100\n",
      "Batch 1, G1 Loss: 1.1462, G2 Loss: 5.0953, D1 Loss: 0.9046, D2 Loss: 0.0916\n",
      "Batch 2, G1 Loss: 1.9635, G2 Loss: 2.4120, D1 Loss: 0.7669, D2 Loss: 0.2666\n",
      "Batch 3, G1 Loss: 1.6321, G2 Loss: 5.2483, D1 Loss: 0.7963, D2 Loss: 0.1820\n",
      "Batch 4, G1 Loss: 1.1810, G2 Loss: 3.8776, D1 Loss: 0.8809, D2 Loss: 0.1545\n",
      "Batch 5, G1 Loss: 1.9315, G2 Loss: 3.2834, D1 Loss: 1.0107, D2 Loss: 0.2386\n",
      "Batch 6, G1 Loss: 1.0210, G2 Loss: 3.3606, D1 Loss: 1.2917, D2 Loss: 0.0814\n",
      "Batch 7, G1 Loss: 1.2105, G2 Loss: 3.3223, D1 Loss: 1.2436, D2 Loss: 0.4112\n",
      "Batch 8, G1 Loss: 1.0937, G2 Loss: 3.4767, D1 Loss: 1.4115, D2 Loss: 0.1015\n",
      "Batch 9, G1 Loss: 1.3196, G2 Loss: 3.3188, D1 Loss: 1.4591, D2 Loss: 0.1337\n",
      "Batch 10, G1 Loss: 0.7433, G2 Loss: 4.8234, D1 Loss: 1.8519, D2 Loss: 0.1633\n",
      "Batch 11, G1 Loss: 2.0941, G2 Loss: 3.0430, D1 Loss: 1.6021, D2 Loss: 0.2727\n",
      "Batch 12, G1 Loss: 0.6342, G2 Loss: 2.9187, D1 Loss: 1.8340, D2 Loss: 0.3195\n",
      "Batch 13, G1 Loss: 1.6394, G2 Loss: 3.6960, D1 Loss: 1.4210, D2 Loss: 0.3545\n",
      "Batch 14, G1 Loss: 0.6827, G2 Loss: 2.7175, D1 Loss: 1.6943, D2 Loss: 0.1919\n",
      "Batch 15, G1 Loss: 1.6976, G2 Loss: 4.2050, D1 Loss: 1.5079, D2 Loss: 0.1489\n",
      "Batch 16, G1 Loss: 0.7279, G2 Loss: 3.3175, D1 Loss: 1.5528, D2 Loss: 0.0820\n",
      "Batch 17, G1 Loss: 1.4627, G2 Loss: 3.8338, D1 Loss: 1.3073, D2 Loss: 0.1145\n",
      "Batch 18, G1 Loss: 1.0681, G2 Loss: 3.7791, D1 Loss: 1.2420, D2 Loss: 0.1168\n",
      "Batch 19, G1 Loss: 1.0809, G2 Loss: 3.2309, D1 Loss: 1.2601, D2 Loss: 0.3594\n",
      "Batch 20, G1 Loss: 1.2157, G2 Loss: 2.5005, D1 Loss: 1.3375, D2 Loss: 0.1885\n",
      "Batch 21, G1 Loss: 1.1146, G2 Loss: 4.2558, D1 Loss: 1.3078, D2 Loss: 0.1607\n",
      "Batch 22, G1 Loss: 0.9891, G2 Loss: 4.1862, D1 Loss: 1.3286, D2 Loss: 0.1805\n",
      "Batch 23, G1 Loss: 1.6471, G2 Loss: 3.1058, D1 Loss: 1.1118, D2 Loss: 0.2212\n",
      "Batch 24, G1 Loss: 1.0221, G2 Loss: 4.5570, D1 Loss: 1.2075, D2 Loss: 0.1744\n",
      "Batch 25, G1 Loss: 1.3370, G2 Loss: 5.3163, D1 Loss: 1.2299, D2 Loss: 0.3764\n",
      "Batch 26, G1 Loss: 1.1101, G2 Loss: 5.9319, D1 Loss: 1.0566, D2 Loss: 0.0675\n",
      "Batch 27, G1 Loss: 1.7106, G2 Loss: 0.7851, D1 Loss: 1.1160, D2 Loss: 1.2537\n",
      "Batch 28, G1 Loss: 1.1567, G2 Loss: 13.7203, D1 Loss: 1.0213, D2 Loss: 1.3619\n",
      "Batch 29, G1 Loss: 1.5540, G2 Loss: 10.6252, D1 Loss: 1.0929, D2 Loss: 0.5676\n",
      "Batch 30, G1 Loss: 1.1678, G2 Loss: 4.1322, D1 Loss: 1.1111, D2 Loss: 0.0692\n",
      "Batch 31, G1 Loss: 1.6133, G2 Loss: 1.4463, D1 Loss: 1.1258, D2 Loss: 0.5165\n",
      "Batch 32, G1 Loss: 0.9641, G2 Loss: 5.3654, D1 Loss: 1.1663, D2 Loss: 0.4286\n",
      "Epoch 46 Average -> G1 Loss: 1.279122233390808, G2 Loss: 4.215245723724365, D1 Loss: 1.2578697204589844, D2 Loss: 0.2922284007072449\n",
      "Epoch 47/100\n",
      "Batch 1, G1 Loss: 1.9762, G2 Loss: 3.8309, D1 Loss: 1.0136, D2 Loss: 0.2038\n",
      "Batch 2, G1 Loss: 1.1481, G2 Loss: 2.2131, D1 Loss: 0.9929, D2 Loss: 0.3625\n",
      "Batch 3, G1 Loss: 1.3850, G2 Loss: 3.7000, D1 Loss: 1.0230, D2 Loss: 0.1892\n",
      "Batch 4, G1 Loss: 1.5323, G2 Loss: 3.1788, D1 Loss: 0.9127, D2 Loss: 0.1390\n",
      "Batch 5, G1 Loss: 1.0578, G2 Loss: 2.9780, D1 Loss: 1.0278, D2 Loss: 0.3919\n",
      "Batch 6, G1 Loss: 1.1669, G2 Loss: 1.8806, D1 Loss: 1.1403, D2 Loss: 0.4246\n",
      "Batch 7, G1 Loss: 1.5959, G2 Loss: 5.9676, D1 Loss: 1.3624, D2 Loss: 0.6137\n",
      "Batch 8, G1 Loss: 0.8610, G2 Loss: 2.7522, D1 Loss: 1.1800, D2 Loss: 0.1565\n",
      "Batch 9, G1 Loss: 1.6999, G2 Loss: 3.5981, D1 Loss: 0.9451, D2 Loss: 0.1414\n",
      "Batch 10, G1 Loss: 1.6813, G2 Loss: 7.6101, D1 Loss: 0.8841, D2 Loss: 0.0366\n",
      "Batch 11, G1 Loss: 1.0021, G2 Loss: 3.7353, D1 Loss: 0.9567, D2 Loss: 0.0840\n",
      "Batch 12, G1 Loss: 1.3668, G2 Loss: 10.2619, D1 Loss: 1.2890, D2 Loss: 0.0546\n",
      "Batch 13, G1 Loss: 1.1574, G2 Loss: 8.1961, D1 Loss: 1.1079, D2 Loss: 0.0020\n",
      "Batch 14, G1 Loss: 1.3048, G2 Loss: 6.7044, D1 Loss: 1.2259, D2 Loss: 0.0124\n",
      "Batch 15, G1 Loss: 1.0769, G2 Loss: 3.2396, D1 Loss: 1.1885, D2 Loss: 0.1128\n",
      "Batch 16, G1 Loss: 1.1948, G2 Loss: 4.1917, D1 Loss: 1.1289, D2 Loss: 0.0362\n",
      "Batch 17, G1 Loss: 1.4613, G2 Loss: 2.8141, D1 Loss: 0.9848, D2 Loss: 0.1925\n",
      "Batch 18, G1 Loss: 1.0969, G2 Loss: 5.7835, D1 Loss: 1.2532, D2 Loss: 0.0489\n",
      "Batch 19, G1 Loss: 1.2788, G2 Loss: 5.1615, D1 Loss: 1.1439, D2 Loss: 0.0903\n",
      "Batch 20, G1 Loss: 1.3599, G2 Loss: 3.6876, D1 Loss: 1.1809, D2 Loss: 0.1118\n",
      "Batch 21, G1 Loss: 0.8054, G2 Loss: 6.2867, D1 Loss: 1.4656, D2 Loss: 0.0153\n",
      "Batch 22, G1 Loss: 1.8744, G2 Loss: 1.6747, D1 Loss: 1.3479, D2 Loss: 0.5394\n",
      "Batch 23, G1 Loss: 0.8152, G2 Loss: 8.7966, D1 Loss: 1.1890, D2 Loss: 0.0500\n",
      "Batch 24, G1 Loss: 1.7153, G2 Loss: 9.8399, D1 Loss: 1.0268, D2 Loss: 0.2713\n",
      "Batch 25, G1 Loss: 1.1917, G2 Loss: 6.6242, D1 Loss: 0.9987, D2 Loss: 0.0781\n",
      "Batch 26, G1 Loss: 1.3357, G2 Loss: 4.5251, D1 Loss: 1.1413, D2 Loss: 0.1107\n",
      "Batch 27, G1 Loss: 1.1611, G2 Loss: 2.2228, D1 Loss: 1.0870, D2 Loss: 0.3450\n",
      "Batch 28, G1 Loss: 1.0656, G2 Loss: 7.2183, D1 Loss: 1.2178, D2 Loss: 0.1251\n",
      "Batch 29, G1 Loss: 1.3253, G2 Loss: 5.6482, D1 Loss: 1.2313, D2 Loss: 0.1637\n",
      "Batch 30, G1 Loss: 0.8259, G2 Loss: 3.5232, D1 Loss: 1.3929, D2 Loss: 0.1639\n",
      "Batch 31, G1 Loss: 2.0690, G2 Loss: 3.0245, D1 Loss: 1.4820, D2 Loss: 0.2178\n",
      "Batch 32, G1 Loss: 0.4879, G2 Loss: 3.0424, D1 Loss: 1.9800, D2 Loss: 0.2567\n",
      "Epoch 47 Average -> G1 Loss: 1.283631443977356, G2 Loss: 4.809743881225586, D1 Loss: 1.1719352006912231, D2 Loss: 0.17942965030670166\n",
      "Epoch 48/100\n",
      "Batch 1, G1 Loss: 2.4521, G2 Loss: 3.5803, D1 Loss: 1.4538, D2 Loss: 0.1400\n",
      "Batch 2, G1 Loss: 0.6437, G2 Loss: 3.1778, D1 Loss: 1.5726, D2 Loss: 0.1213\n",
      "Batch 3, G1 Loss: 1.8634, G2 Loss: 3.5937, D1 Loss: 1.4244, D2 Loss: 0.4061\n",
      "Batch 4, G1 Loss: 0.6739, G2 Loss: 5.7856, D1 Loss: 1.6056, D2 Loss: 0.1189\n",
      "Batch 5, G1 Loss: 1.6584, G2 Loss: 1.6198, D1 Loss: 1.1420, D2 Loss: 0.5815\n",
      "Batch 6, G1 Loss: 1.0462, G2 Loss: 9.4265, D1 Loss: 1.1395, D2 Loss: 1.1288\n",
      "Batch 7, G1 Loss: 1.4518, G2 Loss: 5.1810, D1 Loss: 1.0437, D2 Loss: 0.1054\n",
      "Batch 8, G1 Loss: 1.0214, G2 Loss: 2.1058, D1 Loss: 1.1859, D2 Loss: 0.2617\n",
      "Batch 9, G1 Loss: 1.4450, G2 Loss: 4.2029, D1 Loss: 1.0727, D2 Loss: 0.3114\n",
      "Batch 10, G1 Loss: 1.2675, G2 Loss: 2.7056, D1 Loss: 1.0533, D2 Loss: 0.1900\n",
      "Batch 11, G1 Loss: 1.1136, G2 Loss: 3.4603, D1 Loss: 1.0285, D2 Loss: 0.1605\n",
      "Batch 12, G1 Loss: 1.4489, G2 Loss: 3.2219, D1 Loss: 1.0765, D2 Loss: 0.1564\n",
      "Batch 13, G1 Loss: 1.0592, G2 Loss: 3.0880, D1 Loss: 0.9575, D2 Loss: 0.1807\n",
      "Batch 14, G1 Loss: 1.8396, G2 Loss: 3.1453, D1 Loss: 1.0401, D2 Loss: 0.2311\n",
      "Batch 15, G1 Loss: 0.9078, G2 Loss: 6.3753, D1 Loss: 1.0735, D2 Loss: 0.1540\n",
      "Batch 16, G1 Loss: 2.1037, G2 Loss: 8.6369, D1 Loss: 1.0110, D2 Loss: 0.1106\n",
      "Batch 17, G1 Loss: 0.9866, G2 Loss: 2.1513, D1 Loss: 0.9455, D2 Loss: 0.3790\n",
      "Batch 18, G1 Loss: 1.9909, G2 Loss: 5.8218, D1 Loss: 0.9456, D2 Loss: 0.2118\n",
      "Batch 19, G1 Loss: 1.1081, G2 Loss: 4.8227, D1 Loss: 1.0862, D2 Loss: 0.1420\n",
      "Batch 20, G1 Loss: 1.2397, G2 Loss: 3.5118, D1 Loss: 1.2487, D2 Loss: 0.1932\n",
      "Batch 21, G1 Loss: 1.1853, G2 Loss: 3.0488, D1 Loss: 1.0067, D2 Loss: 0.1014\n",
      "Batch 22, G1 Loss: 2.6201, G2 Loss: 4.9734, D1 Loss: 1.0973, D2 Loss: 0.2011\n",
      "Batch 23, G1 Loss: 0.6727, G2 Loss: 4.1933, D1 Loss: 1.5912, D2 Loss: 0.1154\n",
      "Batch 24, G1 Loss: 3.8140, G2 Loss: 2.4085, D1 Loss: 1.2206, D2 Loss: 0.2878\n",
      "Batch 25, G1 Loss: 0.6901, G2 Loss: 6.3248, D1 Loss: 1.5521, D2 Loss: 0.2253\n",
      "Batch 26, G1 Loss: 1.5202, G2 Loss: 6.2100, D1 Loss: 1.1960, D2 Loss: 0.2781\n",
      "Batch 27, G1 Loss: 1.0655, G2 Loss: 3.5530, D1 Loss: 1.5938, D2 Loss: 0.1154\n",
      "Batch 28, G1 Loss: 1.0009, G2 Loss: 2.4304, D1 Loss: 1.5848, D2 Loss: 0.3065\n",
      "Batch 29, G1 Loss: 1.3494, G2 Loss: 4.8720, D1 Loss: 1.3483, D2 Loss: 0.1657\n",
      "Batch 30, G1 Loss: 0.8345, G2 Loss: 4.2629, D1 Loss: 1.3404, D2 Loss: 0.1788\n",
      "Batch 31, G1 Loss: 2.0205, G2 Loss: 2.6733, D1 Loss: 1.5777, D2 Loss: 0.1972\n",
      "Batch 32, G1 Loss: 0.5020, G2 Loss: 3.2266, D1 Loss: 1.9111, D2 Loss: 0.1498\n",
      "Epoch 48 Average -> G1 Loss: 1.3936402797698975, G2 Loss: 4.180974960327148, D1 Loss: 1.2539587020874023, D2 Loss: 0.2377309650182724\n",
      "Epoch 49/100\n",
      "Batch 1, G1 Loss: 2.4249, G2 Loss: 3.7351, D1 Loss: 1.4051, D2 Loss: 0.1469\n",
      "Batch 2, G1 Loss: 0.8104, G2 Loss: 6.0433, D1 Loss: 1.2778, D2 Loss: 0.1008\n",
      "Batch 3, G1 Loss: 1.4410, G2 Loss: 3.0673, D1 Loss: 1.1889, D2 Loss: 0.1805\n",
      "Batch 4, G1 Loss: 0.9092, G2 Loss: 2.8663, D1 Loss: 1.4927, D2 Loss: 0.4002\n",
      "Batch 5, G1 Loss: 1.2237, G2 Loss: 4.3831, D1 Loss: 1.1173, D2 Loss: 0.1208\n",
      "Batch 6, G1 Loss: 1.4928, G2 Loss: 3.4913, D1 Loss: 1.0209, D2 Loss: 0.2212\n",
      "Batch 7, G1 Loss: 1.3258, G2 Loss: 2.7528, D1 Loss: 1.0240, D2 Loss: 0.2351\n",
      "Batch 8, G1 Loss: 1.4834, G2 Loss: 3.9476, D1 Loss: 0.7740, D2 Loss: 0.2146\n",
      "Batch 9, G1 Loss: 1.7871, G2 Loss: 3.2750, D1 Loss: 0.7693, D2 Loss: 0.1962\n",
      "Batch 10, G1 Loss: 1.0261, G2 Loss: 10.0443, D1 Loss: 1.2544, D2 Loss: 0.0948\n",
      "Batch 11, G1 Loss: 1.3238, G2 Loss: 3.9475, D1 Loss: 1.3754, D2 Loss: 0.1131\n",
      "Batch 12, G1 Loss: 0.9032, G2 Loss: 2.5924, D1 Loss: 1.4231, D2 Loss: 0.1697\n",
      "Batch 13, G1 Loss: 2.3894, G2 Loss: 5.8314, D1 Loss: 1.3049, D2 Loss: 0.1130\n",
      "Batch 14, G1 Loss: 0.6792, G2 Loss: 5.4972, D1 Loss: 1.3986, D2 Loss: 0.2334\n",
      "Batch 15, G1 Loss: 2.4202, G2 Loss: 2.5424, D1 Loss: 1.1904, D2 Loss: 0.3270\n",
      "Batch 16, G1 Loss: 1.1783, G2 Loss: 3.7910, D1 Loss: 0.9241, D2 Loss: 0.0930\n",
      "Batch 17, G1 Loss: 1.4119, G2 Loss: 3.6757, D1 Loss: 0.8550, D2 Loss: 0.1186\n",
      "Batch 18, G1 Loss: 1.4623, G2 Loss: 3.8001, D1 Loss: 0.9467, D2 Loss: 0.0860\n",
      "Batch 19, G1 Loss: 1.4721, G2 Loss: 4.3058, D1 Loss: 1.2794, D2 Loss: 0.1514\n",
      "Batch 20, G1 Loss: 1.1024, G2 Loss: 6.4600, D1 Loss: 1.2765, D2 Loss: 0.1280\n",
      "Batch 21, G1 Loss: 1.6418, G2 Loss: 2.8274, D1 Loss: 1.0876, D2 Loss: 0.1516\n",
      "Batch 22, G1 Loss: 1.3242, G2 Loss: 3.6229, D1 Loss: 0.9906, D2 Loss: 0.1319\n",
      "Batch 23, G1 Loss: 1.5821, G2 Loss: 4.0136, D1 Loss: 0.9034, D2 Loss: 0.1619\n",
      "Batch 24, G1 Loss: 1.5238, G2 Loss: 3.5524, D1 Loss: 0.9318, D2 Loss: 0.2546\n",
      "Batch 25, G1 Loss: 1.4149, G2 Loss: 2.1820, D1 Loss: 0.9918, D2 Loss: 0.3404\n",
      "Batch 26, G1 Loss: 1.6483, G2 Loss: 6.9480, D1 Loss: 0.7715, D2 Loss: 0.1885\n",
      "Batch 27, G1 Loss: 1.6489, G2 Loss: 6.2984, D1 Loss: 1.0282, D2 Loss: 0.4151\n",
      "Batch 28, G1 Loss: 1.3026, G2 Loss: 2.3409, D1 Loss: 0.8383, D2 Loss: 0.2030\n",
      "Batch 29, G1 Loss: 1.6625, G2 Loss: 8.5055, D1 Loss: 0.9132, D2 Loss: 0.1028\n",
      "Batch 30, G1 Loss: 1.1869, G2 Loss: 4.3500, D1 Loss: 1.1587, D2 Loss: 0.1744\n",
      "Batch 31, G1 Loss: 1.3826, G2 Loss: 1.7012, D1 Loss: 1.1933, D2 Loss: 0.4399\n",
      "Batch 32, G1 Loss: 1.1085, G2 Loss: 7.4190, D1 Loss: 1.1486, D2 Loss: 0.4574\n",
      "Epoch 49 Average -> G1 Loss: 1.4279485940933228, G2 Loss: 4.369090557098389, D1 Loss: 1.1017444133758545, D2 Loss: 0.20206624269485474\n",
      "Epoch 50/100\n",
      "Batch 1, G1 Loss: 1.4406, G2 Loss: 6.0022, D1 Loss: 1.3885, D2 Loss: 0.1474\n",
      "Batch 2, G1 Loss: 1.0250, G2 Loss: 2.5481, D1 Loss: 1.1965, D2 Loss: 0.2710\n",
      "Batch 3, G1 Loss: 1.7268, G2 Loss: 4.7286, D1 Loss: 1.4391, D2 Loss: 0.2095\n",
      "Batch 4, G1 Loss: 0.6927, G2 Loss: 3.3404, D1 Loss: 1.5666, D2 Loss: 0.1064\n",
      "Batch 5, G1 Loss: 2.4076, G2 Loss: 3.6487, D1 Loss: 1.2560, D2 Loss: 0.1398\n",
      "Batch 6, G1 Loss: 0.6735, G2 Loss: 3.4067, D1 Loss: 1.5038, D2 Loss: 0.1541\n",
      "Batch 7, G1 Loss: 2.2224, G2 Loss: 9.4705, D1 Loss: 1.5439, D2 Loss: 0.1987\n",
      "Batch 8, G1 Loss: 0.7139, G2 Loss: 2.7939, D1 Loss: 1.5201, D2 Loss: 0.1958\n",
      "Batch 9, G1 Loss: 1.9935, G2 Loss: 4.9223, D1 Loss: 1.1207, D2 Loss: 0.3715\n",
      "Batch 10, G1 Loss: 0.9146, G2 Loss: 2.3491, D1 Loss: 1.2447, D2 Loss: 0.1758\n",
      "Batch 11, G1 Loss: 1.8711, G2 Loss: 4.1824, D1 Loss: 1.3786, D2 Loss: 0.0853\n",
      "Batch 12, G1 Loss: 0.6797, G2 Loss: 4.1771, D1 Loss: 1.5001, D2 Loss: 0.1878\n",
      "Batch 13, G1 Loss: 1.7376, G2 Loss: 3.6905, D1 Loss: 1.1579, D2 Loss: 0.2534\n",
      "Batch 14, G1 Loss: 1.0998, G2 Loss: 2.5780, D1 Loss: 1.2066, D2 Loss: 0.2865\n",
      "Batch 15, G1 Loss: 1.1987, G2 Loss: 6.1463, D1 Loss: 1.1061, D2 Loss: 0.0708\n",
      "Batch 16, G1 Loss: 1.2754, G2 Loss: 5.2872, D1 Loss: 1.1481, D2 Loss: 0.0884\n",
      "Batch 17, G1 Loss: 0.9479, G2 Loss: 3.3573, D1 Loss: 1.2632, D2 Loss: 0.2531\n",
      "Batch 18, G1 Loss: 1.3444, G2 Loss: 3.5021, D1 Loss: 1.3789, D2 Loss: 0.1442\n",
      "Batch 19, G1 Loss: 1.0373, G2 Loss: 10.0923, D1 Loss: 1.2795, D2 Loss: 0.1405\n",
      "Batch 20, G1 Loss: 1.1630, G2 Loss: 2.2537, D1 Loss: 1.0219, D2 Loss: 0.3019\n",
      "Batch 21, G1 Loss: 1.7752, G2 Loss: 3.8146, D1 Loss: 1.3116, D2 Loss: 0.1443\n",
      "Batch 22, G1 Loss: 0.7631, G2 Loss: 4.0128, D1 Loss: 1.2626, D2 Loss: 0.1955\n",
      "Batch 23, G1 Loss: 2.2176, G2 Loss: 2.1048, D1 Loss: 1.1026, D2 Loss: 0.2802\n",
      "Batch 24, G1 Loss: 1.0043, G2 Loss: 6.8886, D1 Loss: 1.0311, D2 Loss: 0.1008\n",
      "Batch 25, G1 Loss: 1.5819, G2 Loss: 6.8056, D1 Loss: 0.9287, D2 Loss: 0.2401\n",
      "Batch 26, G1 Loss: 1.2456, G2 Loss: 3.9605, D1 Loss: 1.0004, D2 Loss: 0.1118\n",
      "Batch 27, G1 Loss: 1.3414, G2 Loss: 4.9963, D1 Loss: 1.0342, D2 Loss: 0.0349\n",
      "Batch 28, G1 Loss: 1.1361, G2 Loss: 2.1504, D1 Loss: 1.0898, D2 Loss: 0.2981\n",
      "Batch 29, G1 Loss: 1.8052, G2 Loss: 7.7669, D1 Loss: 1.2174, D2 Loss: 0.1286\n",
      "Batch 30, G1 Loss: 0.9093, G2 Loss: 7.3236, D1 Loss: 1.3243, D2 Loss: 0.1551\n",
      "Batch 31, G1 Loss: 1.3783, G2 Loss: 4.7945, D1 Loss: 1.1959, D2 Loss: 0.0867\n",
      "Batch 32, G1 Loss: 1.0567, G2 Loss: 2.6532, D1 Loss: 1.1407, D2 Loss: 0.1968\n",
      "Epoch 50 Average -> G1 Loss: 1.324378490447998, G2 Loss: 4.554666519165039, D1 Loss: 1.2456377744674683, D2 Loss: 0.17983382940292358\n",
      "Epoch 51/100\n",
      "Batch 1, G1 Loss: 1.3307, G2 Loss: 3.5349, D1 Loss: 1.0321, D2 Loss: 0.1387\n",
      "Batch 2, G1 Loss: 1.5872, G2 Loss: 4.4536, D1 Loss: 0.9821, D2 Loss: 0.1221\n",
      "Batch 3, G1 Loss: 0.9551, G2 Loss: 3.6601, D1 Loss: 1.1075, D2 Loss: 0.0647\n",
      "Batch 4, G1 Loss: 1.6182, G2 Loss: 3.6048, D1 Loss: 0.9342, D2 Loss: 0.0913\n",
      "Batch 5, G1 Loss: 1.3245, G2 Loss: 6.8915, D1 Loss: 0.9865, D2 Loss: 0.1257\n",
      "Batch 6, G1 Loss: 1.3389, G2 Loss: 3.1073, D1 Loss: 1.0987, D2 Loss: 0.1435\n",
      "Batch 7, G1 Loss: 1.5431, G2 Loss: 3.6614, D1 Loss: 0.9882, D2 Loss: 0.1316\n",
      "Batch 8, G1 Loss: 1.3284, G2 Loss: 4.1593, D1 Loss: 1.1949, D2 Loss: 0.4027\n",
      "Batch 9, G1 Loss: 0.7443, G2 Loss: 1.5970, D1 Loss: 1.3533, D2 Loss: 0.4472\n",
      "Batch 10, G1 Loss: 3.0341, G2 Loss: 7.9569, D1 Loss: 1.6895, D2 Loss: 1.5020\n",
      "Batch 11, G1 Loss: 0.4455, G2 Loss: 3.0329, D1 Loss: 2.0993, D2 Loss: 0.1522\n",
      "Batch 12, G1 Loss: 2.0175, G2 Loss: 5.2770, D1 Loss: 1.3574, D2 Loss: 0.1784\n",
      "Batch 13, G1 Loss: 0.8446, G2 Loss: 1.5578, D1 Loss: 1.3844, D2 Loss: 0.3861\n",
      "Batch 14, G1 Loss: 1.5356, G2 Loss: 8.9660, D1 Loss: 1.1482, D2 Loss: 2.0406\n",
      "Batch 15, G1 Loss: 1.1867, G2 Loss: 1.4915, D1 Loss: 1.0889, D2 Loss: 0.5309\n",
      "Batch 16, G1 Loss: 1.5075, G2 Loss: 7.2115, D1 Loss: 1.1161, D2 Loss: 0.1808\n",
      "Batch 17, G1 Loss: 0.9169, G2 Loss: 6.6088, D1 Loss: 1.2153, D2 Loss: 0.2662\n",
      "Batch 18, G1 Loss: 2.1759, G2 Loss: 3.0629, D1 Loss: 1.1295, D2 Loss: 0.1790\n",
      "Batch 19, G1 Loss: 0.9562, G2 Loss: 2.3667, D1 Loss: 1.0309, D2 Loss: 0.2722\n",
      "Batch 20, G1 Loss: 1.7814, G2 Loss: 5.0917, D1 Loss: 0.9082, D2 Loss: 0.0353\n",
      "Batch 21, G1 Loss: 1.2723, G2 Loss: 5.7975, D1 Loss: 0.9148, D2 Loss: 0.1241\n",
      "Batch 22, G1 Loss: 1.3403, G2 Loss: 4.5552, D1 Loss: 0.9296, D2 Loss: 0.0829\n",
      "Batch 23, G1 Loss: 1.3793, G2 Loss: 4.2556, D1 Loss: 0.8559, D2 Loss: 0.2262\n",
      "Batch 24, G1 Loss: 1.7640, G2 Loss: 2.7092, D1 Loss: 0.8599, D2 Loss: 0.3779\n",
      "Batch 25, G1 Loss: 1.2018, G2 Loss: 6.5167, D1 Loss: 1.0364, D2 Loss: 0.2325\n",
      "Batch 26, G1 Loss: 1.8701, G2 Loss: 5.7795, D1 Loss: 0.9130, D2 Loss: 0.6778\n",
      "Batch 27, G1 Loss: 1.1255, G2 Loss: 1.2809, D1 Loss: 0.8601, D2 Loss: 1.3100\n",
      "Batch 28, G1 Loss: 2.4405, G2 Loss: 9.5483, D1 Loss: 0.6845, D2 Loss: 0.1309\n",
      "Batch 29, G1 Loss: 1.3295, G2 Loss: 10.6477, D1 Loss: 0.8228, D2 Loss: 0.6162\n",
      "Batch 30, G1 Loss: 1.8465, G2 Loss: 5.4694, D1 Loss: 0.6884, D2 Loss: 0.1605\n",
      "Batch 31, G1 Loss: 0.9217, G2 Loss: 2.4803, D1 Loss: 1.1649, D2 Loss: 0.2590\n",
      "Batch 32, G1 Loss: 2.7873, G2 Loss: 7.4153, D1 Loss: 1.7112, D2 Loss: 0.1215\n",
      "Epoch 51 Average -> G1 Loss: 1.4828511476516724, G2 Loss: 4.804659843444824, D1 Loss: 1.1027041673660278, D2 Loss: 0.3659549653530121\n",
      "Epoch 52/100\n",
      "Batch 1, G1 Loss: 0.4796, G2 Loss: 2.3644, D1 Loss: 2.1592, D2 Loss: 0.4231\n",
      "Batch 2, G1 Loss: 3.6513, G2 Loss: 6.5041, D1 Loss: 1.1286, D2 Loss: 0.2373\n",
      "Batch 3, G1 Loss: 1.4139, G2 Loss: 5.0539, D1 Loss: 0.6673, D2 Loss: 0.1391\n",
      "Batch 4, G1 Loss: 1.1729, G2 Loss: 3.0075, D1 Loss: 0.8038, D2 Loss: 0.1095\n",
      "Batch 5, G1 Loss: 1.7524, G2 Loss: 3.4554, D1 Loss: 0.8765, D2 Loss: 0.1967\n",
      "Batch 6, G1 Loss: 1.7090, G2 Loss: 3.5278, D1 Loss: 0.7603, D2 Loss: 0.1325\n",
      "Batch 7, G1 Loss: 1.7009, G2 Loss: 3.3826, D1 Loss: 0.5339, D2 Loss: 0.1275\n",
      "Batch 8, G1 Loss: 2.1413, G2 Loss: 3.1745, D1 Loss: 0.4131, D2 Loss: 0.5852\n",
      "Batch 9, G1 Loss: 2.3448, G2 Loss: 2.2896, D1 Loss: 0.4614, D2 Loss: 0.4365\n",
      "Batch 10, G1 Loss: 2.0735, G2 Loss: 4.9624, D1 Loss: 0.4096, D2 Loss: 0.3576\n",
      "Batch 11, G1 Loss: 1.7830, G2 Loss: 4.0881, D1 Loss: 0.3678, D2 Loss: 0.1079\n",
      "Batch 12, G1 Loss: 2.4655, G2 Loss: 6.6570, D1 Loss: 0.3934, D2 Loss: 0.0373\n",
      "Batch 13, G1 Loss: 1.8971, G2 Loss: 1.3666, D1 Loss: 0.8083, D2 Loss: 0.5620\n",
      "Batch 14, G1 Loss: 1.5392, G2 Loss: 9.7985, D1 Loss: 0.6670, D2 Loss: 0.2968\n",
      "Batch 15, G1 Loss: 2.1226, G2 Loss: 8.7274, D1 Loss: 0.4995, D2 Loss: 0.5462\n",
      "Batch 16, G1 Loss: 2.3685, G2 Loss: 4.8141, D1 Loss: 0.4667, D2 Loss: 0.0554\n",
      "Batch 17, G1 Loss: 1.6322, G2 Loss: 1.8415, D1 Loss: 0.7614, D2 Loss: 0.5072\n",
      "Batch 18, G1 Loss: 2.1325, G2 Loss: 5.4006, D1 Loss: 0.5777, D2 Loss: 0.1126\n",
      "Batch 19, G1 Loss: 2.0776, G2 Loss: 5.4892, D1 Loss: 0.6194, D2 Loss: 0.0966\n",
      "Batch 20, G1 Loss: 1.1736, G2 Loss: 4.6739, D1 Loss: 0.9188, D2 Loss: 0.0737\n",
      "Batch 21, G1 Loss: 2.5012, G2 Loss: 3.1617, D1 Loss: 0.9653, D2 Loss: 0.2019\n",
      "Batch 22, G1 Loss: 0.9069, G2 Loss: 3.3410, D1 Loss: 1.2383, D2 Loss: 0.1485\n",
      "Batch 23, G1 Loss: 2.4905, G2 Loss: 4.8299, D1 Loss: 1.3196, D2 Loss: 0.2294\n",
      "Batch 24, G1 Loss: 0.8239, G2 Loss: 2.9636, D1 Loss: 1.3805, D2 Loss: 0.1881\n",
      "Batch 25, G1 Loss: 2.2603, G2 Loss: 4.9730, D1 Loss: 0.9816, D2 Loss: 0.2264\n",
      "Batch 26, G1 Loss: 1.1114, G2 Loss: 5.6161, D1 Loss: 1.2419, D2 Loss: 0.0642\n",
      "Batch 27, G1 Loss: 1.2511, G2 Loss: 1.3135, D1 Loss: 1.1034, D2 Loss: 0.7890\n",
      "Batch 28, G1 Loss: 1.3975, G2 Loss: 9.9190, D1 Loss: 1.2411, D2 Loss: 1.1739\n",
      "Batch 29, G1 Loss: 1.4884, G2 Loss: 7.0666, D1 Loss: 0.8645, D2 Loss: 0.2877\n",
      "Batch 30, G1 Loss: 1.5506, G2 Loss: 3.1076, D1 Loss: 0.6324, D2 Loss: 0.2285\n",
      "Batch 31, G1 Loss: 1.5390, G2 Loss: 3.1680, D1 Loss: 0.8575, D2 Loss: 0.1743\n",
      "Batch 32, G1 Loss: 0.8885, G2 Loss: 2.9519, D1 Loss: 1.3357, D2 Loss: 0.1750\n",
      "Epoch 52 Average -> G1 Loss: 1.7450218200683594, G2 Loss: 4.468465328216553, D1 Loss: 0.8579796552658081, D2 Loss: 0.28210869431495667\n",
      "Epoch 53/100\n",
      "Batch 1, G1 Loss: 2.0771, G2 Loss: 3.3632, D1 Loss: 1.4451, D2 Loss: 0.1879\n",
      "Batch 2, G1 Loss: 0.3627, G2 Loss: 3.6631, D1 Loss: 2.5234, D2 Loss: 0.3464\n",
      "Batch 3, G1 Loss: 3.2407, G2 Loss: 1.6084, D1 Loss: 1.7011, D2 Loss: 0.4474\n",
      "Batch 4, G1 Loss: 1.1635, G2 Loss: 7.7375, D1 Loss: 0.9689, D2 Loss: 0.8445\n",
      "Batch 5, G1 Loss: 0.7340, G2 Loss: 4.5125, D1 Loss: 1.2002, D2 Loss: 0.0664\n",
      "Batch 6, G1 Loss: 2.9552, G2 Loss: 5.1847, D1 Loss: 1.3088, D2 Loss: 0.0315\n",
      "Batch 7, G1 Loss: 0.8670, G2 Loss: 1.6010, D1 Loss: 1.1573, D2 Loss: 0.5868\n",
      "Batch 8, G1 Loss: 1.4313, G2 Loss: 8.7355, D1 Loss: 0.9149, D2 Loss: 0.3622\n",
      "Batch 9, G1 Loss: 1.5389, G2 Loss: 5.9254, D1 Loss: 0.9110, D2 Loss: 0.0330\n",
      "Batch 10, G1 Loss: 1.1820, G2 Loss: 4.2880, D1 Loss: 0.9387, D2 Loss: 0.0479\n",
      "Batch 11, G1 Loss: 1.4255, G2 Loss: 2.7899, D1 Loss: 0.9396, D2 Loss: 0.1578\n",
      "Batch 12, G1 Loss: 1.8552, G2 Loss: 3.0849, D1 Loss: 1.0192, D2 Loss: 0.1513\n",
      "Batch 13, G1 Loss: 0.9273, G2 Loss: 3.7589, D1 Loss: 1.1143, D2 Loss: 0.1160\n",
      "Batch 14, G1 Loss: 2.2996, G2 Loss: 3.4754, D1 Loss: 0.9063, D2 Loss: 0.1359\n",
      "Batch 15, G1 Loss: 1.1843, G2 Loss: 2.7136, D1 Loss: 0.8772, D2 Loss: 0.3008\n",
      "Batch 16, G1 Loss: 1.5153, G2 Loss: 3.4342, D1 Loss: 0.8904, D2 Loss: 0.1651\n",
      "Batch 17, G1 Loss: 1.6171, G2 Loss: 3.1810, D1 Loss: 0.8480, D2 Loss: 0.2403\n",
      "Batch 18, G1 Loss: 1.0765, G2 Loss: 5.3663, D1 Loss: 1.1146, D2 Loss: 0.3973\n",
      "Batch 19, G1 Loss: 1.9011, G2 Loss: 4.6272, D1 Loss: 0.9672, D2 Loss: 0.0546\n",
      "Batch 20, G1 Loss: 1.0245, G2 Loss: 1.6890, D1 Loss: 1.1089, D2 Loss: 0.3246\n",
      "Batch 21, G1 Loss: 1.7357, G2 Loss: 7.9467, D1 Loss: 1.2333, D2 Loss: 0.1989\n",
      "Batch 22, G1 Loss: 0.7724, G2 Loss: 6.9417, D1 Loss: 1.4634, D2 Loss: 0.1638\n",
      "Batch 23, G1 Loss: 2.4325, G2 Loss: 4.4659, D1 Loss: 1.1783, D2 Loss: 0.0565\n",
      "Batch 24, G1 Loss: 0.7635, G2 Loss: 2.4010, D1 Loss: 1.6529, D2 Loss: 0.3084\n",
      "Batch 25, G1 Loss: 1.9967, G2 Loss: 4.4288, D1 Loss: 1.1176, D2 Loss: 0.3072\n",
      "Batch 26, G1 Loss: 1.1941, G2 Loss: 3.5580, D1 Loss: 0.8836, D2 Loss: 0.1692\n",
      "Batch 27, G1 Loss: 1.8028, G2 Loss: 5.4815, D1 Loss: 1.1754, D2 Loss: 0.0693\n",
      "Batch 28, G1 Loss: 1.2120, G2 Loss: 5.9374, D1 Loss: 0.9418, D2 Loss: 0.1311\n",
      "Batch 29, G1 Loss: 1.5432, G2 Loss: 2.0290, D1 Loss: 0.7630, D2 Loss: 0.3464\n",
      "Batch 30, G1 Loss: 2.2816, G2 Loss: 5.4745, D1 Loss: 0.7310, D2 Loss: 0.1605\n",
      "Batch 31, G1 Loss: 1.0526, G2 Loss: 4.6597, D1 Loss: 1.0581, D2 Loss: 0.1503\n",
      "Batch 32, G1 Loss: 1.5389, G2 Loss: 3.1244, D1 Loss: 1.0435, D2 Loss: 0.2206\n",
      "Epoch 53 Average -> G1 Loss: 1.5220286846160889, G2 Loss: 4.28713846206665, D1 Loss: 1.128036618232727, D2 Loss: 0.2274995595216751\n",
      "Epoch 54/100\n",
      "Batch 1, G1 Loss: 1.3059, G2 Loss: 2.9412, D1 Loss: 1.1208, D2 Loss: 0.1812\n",
      "Batch 2, G1 Loss: 1.5123, G2 Loss: 3.9572, D1 Loss: 1.1832, D2 Loss: 0.1632\n",
      "Batch 3, G1 Loss: 0.8042, G2 Loss: 3.8062, D1 Loss: 1.3632, D2 Loss: 0.1319\n",
      "Batch 4, G1 Loss: 2.7846, G2 Loss: 2.7507, D1 Loss: 1.0520, D2 Loss: 0.2560\n",
      "Batch 5, G1 Loss: 0.6850, G2 Loss: 4.6543, D1 Loss: 1.5791, D2 Loss: 0.2353\n",
      "Batch 6, G1 Loss: 2.6249, G2 Loss: 4.3800, D1 Loss: 1.5799, D2 Loss: 0.2461\n",
      "Batch 7, G1 Loss: 0.6867, G2 Loss: 5.3840, D1 Loss: 2.1831, D2 Loss: 0.1099\n",
      "Batch 8, G1 Loss: 3.1276, G2 Loss: 4.6695, D1 Loss: 1.0095, D2 Loss: 0.0741\n",
      "Batch 9, G1 Loss: 1.2920, G2 Loss: 3.0606, D1 Loss: 0.9610, D2 Loss: 0.2455\n",
      "Batch 10, G1 Loss: 1.2694, G2 Loss: 4.0107, D1 Loss: 0.8858, D2 Loss: 0.0947\n",
      "Batch 11, G1 Loss: 1.7001, G2 Loss: 5.2466, D1 Loss: 0.9839, D2 Loss: 0.2920\n",
      "Batch 12, G1 Loss: 1.0082, G2 Loss: 3.6043, D1 Loss: 1.0469, D2 Loss: 0.1504\n",
      "Batch 13, G1 Loss: 1.8846, G2 Loss: 2.9247, D1 Loss: 1.1147, D2 Loss: 0.2664\n",
      "Batch 14, G1 Loss: 1.3165, G2 Loss: 4.2123, D1 Loss: 1.0799, D2 Loss: 0.1573\n",
      "Batch 15, G1 Loss: 1.1105, G2 Loss: 3.8193, D1 Loss: 1.0646, D2 Loss: 0.0706\n",
      "Batch 16, G1 Loss: 1.7888, G2 Loss: 4.6140, D1 Loss: 0.9021, D2 Loss: 0.2081\n",
      "Batch 17, G1 Loss: 1.5918, G2 Loss: 4.2455, D1 Loss: 0.9466, D2 Loss: 0.1255\n",
      "Batch 18, G1 Loss: 1.1676, G2 Loss: 1.8679, D1 Loss: 0.9594, D2 Loss: 0.4301\n",
      "Batch 19, G1 Loss: 1.6869, G2 Loss: 6.5784, D1 Loss: 0.8568, D2 Loss: 0.2834\n",
      "Batch 20, G1 Loss: 1.6558, G2 Loss: 5.5999, D1 Loss: 1.0723, D2 Loss: 0.1003\n",
      "Batch 21, G1 Loss: 0.9187, G2 Loss: 3.6218, D1 Loss: 1.0199, D2 Loss: 0.0988\n",
      "Batch 22, G1 Loss: 2.2035, G2 Loss: 2.8456, D1 Loss: 0.8921, D2 Loss: 0.1626\n",
      "Batch 23, G1 Loss: 1.4970, G2 Loss: 3.5347, D1 Loss: 0.9222, D2 Loss: 0.1214\n",
      "Batch 24, G1 Loss: 1.0158, G2 Loss: 2.5743, D1 Loss: 1.1021, D2 Loss: 0.2364\n",
      "Batch 25, G1 Loss: 1.9759, G2 Loss: 3.4516, D1 Loss: 1.0423, D2 Loss: 0.2181\n",
      "Batch 26, G1 Loss: 1.1169, G2 Loss: 4.0969, D1 Loss: 1.2250, D2 Loss: 0.2572\n",
      "Batch 27, G1 Loss: 1.6941, G2 Loss: 3.0474, D1 Loss: 1.5178, D2 Loss: 0.1528\n",
      "Batch 28, G1 Loss: 0.6631, G2 Loss: 3.4014, D1 Loss: 1.6709, D2 Loss: 0.1286\n",
      "Batch 29, G1 Loss: 3.1732, G2 Loss: 6.3427, D1 Loss: 1.6511, D2 Loss: 0.2656\n",
      "Batch 30, G1 Loss: 0.7212, G2 Loss: 1.2716, D1 Loss: 1.5224, D2 Loss: 0.5453\n",
      "Batch 31, G1 Loss: 2.3586, G2 Loss: 10.7905, D1 Loss: 1.0990, D2 Loss: 0.7811\n",
      "Batch 32, G1 Loss: 1.3047, G2 Loss: 8.2218, D1 Loss: 0.8295, D2 Loss: 0.3402\n",
      "Epoch 54 Average -> G1 Loss: 1.5514322519302368, G2 Loss: 4.235229969024658, D1 Loss: 1.169975757598877, D2 Loss: 0.22280782461166382\n",
      "Epoch 55/100\n",
      "Batch 1, G1 Loss: 1.5785, G2 Loss: 3.6768, D1 Loss: 0.7544, D2 Loss: 0.0744\n",
      "Batch 2, G1 Loss: 1.4777, G2 Loss: 1.6710, D1 Loss: 0.9560, D2 Loss: 0.6585\n",
      "Batch 3, G1 Loss: 0.9711, G2 Loss: 9.1494, D1 Loss: 1.2085, D2 Loss: 0.3049\n",
      "Batch 4, G1 Loss: 1.7064, G2 Loss: 8.6499, D1 Loss: 0.9510, D2 Loss: 0.1632\n",
      "Batch 5, G1 Loss: 1.3069, G2 Loss: 5.7812, D1 Loss: 0.9074, D2 Loss: 0.0673\n",
      "Batch 6, G1 Loss: 1.5315, G2 Loss: 5.9133, D1 Loss: 0.7694, D2 Loss: 0.0474\n",
      "Batch 7, G1 Loss: 1.6698, G2 Loss: 3.5548, D1 Loss: 0.6669, D2 Loss: 0.0766\n",
      "Batch 8, G1 Loss: 1.8180, G2 Loss: 2.4676, D1 Loss: 0.7784, D2 Loss: 0.2324\n",
      "Batch 9, G1 Loss: 1.2045, G2 Loss: 5.5201, D1 Loss: 1.0470, D2 Loss: 0.0605\n",
      "Batch 10, G1 Loss: 2.2582, G2 Loss: 5.1507, D1 Loss: 0.6302, D2 Loss: 0.2143\n",
      "Batch 11, G1 Loss: 1.7450, G2 Loss: 2.3305, D1 Loss: 0.8157, D2 Loss: 0.2393\n",
      "Batch 12, G1 Loss: 1.2068, G2 Loss: 5.3909, D1 Loss: 0.8924, D2 Loss: 0.3405\n",
      "Batch 13, G1 Loss: 2.1948, G2 Loss: 3.2522, D1 Loss: 0.8142, D2 Loss: 0.1353\n",
      "Batch 14, G1 Loss: 1.1549, G2 Loss: 3.6094, D1 Loss: 1.3035, D2 Loss: 0.1582\n",
      "Batch 15, G1 Loss: 1.8414, G2 Loss: 6.0806, D1 Loss: 0.9724, D2 Loss: 0.0580\n",
      "Batch 16, G1 Loss: 1.0441, G2 Loss: 4.4357, D1 Loss: 1.2843, D2 Loss: 0.0662\n",
      "Batch 17, G1 Loss: 2.9241, G2 Loss: 2.1270, D1 Loss: 0.9252, D2 Loss: 0.2936\n",
      "Batch 18, G1 Loss: 0.5888, G2 Loss: 7.3194, D1 Loss: 1.8472, D2 Loss: 0.2113\n",
      "Batch 19, G1 Loss: 3.3247, G2 Loss: 5.5182, D1 Loss: 1.9764, D2 Loss: 0.1547\n",
      "Batch 20, G1 Loss: 0.4911, G2 Loss: 1.7632, D1 Loss: 2.0665, D2 Loss: 0.4043\n",
      "Batch 21, G1 Loss: 2.5391, G2 Loss: 9.9736, D1 Loss: 1.1711, D2 Loss: 0.3064\n",
      "Batch 22, G1 Loss: 1.2777, G2 Loss: 8.0017, D1 Loss: 0.9321, D2 Loss: 0.0245\n",
      "Batch 23, G1 Loss: 1.2035, G2 Loss: 5.1207, D1 Loss: 1.0766, D2 Loss: 0.0637\n",
      "Batch 24, G1 Loss: 1.8560, G2 Loss: 3.0501, D1 Loss: 0.9755, D2 Loss: 0.1076\n",
      "Batch 25, G1 Loss: 1.1078, G2 Loss: 3.4358, D1 Loss: 1.2746, D2 Loss: 0.1184\n",
      "Batch 26, G1 Loss: 1.6443, G2 Loss: 4.5936, D1 Loss: 0.9569, D2 Loss: 0.1366\n",
      "Batch 27, G1 Loss: 1.2616, G2 Loss: 8.3856, D1 Loss: 0.9667, D2 Loss: 0.0230\n",
      "Batch 28, G1 Loss: 1.5895, G2 Loss: 3.0605, D1 Loss: 0.9691, D2 Loss: 0.1339\n",
      "Batch 29, G1 Loss: 1.3689, G2 Loss: 4.2659, D1 Loss: 1.0122, D2 Loss: 0.0607\n",
      "Batch 30, G1 Loss: 1.5553, G2 Loss: 3.9903, D1 Loss: 0.9144, D2 Loss: 0.1066\n",
      "Batch 31, G1 Loss: 1.5010, G2 Loss: 4.6555, D1 Loss: 1.0806, D2 Loss: 0.0877\n",
      "Batch 32, G1 Loss: 1.2563, G2 Loss: 3.9155, D1 Loss: 0.9479, D2 Loss: 0.1286\n",
      "Epoch 55 Average -> G1 Loss: 1.5687239170074463, G2 Loss: 4.869087219238281, D1 Loss: 1.0576553344726562, D2 Loss: 0.16432911157608032\n",
      "Epoch 56/100\n",
      "Batch 1, G1 Loss: 2.1555, G2 Loss: 2.5005, D1 Loss: 0.9802, D2 Loss: 0.3806\n",
      "Batch 2, G1 Loss: 0.9120, G2 Loss: 5.7611, D1 Loss: 1.1218, D2 Loss: 0.3643\n",
      "Batch 3, G1 Loss: 1.9567, G2 Loss: 3.6529, D1 Loss: 1.2118, D2 Loss: 0.1399\n",
      "Batch 4, G1 Loss: 0.7892, G2 Loss: 5.5524, D1 Loss: 1.2362, D2 Loss: 0.0369\n",
      "Batch 5, G1 Loss: 2.2231, G2 Loss: 1.6942, D1 Loss: 1.1185, D2 Loss: 0.3919\n",
      "Batch 6, G1 Loss: 1.3437, G2 Loss: 11.3607, D1 Loss: 0.9971, D2 Loss: 0.4557\n",
      "Batch 7, G1 Loss: 1.2417, G2 Loss: 10.3200, D1 Loss: 1.0406, D2 Loss: 0.5005\n",
      "Batch 8, G1 Loss: 1.3483, G2 Loss: 4.9989, D1 Loss: 1.0094, D2 Loss: 0.0313\n",
      "Batch 9, G1 Loss: 1.5959, G2 Loss: 3.0320, D1 Loss: 1.4799, D2 Loss: 0.2368\n",
      "Batch 10, G1 Loss: 0.8427, G2 Loss: 4.0206, D1 Loss: 1.3708, D2 Loss: 0.0675\n",
      "Batch 11, G1 Loss: 1.9628, G2 Loss: 3.9188, D1 Loss: 1.1601, D2 Loss: 0.1099\n",
      "Batch 12, G1 Loss: 1.1510, G2 Loss: 4.5925, D1 Loss: 1.0771, D2 Loss: 0.1841\n",
      "Batch 13, G1 Loss: 1.6681, G2 Loss: 4.0961, D1 Loss: 0.7902, D2 Loss: 0.1036\n",
      "Batch 14, G1 Loss: 1.2339, G2 Loss: 1.5049, D1 Loss: 1.0534, D2 Loss: 0.4952\n",
      "Batch 15, G1 Loss: 1.3706, G2 Loss: 10.6595, D1 Loss: 1.2568, D2 Loss: 0.6251\n",
      "Batch 16, G1 Loss: 0.9408, G2 Loss: 9.2537, D1 Loss: 1.3869, D2 Loss: 0.2675\n",
      "Batch 17, G1 Loss: 1.9937, G2 Loss: 6.1138, D1 Loss: 1.2576, D2 Loss: 0.1526\n",
      "Batch 18, G1 Loss: 0.7866, G2 Loss: 2.7385, D1 Loss: 1.3703, D2 Loss: 0.1691\n",
      "Batch 19, G1 Loss: 1.8393, G2 Loss: 3.7970, D1 Loss: 0.9356, D2 Loss: 0.0714\n",
      "Batch 20, G1 Loss: 1.6910, G2 Loss: 4.7343, D1 Loss: 1.1281, D2 Loss: 0.0708\n",
      "Batch 21, G1 Loss: 0.8746, G2 Loss: 4.5920, D1 Loss: 1.6440, D2 Loss: 0.0580\n",
      "Batch 22, G1 Loss: 2.2607, G2 Loss: 2.9694, D1 Loss: 1.2326, D2 Loss: 0.1468\n",
      "Batch 23, G1 Loss: 0.9737, G2 Loss: 4.4771, D1 Loss: 1.2071, D2 Loss: 0.2875\n",
      "Batch 24, G1 Loss: 1.4665, G2 Loss: 2.8368, D1 Loss: 0.9499, D2 Loss: 0.1620\n",
      "Batch 25, G1 Loss: 1.6792, G2 Loss: 3.9780, D1 Loss: 0.8779, D2 Loss: 0.1157\n",
      "Batch 26, G1 Loss: 1.2336, G2 Loss: 3.0646, D1 Loss: 0.9241, D2 Loss: 0.1511\n",
      "Batch 27, G1 Loss: 1.8842, G2 Loss: 4.6776, D1 Loss: 0.9101, D2 Loss: 0.7217\n",
      "Batch 28, G1 Loss: 1.2017, G2 Loss: 2.5216, D1 Loss: 0.8135, D2 Loss: 0.2303\n",
      "Batch 29, G1 Loss: 1.9215, G2 Loss: 6.9579, D1 Loss: 0.7287, D2 Loss: 0.1930\n",
      "Batch 30, G1 Loss: 1.2458, G2 Loss: 11.1746, D1 Loss: 0.9309, D2 Loss: 0.0429\n",
      "Batch 31, G1 Loss: 1.4669, G2 Loss: 5.7802, D1 Loss: 0.9887, D2 Loss: 0.0545\n",
      "Batch 32, G1 Loss: 1.3549, G2 Loss: 1.4431, D1 Loss: 0.8632, D2 Loss: 0.6624\n",
      "Epoch 56 Average -> G1 Loss: 1.4565497636795044, G2 Loss: 4.961724758148193, D1 Loss: 1.0954065322875977, D2 Loss: 0.24001577496528625\n",
      "Epoch 57/100\n",
      "Batch 1, G1 Loss: 1.7285, G2 Loss: 10.4398, D1 Loss: 0.8505, D2 Loss: 0.1898\n",
      "Batch 2, G1 Loss: 1.7400, G2 Loss: 10.1252, D1 Loss: 1.2188, D2 Loss: 0.5677\n",
      "Batch 3, G1 Loss: 1.1077, G2 Loss: 6.0380, D1 Loss: 1.1594, D2 Loss: 0.0409\n",
      "Batch 4, G1 Loss: 2.3415, G2 Loss: 2.4284, D1 Loss: 1.1085, D2 Loss: 0.2898\n",
      "Batch 5, G1 Loss: 1.4048, G2 Loss: 5.0412, D1 Loss: 0.6834, D2 Loss: 0.0976\n",
      "Batch 6, G1 Loss: 1.3329, G2 Loss: 6.1177, D1 Loss: 1.1456, D2 Loss: 0.1394\n",
      "Batch 7, G1 Loss: 1.1841, G2 Loss: 3.8148, D1 Loss: 1.1515, D2 Loss: 0.1313\n",
      "Batch 8, G1 Loss: 2.4567, G2 Loss: 3.8739, D1 Loss: 1.1404, D2 Loss: 0.2003\n",
      "Batch 9, G1 Loss: 1.0680, G2 Loss: 3.7057, D1 Loss: 1.1703, D2 Loss: 0.1242\n",
      "Batch 10, G1 Loss: 1.8506, G2 Loss: 4.4423, D1 Loss: 0.8024, D2 Loss: 0.4787\n",
      "Batch 11, G1 Loss: 1.6142, G2 Loss: 1.9568, D1 Loss: 0.8911, D2 Loss: 0.5636\n",
      "Batch 12, G1 Loss: 1.2523, G2 Loss: 7.7032, D1 Loss: 0.8707, D2 Loss: 0.3047\n",
      "Batch 13, G1 Loss: 1.9736, G2 Loss: 7.1422, D1 Loss: 1.0413, D2 Loss: 0.2440\n",
      "Batch 14, G1 Loss: 0.7785, G2 Loss: 3.8211, D1 Loss: 1.4643, D2 Loss: 0.2129\n",
      "Batch 15, G1 Loss: 1.9293, G2 Loss: 2.3047, D1 Loss: 1.2791, D2 Loss: 0.2619\n",
      "Batch 16, G1 Loss: 0.9792, G2 Loss: 4.8586, D1 Loss: 1.4517, D2 Loss: 0.1490\n",
      "Batch 17, G1 Loss: 1.0752, G2 Loss: 3.6760, D1 Loss: 1.2117, D2 Loss: 0.2564\n",
      "Batch 18, G1 Loss: 2.1919, G2 Loss: 2.6084, D1 Loss: 1.1472, D2 Loss: 0.2758\n",
      "Batch 19, G1 Loss: 0.7896, G2 Loss: 8.0115, D1 Loss: 1.3041, D2 Loss: 0.1046\n",
      "Batch 20, G1 Loss: 1.8469, G2 Loss: 9.1970, D1 Loss: 0.8957, D2 Loss: 0.1159\n",
      "Batch 21, G1 Loss: 1.3081, G2 Loss: 2.5374, D1 Loss: 0.7884, D2 Loss: 0.3290\n",
      "Batch 22, G1 Loss: 1.2303, G2 Loss: 4.3665, D1 Loss: 1.0613, D2 Loss: 0.0686\n",
      "Batch 23, G1 Loss: 1.6107, G2 Loss: 4.4039, D1 Loss: 0.9978, D2 Loss: 0.3912\n",
      "Batch 24, G1 Loss: 1.1270, G2 Loss: 1.1556, D1 Loss: 1.1373, D2 Loss: 0.9074\n",
      "Batch 25, G1 Loss: 1.7139, G2 Loss: 11.0661, D1 Loss: 1.1220, D2 Loss: 0.8073\n",
      "Batch 26, G1 Loss: 1.0005, G2 Loss: 9.9214, D1 Loss: 1.2660, D2 Loss: 0.3161\n",
      "Batch 27, G1 Loss: 2.1868, G2 Loss: 5.9794, D1 Loss: 1.1302, D2 Loss: 0.0832\n",
      "Batch 28, G1 Loss: 0.9668, G2 Loss: 3.1929, D1 Loss: 1.0478, D2 Loss: 0.2086\n",
      "Batch 29, G1 Loss: 2.1026, G2 Loss: 2.0815, D1 Loss: 0.8107, D2 Loss: 0.3375\n",
      "Batch 30, G1 Loss: 1.4677, G2 Loss: 5.4969, D1 Loss: 0.8348, D2 Loss: 0.1901\n",
      "Batch 31, G1 Loss: 1.1878, G2 Loss: 8.6252, D1 Loss: 0.8101, D2 Loss: 0.0487\n",
      "Batch 32, G1 Loss: 2.1633, G2 Loss: 4.8420, D1 Loss: 0.8889, D2 Loss: 0.0806\n",
      "Epoch 57 Average -> G1 Loss: 1.5222200155258179, G2 Loss: 5.342968940734863, D1 Loss: 1.0588387250900269, D2 Loss: 0.2661464214324951\n",
      "Epoch 58/100\n",
      "Batch 1, G1 Loss: 1.1327, G2 Loss: 3.0345, D1 Loss: 1.0621, D2 Loss: 0.1199\n",
      "Batch 2, G1 Loss: 1.3676, G2 Loss: 3.3753, D1 Loss: 0.9978, D2 Loss: 0.1687\n",
      "Batch 3, G1 Loss: 1.6122, G2 Loss: 4.4794, D1 Loss: 1.0854, D2 Loss: 0.1183\n",
      "Batch 4, G1 Loss: 1.3300, G2 Loss: 3.3205, D1 Loss: 0.8657, D2 Loss: 0.2139\n",
      "Batch 5, G1 Loss: 1.7757, G2 Loss: 2.4317, D1 Loss: 0.7419, D2 Loss: 0.4159\n",
      "Batch 6, G1 Loss: 1.4800, G2 Loss: 4.9189, D1 Loss: 0.7876, D2 Loss: 0.2629\n",
      "Batch 7, G1 Loss: 1.7990, G2 Loss: 3.4204, D1 Loss: 0.7734, D2 Loss: 0.2423\n",
      "Batch 8, G1 Loss: 1.2419, G2 Loss: 5.1167, D1 Loss: 1.0088, D2 Loss: 0.0671\n",
      "Batch 9, G1 Loss: 2.0951, G2 Loss: 10.3264, D1 Loss: 0.9775, D2 Loss: 0.0211\n",
      "Batch 10, G1 Loss: 0.6730, G2 Loss: 6.2671, D1 Loss: 1.4412, D2 Loss: 0.1467\n",
      "Batch 11, G1 Loss: 3.0086, G2 Loss: 2.0779, D1 Loss: 1.7749, D2 Loss: 0.5208\n",
      "Batch 12, G1 Loss: 0.7634, G2 Loss: 8.8356, D1 Loss: 1.2558, D2 Loss: 0.3547\n",
      "Batch 13, G1 Loss: 2.4598, G2 Loss: 7.5294, D1 Loss: 1.3731, D2 Loss: 0.2509\n",
      "Batch 14, G1 Loss: 0.5879, G2 Loss: 4.9207, D1 Loss: 1.5818, D2 Loss: 0.1090\n",
      "Batch 15, G1 Loss: 2.1293, G2 Loss: 1.6719, D1 Loss: 1.0899, D2 Loss: 0.4922\n",
      "Batch 16, G1 Loss: 1.7456, G2 Loss: 7.7966, D1 Loss: 0.9975, D2 Loss: 0.2726\n",
      "Batch 17, G1 Loss: 0.9842, G2 Loss: 7.2510, D1 Loss: 1.1620, D2 Loss: 0.1806\n",
      "Batch 18, G1 Loss: 1.6098, G2 Loss: 4.6995, D1 Loss: 1.0650, D2 Loss: 0.2024\n",
      "Batch 19, G1 Loss: 1.4014, G2 Loss: 1.9271, D1 Loss: 1.1822, D2 Loss: 0.3596\n",
      "Batch 20, G1 Loss: 1.1023, G2 Loss: 5.8062, D1 Loss: 1.1405, D2 Loss: 0.0966\n",
      "Batch 21, G1 Loss: 1.5796, G2 Loss: 5.4628, D1 Loss: 1.1386, D2 Loss: 0.1353\n",
      "Batch 22, G1 Loss: 0.8962, G2 Loss: 3.3402, D1 Loss: 1.3382, D2 Loss: 0.1350\n",
      "Batch 23, G1 Loss: 2.0215, G2 Loss: 2.1582, D1 Loss: 1.4187, D2 Loss: 0.3070\n",
      "Batch 24, G1 Loss: 0.8811, G2 Loss: 6.4251, D1 Loss: 1.2281, D2 Loss: 0.3122\n",
      "Batch 25, G1 Loss: 1.4593, G2 Loss: 6.3912, D1 Loss: 0.9175, D2 Loss: 0.1944\n",
      "Batch 26, G1 Loss: 1.7997, G2 Loss: 4.9397, D1 Loss: 0.8707, D2 Loss: 0.0926\n",
      "Batch 27, G1 Loss: 1.4964, G2 Loss: 1.2993, D1 Loss: 1.0526, D2 Loss: 0.6108\n",
      "Batch 28, G1 Loss: 0.9455, G2 Loss: 9.4279, D1 Loss: 0.9526, D2 Loss: 0.6420\n",
      "Batch 29, G1 Loss: 2.3832, G2 Loss: 7.4212, D1 Loss: 0.9611, D2 Loss: 0.2932\n",
      "Batch 30, G1 Loss: 1.4011, G2 Loss: 3.4933, D1 Loss: 0.7473, D2 Loss: 0.0898\n",
      "Batch 31, G1 Loss: 1.4857, G2 Loss: 2.7638, D1 Loss: 0.8479, D2 Loss: 0.2765\n",
      "Batch 32, G1 Loss: 1.4568, G2 Loss: 4.3949, D1 Loss: 0.8431, D2 Loss: 0.1804\n",
      "Epoch 58 Average -> G1 Loss: 1.503294825553894, G2 Loss: 4.89764404296875, D1 Loss: 1.0837736129760742, D2 Loss: 0.24642111361026764\n",
      "Epoch 59/100\n",
      "Batch 1, G1 Loss: 1.7435, G2 Loss: 5.1081, D1 Loss: 0.7896, D2 Loss: 0.0898\n",
      "Batch 2, G1 Loss: 1.3385, G2 Loss: 5.4794, D1 Loss: 0.7606, D2 Loss: 0.0358\n",
      "Batch 3, G1 Loss: 1.6899, G2 Loss: 4.5645, D1 Loss: 0.8005, D2 Loss: 0.0502\n",
      "Batch 4, G1 Loss: 1.5911, G2 Loss: 1.8651, D1 Loss: 0.7457, D2 Loss: 0.5226\n",
      "Batch 5, G1 Loss: 1.3883, G2 Loss: 9.2127, D1 Loss: 1.1710, D2 Loss: 0.7598\n",
      "Batch 6, G1 Loss: 1.3873, G2 Loss: 7.6405, D1 Loss: 1.0038, D2 Loss: 0.2695\n",
      "Batch 7, G1 Loss: 1.4374, G2 Loss: 4.9085, D1 Loss: 1.2170, D2 Loss: 0.0386\n",
      "Batch 8, G1 Loss: 1.1708, G2 Loss: 3.9110, D1 Loss: 1.0575, D2 Loss: 0.0552\n",
      "Batch 9, G1 Loss: 1.9469, G2 Loss: 1.4941, D1 Loss: 1.1365, D2 Loss: 0.5149\n",
      "Batch 10, G1 Loss: 1.0760, G2 Loss: 8.1939, D1 Loss: 1.2536, D2 Loss: 0.2319\n",
      "Batch 11, G1 Loss: 2.1635, G2 Loss: 8.3692, D1 Loss: 1.1090, D2 Loss: 0.2127\n",
      "Batch 12, G1 Loss: 0.9121, G2 Loss: 5.4792, D1 Loss: 1.1492, D2 Loss: 0.0407\n",
      "Batch 13, G1 Loss: 2.1206, G2 Loss: 4.6980, D1 Loss: 0.8571, D2 Loss: 0.1128\n",
      "Batch 14, G1 Loss: 1.1367, G2 Loss: 2.4969, D1 Loss: 1.1029, D2 Loss: 0.3524\n",
      "Batch 15, G1 Loss: 1.4279, G2 Loss: 4.6836, D1 Loss: 0.9946, D2 Loss: 0.1734\n",
      "Batch 16, G1 Loss: 1.4356, G2 Loss: 3.7637, D1 Loss: 1.0424, D2 Loss: 0.1829\n",
      "Batch 17, G1 Loss: 1.0928, G2 Loss: 3.6474, D1 Loss: 1.0747, D2 Loss: 0.1532\n",
      "Batch 18, G1 Loss: 1.8050, G2 Loss: 4.3747, D1 Loss: 1.3523, D2 Loss: 0.0790\n",
      "Batch 19, G1 Loss: 0.5863, G2 Loss: 3.6507, D1 Loss: 1.6750, D2 Loss: 0.2594\n",
      "Batch 20, G1 Loss: 3.0893, G2 Loss: 2.6137, D1 Loss: 1.2670, D2 Loss: 0.1808\n",
      "Batch 21, G1 Loss: 1.2602, G2 Loss: 6.0130, D1 Loss: 0.9503, D2 Loss: 0.1310\n",
      "Batch 22, G1 Loss: 0.9844, G2 Loss: 4.3059, D1 Loss: 1.2446, D2 Loss: 0.1049\n",
      "Batch 23, G1 Loss: 2.6248, G2 Loss: 2.9995, D1 Loss: 1.2327, D2 Loss: 0.1259\n",
      "Batch 24, G1 Loss: 0.7143, G2 Loss: 3.9667, D1 Loss: 1.4481, D2 Loss: 0.1471\n",
      "Batch 25, G1 Loss: 2.4767, G2 Loss: 3.6117, D1 Loss: 1.1926, D2 Loss: 0.3463\n",
      "Batch 26, G1 Loss: 1.4218, G2 Loss: 2.3161, D1 Loss: 0.8342, D2 Loss: 0.2485\n",
      "Batch 27, G1 Loss: 0.9963, G2 Loss: 5.7704, D1 Loss: 1.1104, D2 Loss: 0.3514\n",
      "Batch 28, G1 Loss: 2.3792, G2 Loss: 3.5212, D1 Loss: 1.2207, D2 Loss: 0.1005\n",
      "Batch 29, G1 Loss: 0.9011, G2 Loss: 3.8420, D1 Loss: 1.1536, D2 Loss: 0.1705\n",
      "Batch 30, G1 Loss: 2.0656, G2 Loss: 3.1286, D1 Loss: 0.7872, D2 Loss: 0.2161\n",
      "Batch 31, G1 Loss: 1.2862, G2 Loss: 3.1745, D1 Loss: 0.7530, D2 Loss: 0.1722\n",
      "Batch 32, G1 Loss: 2.5497, G2 Loss: 4.4064, D1 Loss: 0.9540, D2 Loss: 0.2566\n",
      "Epoch 59 Average -> G1 Loss: 1.568738341331482, G2 Loss: 4.475340843200684, D1 Loss: 1.0762900114059448, D2 Loss: 0.20895294845104218\n",
      "Epoch 60/100\n",
      "Batch 1, G1 Loss: 0.7689, G2 Loss: 2.7225, D1 Loss: 1.1300, D2 Loss: 0.1881\n",
      "Batch 2, G1 Loss: 2.2143, G2 Loss: 5.3378, D1 Loss: 0.9060, D2 Loss: 0.0624\n",
      "Batch 3, G1 Loss: 1.8069, G2 Loss: 6.3572, D1 Loss: 0.5656, D2 Loss: 0.0579\n",
      "Batch 4, G1 Loss: 2.1607, G2 Loss: 4.7993, D1 Loss: 0.3837, D2 Loss: 0.0631\n",
      "Batch 5, G1 Loss: 2.0445, G2 Loss: 4.2859, D1 Loss: 0.4533, D2 Loss: 0.0901\n",
      "Batch 6, G1 Loss: 1.6647, G2 Loss: 2.2582, D1 Loss: 0.5155, D2 Loss: 0.3166\n",
      "Batch 7, G1 Loss: 1.8380, G2 Loss: 5.6126, D1 Loss: 0.7208, D2 Loss: 0.1634\n",
      "Batch 8, G1 Loss: 1.6887, G2 Loss: 4.9937, D1 Loss: 0.6623, D2 Loss: 0.2246\n",
      "Batch 9, G1 Loss: 1.4714, G2 Loss: 2.9196, D1 Loss: 0.8608, D2 Loss: 0.1703\n",
      "Batch 10, G1 Loss: 1.3087, G2 Loss: 3.3283, D1 Loss: 0.8482, D2 Loss: 0.0984\n",
      "Batch 11, G1 Loss: 1.8582, G2 Loss: 3.9416, D1 Loss: 1.0933, D2 Loss: 0.1077\n",
      "Batch 12, G1 Loss: 1.3047, G2 Loss: 3.7722, D1 Loss: 1.0558, D2 Loss: 0.3064\n",
      "Batch 13, G1 Loss: 1.3723, G2 Loss: 2.3480, D1 Loss: 1.0210, D2 Loss: 0.1871\n",
      "Batch 14, G1 Loss: 1.8449, G2 Loss: 8.0336, D1 Loss: 0.8287, D2 Loss: 0.2231\n",
      "Batch 15, G1 Loss: 1.7476, G2 Loss: 5.8068, D1 Loss: 0.8850, D2 Loss: 0.0408\n",
      "Batch 16, G1 Loss: 1.2623, G2 Loss: 1.9644, D1 Loss: 0.9782, D2 Loss: 0.3074\n",
      "Batch 17, G1 Loss: 2.4626, G2 Loss: 7.8955, D1 Loss: 1.1206, D2 Loss: 0.4859\n",
      "Batch 18, G1 Loss: 1.2246, G2 Loss: 4.6509, D1 Loss: 1.1197, D2 Loss: 0.0569\n",
      "Batch 19, G1 Loss: 1.3345, G2 Loss: 2.9142, D1 Loss: 0.9975, D2 Loss: 0.1162\n",
      "Batch 20, G1 Loss: 2.0015, G2 Loss: 4.4524, D1 Loss: 0.8453, D2 Loss: 0.4665\n",
      "Batch 21, G1 Loss: 1.3469, G2 Loss: 0.5043, D1 Loss: 0.8872, D2 Loss: 1.8770\n",
      "Batch 22, G1 Loss: 1.5401, G2 Loss: 16.6377, D1 Loss: 0.8488, D2 Loss: 1.9009\n",
      "Batch 23, G1 Loss: 1.3086, G2 Loss: 14.2639, D1 Loss: 0.7845, D2 Loss: 0.3825\n",
      "Batch 24, G1 Loss: 2.4432, G2 Loss: 7.4126, D1 Loss: 0.7577, D2 Loss: 0.4598\n",
      "Batch 25, G1 Loss: 1.3707, G2 Loss: 1.6411, D1 Loss: 0.6601, D2 Loss: 0.5633\n",
      "Batch 26, G1 Loss: 2.2306, G2 Loss: 3.8361, D1 Loss: 0.9303, D2 Loss: 0.1371\n",
      "Batch 27, G1 Loss: 0.8286, G2 Loss: 6.3409, D1 Loss: 1.1084, D2 Loss: 0.1491\n",
      "Batch 28, G1 Loss: 2.8668, G2 Loss: 5.6861, D1 Loss: 0.9683, D2 Loss: 0.1652\n",
      "Batch 29, G1 Loss: 1.3519, G2 Loss: 4.1190, D1 Loss: 1.0228, D2 Loss: 0.1009\n",
      "Batch 30, G1 Loss: 1.1761, G2 Loss: 1.9580, D1 Loss: 1.0888, D2 Loss: 0.4618\n",
      "Batch 31, G1 Loss: 1.5608, G2 Loss: 6.6682, D1 Loss: 0.9452, D2 Loss: 0.4500\n",
      "Batch 32, G1 Loss: 2.2072, G2 Loss: 5.0424, D1 Loss: 1.0716, D2 Loss: 0.1992\n",
      "Epoch 60 Average -> G1 Loss: 1.675360918045044, G2 Loss: 5.078280448913574, D1 Loss: 0.8770352602005005, D2 Loss: 0.33061864972114563\n",
      "Epoch 61/100\n",
      "Batch 1, G1 Loss: 0.9062, G2 Loss: 1.8819, D1 Loss: 1.2008, D2 Loss: 0.4608\n",
      "Batch 2, G1 Loss: 3.0996, G2 Loss: 6.0112, D1 Loss: 1.3043, D2 Loss: 0.2921\n",
      "Batch 3, G1 Loss: 0.3484, G2 Loss: 5.0146, D1 Loss: 2.4531, D2 Loss: 0.2554\n",
      "Batch 4, G1 Loss: 4.2001, G2 Loss: 1.8900, D1 Loss: 2.0971, D2 Loss: 0.3738\n",
      "Batch 5, G1 Loss: 1.8071, G2 Loss: 6.6805, D1 Loss: 0.6606, D2 Loss: 0.3833\n",
      "Batch 6, G1 Loss: 0.4572, G2 Loss: 14.6283, D1 Loss: 1.9223, D2 Loss: 0.1341\n",
      "Batch 7, G1 Loss: 4.8513, G2 Loss: 13.2480, D1 Loss: 2.5798, D2 Loss: 0.0128\n",
      "Batch 8, G1 Loss: 1.3723, G2 Loss: 12.1794, D1 Loss: 0.6955, D2 Loss: 0.0057\n",
      "Batch 9, G1 Loss: 0.9157, G2 Loss: 11.1138, D1 Loss: 0.9292, D2 Loss: 0.0043\n",
      "Batch 10, G1 Loss: 2.4873, G2 Loss: 9.7672, D1 Loss: 0.5740, D2 Loss: 0.0021\n",
      "Batch 11, G1 Loss: 2.3005, G2 Loss: 7.7960, D1 Loss: 0.6505, D2 Loss: 0.0066\n",
      "Batch 12, G1 Loss: 1.3014, G2 Loss: 6.3229, D1 Loss: 0.8322, D2 Loss: 0.0114\n",
      "Batch 13, G1 Loss: 1.7067, G2 Loss: 3.8983, D1 Loss: 0.6437, D2 Loss: 0.0391\n",
      "Batch 14, G1 Loss: 2.3929, G2 Loss: 3.0060, D1 Loss: 0.6284, D2 Loss: 0.1287\n",
      "Batch 15, G1 Loss: 1.8735, G2 Loss: 5.0312, D1 Loss: 0.6970, D2 Loss: 0.0246\n",
      "Batch 16, G1 Loss: 1.1085, G2 Loss: 4.7746, D1 Loss: 1.0179, D2 Loss: 0.0333\n",
      "Batch 17, G1 Loss: 2.6843, G2 Loss: 4.7492, D1 Loss: 0.9267, D2 Loss: 0.0276\n",
      "Batch 18, G1 Loss: 0.7940, G2 Loss: 2.6674, D1 Loss: 1.4181, D2 Loss: 0.2487\n",
      "Batch 19, G1 Loss: 1.9992, G2 Loss: 4.9730, D1 Loss: 0.9369, D2 Loss: 0.1223\n",
      "Batch 20, G1 Loss: 1.1689, G2 Loss: 5.4958, D1 Loss: 1.4632, D2 Loss: 0.0652\n",
      "Batch 21, G1 Loss: 1.8531, G2 Loss: 2.0686, D1 Loss: 1.0130, D2 Loss: 0.3638\n",
      "Batch 22, G1 Loss: 1.7603, G2 Loss: 7.4962, D1 Loss: 1.1516, D2 Loss: 0.0535\n",
      "Batch 23, G1 Loss: 1.3572, G2 Loss: 6.9982, D1 Loss: 0.8814, D2 Loss: 0.1134\n",
      "Batch 24, G1 Loss: 2.4731, G2 Loss: 3.2596, D1 Loss: 1.0208, D2 Loss: 0.1925\n",
      "Batch 25, G1 Loss: 0.7015, G2 Loss: 3.2721, D1 Loss: 1.5782, D2 Loss: 0.1807\n",
      "Batch 26, G1 Loss: 2.2815, G2 Loss: 5.9085, D1 Loss: 1.4493, D2 Loss: 0.0976\n",
      "Batch 27, G1 Loss: 0.9279, G2 Loss: 4.6855, D1 Loss: 1.5474, D2 Loss: 0.0603\n",
      "Batch 28, G1 Loss: 1.4928, G2 Loss: 2.9531, D1 Loss: 1.0113, D2 Loss: 0.1804\n",
      "Batch 29, G1 Loss: 1.4736, G2 Loss: 5.5595, D1 Loss: 1.0869, D2 Loss: 0.2025\n",
      "Batch 30, G1 Loss: 1.2477, G2 Loss: 3.5576, D1 Loss: 1.3116, D2 Loss: 0.0799\n",
      "Batch 31, G1 Loss: 1.6107, G2 Loss: 3.4359, D1 Loss: 0.7504, D2 Loss: 0.1495\n",
      "Batch 32, G1 Loss: 1.9553, G2 Loss: 5.1598, D1 Loss: 0.6956, D2 Loss: 0.0308\n",
      "Epoch 61 Average -> G1 Loss: 1.7784385681152344, G2 Loss: 5.796367168426514, D1 Loss: 1.1602674722671509, D2 Loss: 0.13552996516227722\n",
      "Epoch 62/100\n",
      "Batch 1, G1 Loss: 1.9073, G2 Loss: 5.0848, D1 Loss: 0.5409, D2 Loss: 0.0799\n",
      "Batch 2, G1 Loss: 1.6563, G2 Loss: 4.3933, D1 Loss: 0.6634, D2 Loss: 0.0574\n",
      "Batch 3, G1 Loss: 1.5484, G2 Loss: 3.3299, D1 Loss: 0.8861, D2 Loss: 0.1181\n",
      "Batch 4, G1 Loss: 1.4573, G2 Loss: 4.5287, D1 Loss: 1.0623, D2 Loss: 0.2694\n",
      "Batch 5, G1 Loss: 1.3311, G2 Loss: 4.8625, D1 Loss: 1.1696, D2 Loss: 0.1206\n",
      "Batch 6, G1 Loss: 1.2735, G2 Loss: 3.5435, D1 Loss: 1.1942, D2 Loss: 0.1132\n",
      "Batch 7, G1 Loss: 1.3232, G2 Loss: 4.0275, D1 Loss: 1.1787, D2 Loss: 0.1483\n",
      "Batch 8, G1 Loss: 1.1913, G2 Loss: 6.1308, D1 Loss: 1.4775, D2 Loss: 0.0189\n",
      "Batch 9, G1 Loss: 0.9589, G2 Loss: 3.1393, D1 Loss: 1.3191, D2 Loss: 0.1582\n",
      "Batch 10, G1 Loss: 2.1460, G2 Loss: 4.9968, D1 Loss: 1.4121, D2 Loss: 0.2240\n",
      "Batch 11, G1 Loss: 0.7747, G2 Loss: 3.0609, D1 Loss: 1.3245, D2 Loss: 0.2252\n",
      "Batch 12, G1 Loss: 2.1845, G2 Loss: 3.9021, D1 Loss: 0.9969, D2 Loss: 0.0926\n",
      "Batch 13, G1 Loss: 1.9814, G2 Loss: 3.8248, D1 Loss: 0.6682, D2 Loss: 0.1191\n",
      "Batch 14, G1 Loss: 1.0289, G2 Loss: 3.5901, D1 Loss: 0.9590, D2 Loss: 0.1173\n",
      "Batch 15, G1 Loss: 1.6213, G2 Loss: 4.2194, D1 Loss: 0.9034, D2 Loss: 0.0522\n",
      "Batch 16, G1 Loss: 1.5052, G2 Loss: 7.3404, D1 Loss: 0.8766, D2 Loss: 0.0995\n",
      "Batch 17, G1 Loss: 1.8489, G2 Loss: 2.0876, D1 Loss: 0.9030, D2 Loss: 0.2628\n",
      "Batch 18, G1 Loss: 1.3579, G2 Loss: 5.0517, D1 Loss: 0.8477, D2 Loss: 0.1396\n",
      "Batch 19, G1 Loss: 1.3859, G2 Loss: 4.6615, D1 Loss: 0.8017, D2 Loss: 0.0469\n",
      "Batch 20, G1 Loss: 2.3382, G2 Loss: 3.5387, D1 Loss: 0.8670, D2 Loss: 0.2058\n",
      "Batch 21, G1 Loss: 0.7975, G2 Loss: 2.7526, D1 Loss: 1.3568, D2 Loss: 0.2135\n",
      "Batch 22, G1 Loss: 1.7194, G2 Loss: 4.7935, D1 Loss: 1.3129, D2 Loss: 0.0964\n",
      "Batch 23, G1 Loss: 1.0091, G2 Loss: 4.2416, D1 Loss: 1.1727, D2 Loss: 0.0896\n",
      "Batch 24, G1 Loss: 1.9882, G2 Loss: 2.6482, D1 Loss: 1.2865, D2 Loss: 0.1520\n",
      "Batch 25, G1 Loss: 0.7800, G2 Loss: 4.4686, D1 Loss: 1.5787, D2 Loss: 0.0970\n",
      "Batch 26, G1 Loss: 1.8573, G2 Loss: 4.3024, D1 Loss: 1.3850, D2 Loss: 0.1106\n",
      "Batch 27, G1 Loss: 1.1342, G2 Loss: 3.1523, D1 Loss: 1.3888, D2 Loss: 0.1810\n",
      "Batch 28, G1 Loss: 1.1756, G2 Loss: 7.8136, D1 Loss: 1.0218, D2 Loss: 0.0944\n",
      "Batch 29, G1 Loss: 1.9067, G2 Loss: 4.5729, D1 Loss: 1.0811, D2 Loss: 0.1682\n",
      "Batch 30, G1 Loss: 1.3327, G2 Loss: 4.1519, D1 Loss: 1.0686, D2 Loss: 0.1424\n",
      "Batch 31, G1 Loss: 1.1632, G2 Loss: 0.4163, D1 Loss: 1.0997, D2 Loss: 1.6134\n",
      "Batch 32, G1 Loss: 1.4944, G2 Loss: 30.9497, D1 Loss: 1.0668, D2 Loss: 2.9846\n",
      "Epoch 62 Average -> G1 Loss: 1.4743293523788452, G2 Loss: 4.986802101135254, D1 Loss: 1.0897363424301147, D2 Loss: 0.269126832485199\n",
      "Epoch 63/100\n",
      "Batch 1, G1 Loss: 1.3203, G2 Loss: 28.7321, D1 Loss: 1.1784, D2 Loss: 0.1290\n",
      "Batch 2, G1 Loss: 1.5329, G2 Loss: 19.2085, D1 Loss: 1.2747, D2 Loss: 0.1249\n",
      "Batch 3, G1 Loss: 1.5697, G2 Loss: 8.2378, D1 Loss: 0.7767, D2 Loss: 0.0342\n",
      "Batch 4, G1 Loss: 1.6420, G2 Loss: 4.6242, D1 Loss: 0.6361, D2 Loss: 0.1093\n",
      "Batch 5, G1 Loss: 2.3516, G2 Loss: 3.3910, D1 Loss: 0.7420, D2 Loss: 0.0835\n",
      "Batch 6, G1 Loss: 1.0375, G2 Loss: 2.3421, D1 Loss: 1.0781, D2 Loss: 0.1756\n",
      "Batch 7, G1 Loss: 2.0913, G2 Loss: 9.0731, D1 Loss: 0.8382, D2 Loss: 0.0893\n",
      "Batch 8, G1 Loss: 1.6485, G2 Loss: 8.6428, D1 Loss: 0.9439, D2 Loss: 0.0529\n",
      "Batch 9, G1 Loss: 0.9273, G2 Loss: 6.3866, D1 Loss: 1.0641, D2 Loss: 0.0566\n",
      "Batch 10, G1 Loss: 2.6764, G2 Loss: 3.2792, D1 Loss: 1.1146, D2 Loss: 0.1096\n",
      "Batch 11, G1 Loss: 1.0717, G2 Loss: 4.0974, D1 Loss: 1.0270, D2 Loss: 0.1547\n",
      "Batch 12, G1 Loss: 1.7076, G2 Loss: 2.3501, D1 Loss: 0.7517, D2 Loss: 0.1875\n",
      "Batch 13, G1 Loss: 1.9704, G2 Loss: 5.8972, D1 Loss: 0.8370, D2 Loss: 0.0537\n",
      "Batch 14, G1 Loss: 1.8364, G2 Loss: 5.1866, D1 Loss: 1.0605, D2 Loss: 0.0511\n",
      "Batch 15, G1 Loss: 0.9020, G2 Loss: 5.0562, D1 Loss: 1.1091, D2 Loss: 0.0609\n",
      "Batch 16, G1 Loss: 2.6106, G2 Loss: 5.6017, D1 Loss: 0.8606, D2 Loss: 0.0581\n",
      "Batch 17, G1 Loss: 1.9549, G2 Loss: 1.2135, D1 Loss: 0.5928, D2 Loss: 0.7272\n",
      "Batch 18, G1 Loss: 0.9919, G2 Loss: 14.7363, D1 Loss: 1.0583, D2 Loss: 0.8870\n",
      "Batch 19, G1 Loss: 2.5879, G2 Loss: 12.7392, D1 Loss: 1.1256, D2 Loss: 0.0775\n",
      "Batch 20, G1 Loss: 1.3933, G2 Loss: 9.6713, D1 Loss: 1.0650, D2 Loss: 0.3595\n",
      "Batch 21, G1 Loss: 1.0301, G2 Loss: 4.6408, D1 Loss: 1.0769, D2 Loss: 0.0260\n",
      "Batch 22, G1 Loss: 2.8853, G2 Loss: 2.0176, D1 Loss: 0.9940, D2 Loss: 0.5342\n",
      "Batch 23, G1 Loss: 1.3142, G2 Loss: 5.9937, D1 Loss: 0.7400, D2 Loss: 0.1356\n",
      "Batch 24, G1 Loss: 1.2077, G2 Loss: 5.3657, D1 Loss: 0.7189, D2 Loss: 0.1618\n",
      "Batch 25, G1 Loss: 2.9928, G2 Loss: 2.5893, D1 Loss: 0.9955, D2 Loss: 0.3638\n",
      "Batch 26, G1 Loss: 1.1549, G2 Loss: 2.7728, D1 Loss: 1.0196, D2 Loss: 0.3429\n",
      "Batch 27, G1 Loss: 1.0921, G2 Loss: 4.4157, D1 Loss: 1.2110, D2 Loss: 0.2971\n",
      "Batch 28, G1 Loss: 1.9069, G2 Loss: 3.4548, D1 Loss: 1.2712, D2 Loss: 0.1349\n",
      "Batch 29, G1 Loss: 0.9888, G2 Loss: 3.1777, D1 Loss: 1.4334, D2 Loss: 0.2301\n",
      "Batch 30, G1 Loss: 1.3583, G2 Loss: 2.9426, D1 Loss: 1.3655, D2 Loss: 0.2375\n",
      "Batch 31, G1 Loss: 1.3283, G2 Loss: 4.4135, D1 Loss: 1.4874, D2 Loss: 0.2293\n",
      "Batch 32, G1 Loss: 0.9791, G2 Loss: 2.1039, D1 Loss: 1.2646, D2 Loss: 0.3258\n",
      "Epoch 63 Average -> G1 Loss: 1.626961588859558, G2 Loss: 6.386081695556641, D1 Loss: 1.0222625732421875, D2 Loss: 0.20627707242965698\n",
      "Epoch 64/100\n",
      "Batch 1, G1 Loss: 1.6770, G2 Loss: 4.9374, D1 Loss: 0.9731, D2 Loss: 0.2028\n",
      "Batch 2, G1 Loss: 1.1944, G2 Loss: 4.2587, D1 Loss: 1.1347, D2 Loss: 0.1101\n",
      "Batch 3, G1 Loss: 1.4499, G2 Loss: 3.3797, D1 Loss: 1.1618, D2 Loss: 0.2147\n",
      "Batch 4, G1 Loss: 1.0502, G2 Loss: 5.8778, D1 Loss: 1.0969, D2 Loss: 0.1560\n",
      "Batch 5, G1 Loss: 1.8961, G2 Loss: 2.5021, D1 Loss: 1.2359, D2 Loss: 0.3767\n",
      "Batch 6, G1 Loss: 0.9694, G2 Loss: 4.3503, D1 Loss: 1.1310, D2 Loss: 0.2134\n",
      "Batch 7, G1 Loss: 1.9687, G2 Loss: 4.5531, D1 Loss: 0.9057, D2 Loss: 0.0975\n",
      "Batch 8, G1 Loss: 1.3706, G2 Loss: 2.9668, D1 Loss: 0.9663, D2 Loss: 0.1929\n",
      "Batch 9, G1 Loss: 1.5467, G2 Loss: 4.0348, D1 Loss: 0.7645, D2 Loss: 0.2004\n",
      "Batch 10, G1 Loss: 1.8716, G2 Loss: 4.5068, D1 Loss: 0.9623, D2 Loss: 0.0958\n",
      "Batch 11, G1 Loss: 1.2615, G2 Loss: 8.3012, D1 Loss: 1.0954, D2 Loss: 0.1304\n",
      "Batch 12, G1 Loss: 1.2634, G2 Loss: 1.7845, D1 Loss: 0.9099, D2 Loss: 0.4454\n",
      "Batch 13, G1 Loss: 2.2033, G2 Loss: 7.5662, D1 Loss: 1.2602, D2 Loss: 0.2956\n",
      "Batch 14, G1 Loss: 0.8822, G2 Loss: 8.1646, D1 Loss: 1.1346, D2 Loss: 0.2634\n",
      "Batch 15, G1 Loss: 1.6099, G2 Loss: 8.7739, D1 Loss: 1.1476, D2 Loss: 0.0538\n",
      "Batch 16, G1 Loss: 1.3025, G2 Loss: 2.0986, D1 Loss: 0.9791, D2 Loss: 0.4800\n",
      "Batch 17, G1 Loss: 1.2387, G2 Loss: 6.3675, D1 Loss: 1.0728, D2 Loss: 0.1021\n",
      "Batch 18, G1 Loss: 1.8027, G2 Loss: 6.5121, D1 Loss: 0.9505, D2 Loss: 0.3116\n",
      "Batch 19, G1 Loss: 0.9715, G2 Loss: 2.8732, D1 Loss: 1.1843, D2 Loss: 0.2530\n",
      "Batch 20, G1 Loss: 1.8572, G2 Loss: 3.7158, D1 Loss: 0.9776, D2 Loss: 0.1341\n",
      "Batch 21, G1 Loss: 1.2544, G2 Loss: 3.7898, D1 Loss: 0.9670, D2 Loss: 0.2605\n",
      "Batch 22, G1 Loss: 1.3152, G2 Loss: 4.1813, D1 Loss: 1.0059, D2 Loss: 0.0952\n",
      "Batch 23, G1 Loss: 1.7390, G2 Loss: 4.6518, D1 Loss: 0.9707, D2 Loss: 0.0657\n",
      "Batch 24, G1 Loss: 1.1135, G2 Loss: 2.4476, D1 Loss: 0.9599, D2 Loss: 0.3115\n",
      "Batch 25, G1 Loss: 2.1899, G2 Loss: 5.1727, D1 Loss: 0.9967, D2 Loss: 0.2852\n",
      "Batch 26, G1 Loss: 0.9052, G2 Loss: 3.4477, D1 Loss: 1.2192, D2 Loss: 0.1258\n",
      "Batch 27, G1 Loss: 1.5912, G2 Loss: 2.9999, D1 Loss: 0.9501, D2 Loss: 0.1659\n",
      "Batch 28, G1 Loss: 1.3091, G2 Loss: 4.1230, D1 Loss: 1.1358, D2 Loss: 0.2871\n",
      "Batch 29, G1 Loss: 1.2439, G2 Loss: 4.3277, D1 Loss: 0.9930, D2 Loss: 0.0802\n",
      "Batch 30, G1 Loss: 1.1960, G2 Loss: 9.6204, D1 Loss: 1.2158, D2 Loss: 0.0857\n",
      "Batch 31, G1 Loss: 1.9679, G2 Loss: 3.6721, D1 Loss: 1.1389, D2 Loss: 0.1412\n",
      "Batch 32, G1 Loss: 0.6007, G2 Loss: 2.3549, D1 Loss: 1.7617, D2 Loss: 0.3905\n",
      "Epoch 64 Average -> G1 Loss: 1.4316695928573608, G2 Loss: 4.634805202484131, D1 Loss: 1.0737169981002808, D2 Loss: 0.20700369775295258\n",
      "Epoch 65/100\n",
      "Batch 1, G1 Loss: 2.5326, G2 Loss: 7.5187, D1 Loss: 1.0341, D2 Loss: 0.2302\n",
      "Batch 2, G1 Loss: 1.5509, G2 Loss: 5.5893, D1 Loss: 0.7953, D2 Loss: 0.1274\n",
      "Batch 3, G1 Loss: 1.2283, G2 Loss: 3.4836, D1 Loss: 0.7380, D2 Loss: 0.1348\n",
      "Batch 4, G1 Loss: 2.4026, G2 Loss: 2.5010, D1 Loss: 0.7055, D2 Loss: 0.2508\n",
      "Batch 5, G1 Loss: 1.3598, G2 Loss: 5.6256, D1 Loss: 0.7755, D2 Loss: 0.2604\n",
      "Batch 6, G1 Loss: 1.9787, G2 Loss: 4.2968, D1 Loss: 0.5318, D2 Loss: 0.1220\n",
      "Batch 7, G1 Loss: 1.7506, G2 Loss: 3.1115, D1 Loss: 0.7486, D2 Loss: 0.1780\n",
      "Batch 8, G1 Loss: 1.4802, G2 Loss: 5.7135, D1 Loss: 0.8682, D2 Loss: 0.0725\n",
      "Batch 9, G1 Loss: 1.5659, G2 Loss: 4.2518, D1 Loss: 0.8183, D2 Loss: 0.0579\n",
      "Batch 10, G1 Loss: 1.6799, G2 Loss: 2.3574, D1 Loss: 0.9465, D2 Loss: 0.2464\n",
      "Batch 11, G1 Loss: 1.3010, G2 Loss: 7.3273, D1 Loss: 1.2976, D2 Loss: 0.5401\n",
      "Batch 12, G1 Loss: 1.5561, G2 Loss: 5.1284, D1 Loss: 1.1792, D2 Loss: 0.1035\n",
      "Batch 13, G1 Loss: 1.2831, G2 Loss: 8.3976, D1 Loss: 1.1239, D2 Loss: 0.0282\n",
      "Batch 14, G1 Loss: 1.3522, G2 Loss: 1.2637, D1 Loss: 0.9515, D2 Loss: 0.7648\n",
      "Batch 15, G1 Loss: 1.9506, G2 Loss: 8.8543, D1 Loss: 0.9173, D2 Loss: 0.5760\n",
      "Batch 16, G1 Loss: 0.8884, G2 Loss: 7.3325, D1 Loss: 1.2771, D2 Loss: 0.2011\n",
      "Batch 17, G1 Loss: 1.8531, G2 Loss: 4.8365, D1 Loss: 0.9921, D2 Loss: 0.0444\n",
      "Batch 18, G1 Loss: 1.2723, G2 Loss: 2.7141, D1 Loss: 1.0676, D2 Loss: 0.1761\n",
      "Batch 19, G1 Loss: 1.0146, G2 Loss: 3.6593, D1 Loss: 1.1742, D2 Loss: 0.0797\n",
      "Batch 20, G1 Loss: 1.8579, G2 Loss: 3.3587, D1 Loss: 1.0351, D2 Loss: 0.1875\n",
      "Batch 21, G1 Loss: 1.1369, G2 Loss: 2.7303, D1 Loss: 1.0831, D2 Loss: 0.1802\n",
      "Batch 22, G1 Loss: 1.6293, G2 Loss: 5.0425, D1 Loss: 1.0388, D2 Loss: 0.0913\n",
      "Batch 23, G1 Loss: 1.3198, G2 Loss: 5.3449, D1 Loss: 0.9528, D2 Loss: 0.1914\n",
      "Batch 24, G1 Loss: 2.0990, G2 Loss: 5.5806, D1 Loss: 0.7869, D2 Loss: 0.0649\n",
      "Batch 25, G1 Loss: 1.6091, G2 Loss: 1.4363, D1 Loss: 0.7628, D2 Loss: 0.6617\n",
      "Batch 26, G1 Loss: 1.4688, G2 Loss: 10.8712, D1 Loss: 0.7801, D2 Loss: 2.2418\n",
      "Batch 27, G1 Loss: 2.1633, G2 Loss: 5.6431, D1 Loss: 0.5816, D2 Loss: 0.0443\n",
      "Batch 28, G1 Loss: 1.6681, G2 Loss: 2.3046, D1 Loss: 0.7247, D2 Loss: 0.3532\n",
      "Batch 29, G1 Loss: 1.6169, G2 Loss: 6.7161, D1 Loss: 1.0004, D2 Loss: 0.7362\n",
      "Batch 30, G1 Loss: 2.3475, G2 Loss: 2.2954, D1 Loss: 0.9239, D2 Loss: 0.4440\n",
      "Batch 31, G1 Loss: 0.9663, G2 Loss: 2.9359, D1 Loss: 1.1758, D2 Loss: 0.1920\n",
      "Batch 32, G1 Loss: 2.6036, G2 Loss: 5.9540, D1 Loss: 0.7398, D2 Loss: 0.5928\n",
      "Epoch 65 Average -> G1 Loss: 1.6402270793914795, G2 Loss: 4.81801700592041, D1 Loss: 0.9227505922317505, D2 Loss: 0.31798478960990906\n",
      "Epoch 66/100\n",
      "Batch 1, G1 Loss: 1.5888, G2 Loss: 2.2223, D1 Loss: 0.7173, D2 Loss: 0.3457\n",
      "Batch 2, G1 Loss: 2.0067, G2 Loss: 4.1449, D1 Loss: 0.5955, D2 Loss: 0.1090\n",
      "Batch 3, G1 Loss: 1.3397, G2 Loss: 4.4870, D1 Loss: 0.9237, D2 Loss: 0.1128\n",
      "Batch 4, G1 Loss: 1.7970, G2 Loss: 3.8435, D1 Loss: 0.8508, D2 Loss: 0.1698\n",
      "Batch 5, G1 Loss: 1.2239, G2 Loss: 4.7164, D1 Loss: 0.9513, D2 Loss: 0.0624\n",
      "Batch 6, G1 Loss: 2.2751, G2 Loss: 1.4536, D1 Loss: 0.6613, D2 Loss: 0.5696\n",
      "Batch 7, G1 Loss: 1.8293, G2 Loss: 7.6400, D1 Loss: 0.5753, D2 Loss: 0.6121\n",
      "Batch 8, G1 Loss: 2.1823, G2 Loss: 6.2568, D1 Loss: 0.5339, D2 Loss: 0.3643\n",
      "Batch 9, G1 Loss: 1.8170, G2 Loss: 3.4011, D1 Loss: 0.4630, D2 Loss: 0.1494\n",
      "Batch 10, G1 Loss: 2.2913, G2 Loss: 1.8762, D1 Loss: 0.3896, D2 Loss: 0.5547\n",
      "Batch 11, G1 Loss: 1.5613, G2 Loss: 7.3445, D1 Loss: 1.3554, D2 Loss: 0.4971\n",
      "Batch 12, G1 Loss: 1.0376, G2 Loss: 4.0321, D1 Loss: 1.3146, D2 Loss: 0.0723\n",
      "Batch 13, G1 Loss: 2.4140, G2 Loss: 3.2588, D1 Loss: 1.3766, D2 Loss: 0.2683\n",
      "Batch 14, G1 Loss: 1.2774, G2 Loss: 3.4076, D1 Loss: 1.3836, D2 Loss: 0.0881\n",
      "Batch 15, G1 Loss: 1.1651, G2 Loss: 4.4323, D1 Loss: 1.5879, D2 Loss: 0.4781\n",
      "Batch 16, G1 Loss: 2.9342, G2 Loss: 1.8597, D1 Loss: 0.8912, D2 Loss: 0.5055\n",
      "Batch 17, G1 Loss: 1.0628, G2 Loss: 8.4944, D1 Loss: 1.2054, D2 Loss: 0.0566\n",
      "Batch 18, G1 Loss: 1.9150, G2 Loss: 13.1340, D1 Loss: 0.8830, D2 Loss: 0.1861\n",
      "Batch 19, G1 Loss: 1.0282, G2 Loss: 7.1784, D1 Loss: 1.3966, D2 Loss: 0.0539\n",
      "Batch 20, G1 Loss: 1.4272, G2 Loss: 3.7247, D1 Loss: 1.4387, D2 Loss: 0.1210\n",
      "Batch 21, G1 Loss: 1.1023, G2 Loss: 3.2798, D1 Loss: 1.1929, D2 Loss: 0.1885\n",
      "Batch 22, G1 Loss: 1.6897, G2 Loss: 3.7857, D1 Loss: 1.1093, D2 Loss: 0.1258\n",
      "Batch 23, G1 Loss: 1.2213, G2 Loss: 4.1995, D1 Loss: 1.1167, D2 Loss: 0.1052\n",
      "Batch 24, G1 Loss: 1.6426, G2 Loss: 3.5148, D1 Loss: 0.7796, D2 Loss: 0.2252\n",
      "Batch 25, G1 Loss: 1.7525, G2 Loss: 2.8006, D1 Loss: 0.8411, D2 Loss: 0.2023\n",
      "Batch 26, G1 Loss: 0.9123, G2 Loss: 4.1815, D1 Loss: 1.0838, D2 Loss: 0.1293\n",
      "Batch 27, G1 Loss: 2.6675, G2 Loss: 4.2205, D1 Loss: 1.1661, D2 Loss: 0.3172\n",
      "Batch 28, G1 Loss: 0.6898, G2 Loss: 2.1519, D1 Loss: 1.8390, D2 Loss: 0.4282\n",
      "Batch 29, G1 Loss: 1.3011, G2 Loss: 6.8492, D1 Loss: 1.0577, D2 Loss: 0.1572\n",
      "Batch 30, G1 Loss: 2.3679, G2 Loss: 7.1198, D1 Loss: 0.9186, D2 Loss: 0.1115\n",
      "Batch 31, G1 Loss: 1.3383, G2 Loss: 7.4382, D1 Loss: 1.0438, D2 Loss: 0.1644\n",
      "Batch 32, G1 Loss: 1.0526, G2 Loss: 4.2763, D1 Loss: 1.1545, D2 Loss: 0.0752\n",
      "Epoch 66 Average -> G1 Loss: 1.622243046760559, G2 Loss: 4.710190773010254, D1 Loss: 1.024940848350525, D2 Loss: 0.23771078884601593\n",
      "Epoch 67/100\n",
      "Batch 1, G1 Loss: 3.0755, G2 Loss: 1.4810, D1 Loss: 0.8825, D2 Loss: 0.5612\n",
      "Batch 2, G1 Loss: 1.1758, G2 Loss: 8.5393, D1 Loss: 0.8400, D2 Loss: 0.3320\n",
      "Batch 3, G1 Loss: 1.5682, G2 Loss: 8.3073, D1 Loss: 0.7612, D2 Loss: 0.2454\n",
      "Batch 4, G1 Loss: 1.9865, G2 Loss: 5.2706, D1 Loss: 0.9319, D2 Loss: 0.1273\n",
      "Batch 5, G1 Loss: 0.9852, G2 Loss: 2.3338, D1 Loss: 1.1447, D2 Loss: 0.1686\n",
      "Batch 6, G1 Loss: 1.7303, G2 Loss: 2.8311, D1 Loss: 1.1363, D2 Loss: 0.1490\n",
      "Batch 7, G1 Loss: 1.3553, G2 Loss: 4.7918, D1 Loss: 1.1169, D2 Loss: 0.3328\n",
      "Batch 8, G1 Loss: 1.2322, G2 Loss: 4.0766, D1 Loss: 1.0141, D2 Loss: 0.1065\n",
      "Batch 9, G1 Loss: 1.7225, G2 Loss: 6.0815, D1 Loss: 0.9987, D2 Loss: 0.0715\n",
      "Batch 10, G1 Loss: 1.0481, G2 Loss: 1.4279, D1 Loss: 1.1653, D2 Loss: 0.6338\n",
      "Batch 11, G1 Loss: 1.9429, G2 Loss: 10.1686, D1 Loss: 0.8784, D2 Loss: 0.8559\n",
      "Batch 12, G1 Loss: 1.1497, G2 Loss: 7.5614, D1 Loss: 1.2855, D2 Loss: 0.2911\n",
      "Batch 13, G1 Loss: 1.1576, G2 Loss: 4.2363, D1 Loss: 1.1556, D2 Loss: 0.0642\n",
      "Batch 14, G1 Loss: 2.0233, G2 Loss: 2.5115, D1 Loss: 0.9412, D2 Loss: 0.2037\n",
      "Batch 15, G1 Loss: 1.6006, G2 Loss: 4.3496, D1 Loss: 0.6578, D2 Loss: 0.1200\n",
      "Batch 16, G1 Loss: 1.1405, G2 Loss: 4.1765, D1 Loss: 0.8334, D2 Loss: 0.1322\n",
      "Batch 17, G1 Loss: 2.0302, G2 Loss: 3.9883, D1 Loss: 0.8271, D2 Loss: 0.2381\n",
      "Batch 18, G1 Loss: 1.3002, G2 Loss: 2.5236, D1 Loss: 0.8601, D2 Loss: 0.2541\n",
      "Batch 19, G1 Loss: 1.3734, G2 Loss: 4.1268, D1 Loss: 1.0912, D2 Loss: 0.1195\n",
      "Batch 20, G1 Loss: 1.6310, G2 Loss: 4.1291, D1 Loss: 0.9079, D2 Loss: 0.0811\n",
      "Batch 21, G1 Loss: 1.4888, G2 Loss: 5.0194, D1 Loss: 0.8858, D2 Loss: 0.0996\n",
      "Batch 22, G1 Loss: 1.2632, G2 Loss: 4.8684, D1 Loss: 0.9137, D2 Loss: 0.0591\n",
      "Batch 23, G1 Loss: 1.9147, G2 Loss: 2.4318, D1 Loss: 0.9224, D2 Loss: 0.2375\n",
      "Batch 24, G1 Loss: 0.9527, G2 Loss: 5.8098, D1 Loss: 1.1108, D2 Loss: 0.1889\n",
      "Batch 25, G1 Loss: 2.5180, G2 Loss: 4.8913, D1 Loss: 1.3416, D2 Loss: 0.1665\n",
      "Batch 26, G1 Loss: 0.7761, G2 Loss: 2.4235, D1 Loss: 1.3019, D2 Loss: 0.2897\n",
      "Batch 27, G1 Loss: 1.9814, G2 Loss: 5.3416, D1 Loss: 1.0226, D2 Loss: 0.1931\n",
      "Batch 28, G1 Loss: 1.6662, G2 Loss: 5.6435, D1 Loss: 0.9564, D2 Loss: 0.2104\n",
      "Batch 29, G1 Loss: 0.7653, G2 Loss: 5.0701, D1 Loss: 1.1942, D2 Loss: 0.0620\n",
      "Batch 30, G1 Loss: 2.5324, G2 Loss: 2.3171, D1 Loss: 0.9140, D2 Loss: 0.5322\n",
      "Batch 31, G1 Loss: 1.6927, G2 Loss: 5.4497, D1 Loss: 0.6876, D2 Loss: 0.3211\n",
      "Batch 32, G1 Loss: 1.2834, G2 Loss: 5.5888, D1 Loss: 0.7770, D2 Loss: 0.0989\n",
      "Epoch 67 Average -> G1 Loss: 1.56449556350708, G2 Loss: 4.617737770080566, D1 Loss: 0.9830483794212341, D2 Loss: 0.23585210740566254\n",
      "Epoch 68/100\n",
      "Batch 1, G1 Loss: 2.1750, G2 Loss: 4.2019, D1 Loss: 0.5856, D2 Loss: 0.0443\n",
      "Batch 2, G1 Loss: 1.9619, G2 Loss: 2.5394, D1 Loss: 0.5906, D2 Loss: 0.2377\n",
      "Batch 3, G1 Loss: 1.6558, G2 Loss: 5.1407, D1 Loss: 0.5854, D2 Loss: 0.1298\n",
      "Batch 4, G1 Loss: 2.3864, G2 Loss: 5.9210, D1 Loss: 0.7800, D2 Loss: 0.1528\n",
      "Batch 5, G1 Loss: 1.6113, G2 Loss: 2.9328, D1 Loss: 0.8644, D2 Loss: 0.1656\n",
      "Batch 6, G1 Loss: 0.9252, G2 Loss: 2.4876, D1 Loss: 1.4893, D2 Loss: 0.2049\n",
      "Batch 7, G1 Loss: 3.2092, G2 Loss: 5.3373, D1 Loss: 1.6918, D2 Loss: 0.2274\n",
      "Batch 8, G1 Loss: 0.8414, G2 Loss: 3.3132, D1 Loss: 1.8992, D2 Loss: 0.2175\n",
      "Batch 9, G1 Loss: 1.7928, G2 Loss: 2.5894, D1 Loss: 1.3569, D2 Loss: 0.2234\n",
      "Batch 10, G1 Loss: 1.6094, G2 Loss: 5.2045, D1 Loss: 0.9379, D2 Loss: 0.1066\n",
      "Batch 11, G1 Loss: 1.8946, G2 Loss: 5.0222, D1 Loss: 0.6882, D2 Loss: 0.1447\n",
      "Batch 12, G1 Loss: 1.9415, G2 Loss: 4.3126, D1 Loss: 0.5675, D2 Loss: 0.0611\n",
      "Batch 13, G1 Loss: 1.7272, G2 Loss: 4.0076, D1 Loss: 0.8161, D2 Loss: 0.0364\n",
      "Batch 14, G1 Loss: 1.1665, G2 Loss: 1.4012, D1 Loss: 1.1621, D2 Loss: 0.4924\n",
      "Batch 15, G1 Loss: 1.7080, G2 Loss: 9.8119, D1 Loss: 1.1271, D2 Loss: 0.1920\n",
      "Batch 16, G1 Loss: 1.2218, G2 Loss: 10.5709, D1 Loss: 1.1831, D2 Loss: 0.6927\n",
      "Batch 17, G1 Loss: 1.4851, G2 Loss: 6.2086, D1 Loss: 1.0387, D2 Loss: 0.0333\n",
      "Batch 18, G1 Loss: 1.2147, G2 Loss: 2.9366, D1 Loss: 0.9619, D2 Loss: 0.1770\n",
      "Batch 19, G1 Loss: 2.6264, G2 Loss: 3.4308, D1 Loss: 0.8934, D2 Loss: 0.1192\n",
      "Batch 20, G1 Loss: 1.8021, G2 Loss: 3.9851, D1 Loss: 0.5258, D2 Loss: 0.1182\n",
      "Batch 21, G1 Loss: 1.7551, G2 Loss: 4.1973, D1 Loss: 0.4374, D2 Loss: 0.1987\n",
      "Batch 22, G1 Loss: 2.2673, G2 Loss: 2.5746, D1 Loss: 0.5826, D2 Loss: 0.2330\n",
      "Batch 23, G1 Loss: 1.4042, G2 Loss: 4.3777, D1 Loss: 0.7865, D2 Loss: 0.0987\n",
      "Batch 24, G1 Loss: 2.1165, G2 Loss: 8.7861, D1 Loss: 1.0547, D2 Loss: 0.1532\n",
      "Batch 25, G1 Loss: 1.2403, G2 Loss: 2.4007, D1 Loss: 1.0467, D2 Loss: 0.2712\n",
      "Batch 26, G1 Loss: 2.1424, G2 Loss: 6.1618, D1 Loss: 1.1954, D2 Loss: 0.1710\n",
      "Batch 27, G1 Loss: 1.1558, G2 Loss: 4.4569, D1 Loss: 1.0659, D2 Loss: 0.0811\n",
      "Batch 28, G1 Loss: 2.2104, G2 Loss: 3.7426, D1 Loss: 0.8701, D2 Loss: 0.2005\n",
      "Batch 29, G1 Loss: 1.4604, G2 Loss: 2.5346, D1 Loss: 0.7418, D2 Loss: 0.2894\n",
      "Batch 30, G1 Loss: 2.6031, G2 Loss: 10.5216, D1 Loss: 0.6353, D2 Loss: 0.2392\n",
      "Batch 31, G1 Loss: 1.6067, G2 Loss: 5.1960, D1 Loss: 0.9236, D2 Loss: 0.1302\n",
      "Batch 32, G1 Loss: 1.8485, G2 Loss: 3.8332, D1 Loss: 0.6192, D2 Loss: 0.1774\n",
      "Epoch 68 Average -> G1 Loss: 1.7739629745483398, G2 Loss: 4.691822052001953, D1 Loss: 0.9282567501068115, D2 Loss: 0.18813759088516235\n",
      "Epoch 69/100\n",
      "Batch 1, G1 Loss: 2.1256, G2 Loss: 2.0523, D1 Loss: 0.4917, D2 Loss: 0.3075\n",
      "Batch 2, G1 Loss: 2.1609, G2 Loss: 4.9290, D1 Loss: 0.5824, D2 Loss: 0.1669\n",
      "Batch 3, G1 Loss: 1.9808, G2 Loss: 4.3504, D1 Loss: 0.7477, D2 Loss: 0.1166\n",
      "Batch 4, G1 Loss: 1.1769, G2 Loss: 3.5036, D1 Loss: 1.0513, D2 Loss: 0.1390\n",
      "Batch 5, G1 Loss: 2.0397, G2 Loss: 3.7858, D1 Loss: 1.0675, D2 Loss: 0.1592\n",
      "Batch 6, G1 Loss: 1.3609, G2 Loss: 3.4837, D1 Loss: 0.9509, D2 Loss: 0.0985\n",
      "Batch 7, G1 Loss: 1.9789, G2 Loss: 3.8752, D1 Loss: 0.7130, D2 Loss: 0.2374\n",
      "Batch 8, G1 Loss: 1.4914, G2 Loss: 2.9017, D1 Loss: 0.8386, D2 Loss: 0.1905\n",
      "Batch 9, G1 Loss: 1.8411, G2 Loss: 5.2701, D1 Loss: 1.0296, D2 Loss: 0.1244\n",
      "Batch 10, G1 Loss: 1.3439, G2 Loss: 7.2827, D1 Loss: 0.9671, D2 Loss: 0.1842\n",
      "Batch 11, G1 Loss: 1.9475, G2 Loss: 2.9703, D1 Loss: 1.2797, D2 Loss: 0.2100\n",
      "Batch 12, G1 Loss: 1.0131, G2 Loss: 9.8694, D1 Loss: 1.2488, D2 Loss: 0.0695\n",
      "Batch 13, G1 Loss: 1.7145, G2 Loss: 4.6337, D1 Loss: 0.9460, D2 Loss: 0.1208\n",
      "Batch 14, G1 Loss: 2.1937, G2 Loss: 2.5915, D1 Loss: 0.8809, D2 Loss: 0.3628\n",
      "Batch 15, G1 Loss: 0.8326, G2 Loss: 6.5816, D1 Loss: 1.5981, D2 Loss: 0.1692\n",
      "Batch 16, G1 Loss: 4.0431, G2 Loss: 5.5408, D1 Loss: 1.3173, D2 Loss: 0.0631\n",
      "Batch 17, G1 Loss: 1.1460, G2 Loss: 3.9020, D1 Loss: 0.8560, D2 Loss: 0.1077\n",
      "Batch 18, G1 Loss: 2.0742, G2 Loss: 3.3483, D1 Loss: 0.6253, D2 Loss: 0.1325\n",
      "Batch 19, G1 Loss: 1.8037, G2 Loss: 3.9374, D1 Loss: 0.5653, D2 Loss: 0.1238\n",
      "Batch 20, G1 Loss: 1.6505, G2 Loss: 3.4470, D1 Loss: 0.9257, D2 Loss: 0.1692\n",
      "Batch 21, G1 Loss: 1.2287, G2 Loss: 4.0420, D1 Loss: 1.1616, D2 Loss: 0.1084\n",
      "Batch 22, G1 Loss: 1.7008, G2 Loss: 3.4716, D1 Loss: 1.0115, D2 Loss: 0.1308\n",
      "Batch 23, G1 Loss: 1.1699, G2 Loss: 2.6829, D1 Loss: 1.2544, D2 Loss: 0.1851\n",
      "Batch 24, G1 Loss: 1.4193, G2 Loss: 7.7634, D1 Loss: 1.2573, D2 Loss: 0.0877\n",
      "Batch 25, G1 Loss: 1.7177, G2 Loss: 8.7613, D1 Loss: 1.0576, D2 Loss: 0.0400\n",
      "Batch 26, G1 Loss: 1.0364, G2 Loss: 3.5885, D1 Loss: 0.9665, D2 Loss: 0.1743\n",
      "Batch 27, G1 Loss: 2.6342, G2 Loss: 2.4823, D1 Loss: 0.8036, D2 Loss: 0.3358\n",
      "Batch 28, G1 Loss: 1.4988, G2 Loss: 4.9133, D1 Loss: 0.7050, D2 Loss: 0.2290\n",
      "Batch 29, G1 Loss: 1.3804, G2 Loss: 4.3344, D1 Loss: 0.7556, D2 Loss: 0.0528\n",
      "Batch 30, G1 Loss: 2.3356, G2 Loss: 3.0325, D1 Loss: 1.0265, D2 Loss: 0.1694\n",
      "Batch 31, G1 Loss: 0.7263, G2 Loss: 5.4861, D1 Loss: 1.3236, D2 Loss: 0.0438\n",
      "Batch 32, G1 Loss: 3.2899, G2 Loss: 3.6133, D1 Loss: 1.1509, D2 Loss: 0.1937\n",
      "Epoch 69 Average -> G1 Loss: 1.7517818212509155, G2 Loss: 4.450870513916016, D1 Loss: 0.9736527800559998, D2 Loss: 0.1563640683889389\n",
      "Epoch 70/100\n",
      "Batch 1, G1 Loss: 1.4500, G2 Loss: 1.8628, D1 Loss: 0.8598, D2 Loss: 0.3700\n",
      "Batch 2, G1 Loss: 0.8847, G2 Loss: 8.8878, D1 Loss: 1.2276, D2 Loss: 0.1769\n",
      "Batch 3, G1 Loss: 3.1789, G2 Loss: 9.1317, D1 Loss: 1.2148, D2 Loss: 0.5052\n",
      "Batch 4, G1 Loss: 1.2253, G2 Loss: 6.3867, D1 Loss: 0.9947, D2 Loss: 0.0495\n",
      "Batch 5, G1 Loss: 1.2174, G2 Loss: 3.8005, D1 Loss: 1.1168, D2 Loss: 0.0927\n",
      "Batch 6, G1 Loss: 2.3288, G2 Loss: 2.9356, D1 Loss: 1.0431, D2 Loss: 0.2148\n",
      "Batch 7, G1 Loss: 0.8329, G2 Loss: 6.2908, D1 Loss: 1.2114, D2 Loss: 0.0425\n",
      "Batch 8, G1 Loss: 2.8120, G2 Loss: 6.7063, D1 Loss: 0.8939, D2 Loss: 0.0559\n",
      "Batch 9, G1 Loss: 1.4089, G2 Loss: 4.6420, D1 Loss: 0.7194, D2 Loss: 0.1734\n",
      "Batch 10, G1 Loss: 1.1401, G2 Loss: 2.7334, D1 Loss: 0.8675, D2 Loss: 0.1898\n",
      "Batch 11, G1 Loss: 3.1091, G2 Loss: 6.2351, D1 Loss: 1.1211, D2 Loss: 0.0993\n",
      "Batch 12, G1 Loss: 0.9300, G2 Loss: 7.1630, D1 Loss: 1.0362, D2 Loss: 0.0343\n",
      "Batch 13, G1 Loss: 1.6591, G2 Loss: 3.5172, D1 Loss: 0.8734, D2 Loss: 0.1820\n",
      "Batch 14, G1 Loss: 1.6322, G2 Loss: 4.3404, D1 Loss: 0.8622, D2 Loss: 0.0676\n",
      "Batch 15, G1 Loss: 1.2336, G2 Loss: 3.6802, D1 Loss: 0.8253, D2 Loss: 0.2142\n",
      "Batch 16, G1 Loss: 1.9506, G2 Loss: 2.8068, D1 Loss: 0.8585, D2 Loss: 0.1895\n",
      "Batch 17, G1 Loss: 1.4128, G2 Loss: 4.4065, D1 Loss: 1.0190, D2 Loss: 0.1566\n",
      "Batch 18, G1 Loss: 1.3055, G2 Loss: 4.5991, D1 Loss: 0.9526, D2 Loss: 0.0467\n",
      "Batch 19, G1 Loss: 2.0128, G2 Loss: 2.4904, D1 Loss: 1.0533, D2 Loss: 0.3062\n",
      "Batch 20, G1 Loss: 0.9999, G2 Loss: 5.6416, D1 Loss: 1.1478, D2 Loss: 0.1359\n",
      "Batch 21, G1 Loss: 2.0717, G2 Loss: 4.6343, D1 Loss: 0.9574, D2 Loss: 0.1253\n",
      "Batch 22, G1 Loss: 1.5736, G2 Loss: 3.9788, D1 Loss: 0.8712, D2 Loss: 0.0981\n",
      "Batch 23, G1 Loss: 1.5725, G2 Loss: 4.5322, D1 Loss: 0.8940, D2 Loss: 0.0359\n",
      "Batch 24, G1 Loss: 1.2381, G2 Loss: 2.8861, D1 Loss: 0.8449, D2 Loss: 0.1305\n",
      "Batch 25, G1 Loss: 2.0873, G2 Loss: 3.8389, D1 Loss: 0.8881, D2 Loss: 0.1572\n",
      "Batch 26, G1 Loss: 1.2123, G2 Loss: 3.8205, D1 Loss: 1.0650, D2 Loss: 0.0800\n",
      "Batch 27, G1 Loss: 1.2621, G2 Loss: 4.2910, D1 Loss: 1.0918, D2 Loss: 0.0763\n",
      "Batch 28, G1 Loss: 1.7709, G2 Loss: 5.4055, D1 Loss: 1.0977, D2 Loss: 0.2574\n",
      "Batch 29, G1 Loss: 0.9894, G2 Loss: 8.3690, D1 Loss: 1.1728, D2 Loss: 0.0675\n",
      "Batch 30, G1 Loss: 1.7905, G2 Loss: 2.3737, D1 Loss: 1.1705, D2 Loss: 0.3603\n",
      "Batch 31, G1 Loss: 1.1447, G2 Loss: 6.8880, D1 Loss: 1.0851, D2 Loss: 0.1533\n",
      "Batch 32, G1 Loss: 1.0060, G2 Loss: 7.8030, D1 Loss: 1.2301, D2 Loss: 0.1419\n",
      "Epoch 70 Average -> G1 Loss: 1.5763534307479858, G2 Loss: 4.908712387084961, D1 Loss: 1.0083328485488892, D2 Loss: 0.15583091974258423\n",
      "Epoch 71/100\n",
      "Batch 1, G1 Loss: 2.4342, G2 Loss: 4.5403, D1 Loss: 1.0233, D2 Loss: 0.0804\n",
      "Batch 2, G1 Loss: 1.0200, G2 Loss: 2.3501, D1 Loss: 0.9179, D2 Loss: 0.2212\n",
      "Batch 3, G1 Loss: 1.9011, G2 Loss: 5.6869, D1 Loss: 0.6553, D2 Loss: 0.0722\n",
      "Batch 4, G1 Loss: 2.4165, G2 Loss: 5.5452, D1 Loss: 0.7303, D2 Loss: 0.0678\n",
      "Batch 5, G1 Loss: 1.1000, G2 Loss: 5.6438, D1 Loss: 0.8607, D2 Loss: 0.0390\n",
      "Batch 6, G1 Loss: 2.3022, G2 Loss: 2.9697, D1 Loss: 0.6866, D2 Loss: 0.2198\n",
      "Batch 7, G1 Loss: 1.6965, G2 Loss: 2.2554, D1 Loss: 0.9037, D2 Loss: 0.3179\n",
      "Batch 8, G1 Loss: 0.9686, G2 Loss: 7.7708, D1 Loss: 1.0990, D2 Loss: 0.1974\n",
      "Batch 9, G1 Loss: 3.0583, G2 Loss: 7.4323, D1 Loss: 0.6852, D2 Loss: 0.2203\n",
      "Batch 10, G1 Loss: 1.3910, G2 Loss: 7.2994, D1 Loss: 0.6306, D2 Loss: 0.0806\n",
      "Batch 11, G1 Loss: 1.7696, G2 Loss: 1.0835, D1 Loss: 0.6370, D2 Loss: 0.8919\n",
      "Batch 12, G1 Loss: 1.8921, G2 Loss: 17.6942, D1 Loss: 0.8559, D2 Loss: 0.4131\n",
      "Batch 13, G1 Loss: 0.9868, G2 Loss: 18.0756, D1 Loss: 1.1233, D2 Loss: 0.1860\n",
      "Batch 14, G1 Loss: 2.3019, G2 Loss: 14.0159, D1 Loss: 0.9702, D2 Loss: 0.0536\n",
      "Batch 15, G1 Loss: 1.5934, G2 Loss: 9.2280, D1 Loss: 0.8424, D2 Loss: 0.0286\n",
      "Batch 16, G1 Loss: 1.5977, G2 Loss: 4.8008, D1 Loss: 0.8136, D2 Loss: 0.0459\n",
      "Batch 17, G1 Loss: 2.0466, G2 Loss: 2.6405, D1 Loss: 0.8386, D2 Loss: 0.1863\n",
      "Batch 18, G1 Loss: 1.2867, G2 Loss: 8.5369, D1 Loss: 0.9285, D2 Loss: 0.0174\n",
      "Batch 19, G1 Loss: 1.3607, G2 Loss: 4.7523, D1 Loss: 1.0097, D2 Loss: 0.0328\n",
      "Batch 20, G1 Loss: 1.9434, G2 Loss: 8.0924, D1 Loss: 1.2720, D2 Loss: 0.0518\n",
      "Batch 21, G1 Loss: 0.6137, G2 Loss: 1.7897, D1 Loss: 1.9182, D2 Loss: 0.3281\n",
      "Batch 22, G1 Loss: 2.8340, G2 Loss: 8.1685, D1 Loss: 1.6129, D2 Loss: 0.2256\n",
      "Batch 23, G1 Loss: 1.1205, G2 Loss: 5.8499, D1 Loss: 0.9955, D2 Loss: 0.0486\n",
      "Batch 24, G1 Loss: 2.0669, G2 Loss: 4.1686, D1 Loss: 0.7401, D2 Loss: 0.0853\n",
      "Batch 25, G1 Loss: 0.9999, G2 Loss: 2.5632, D1 Loss: 1.0802, D2 Loss: 0.3712\n",
      "Batch 26, G1 Loss: 1.8466, G2 Loss: 5.8737, D1 Loss: 1.1004, D2 Loss: 0.2205\n",
      "Batch 27, G1 Loss: 0.7116, G2 Loss: 4.8593, D1 Loss: 1.5366, D2 Loss: 0.0371\n",
      "Batch 28, G1 Loss: 3.8115, G2 Loss: 3.8952, D1 Loss: 1.2020, D2 Loss: 0.1127\n",
      "Batch 29, G1 Loss: 1.6560, G2 Loss: 2.1385, D1 Loss: 0.5755, D2 Loss: 0.3029\n",
      "Batch 30, G1 Loss: 1.7736, G2 Loss: 7.6827, D1 Loss: 0.7549, D2 Loss: 0.1455\n",
      "Batch 31, G1 Loss: 1.7417, G2 Loss: 6.2878, D1 Loss: 0.8197, D2 Loss: 0.1353\n",
      "Batch 32, G1 Loss: 1.7391, G2 Loss: 3.2372, D1 Loss: 0.8316, D2 Loss: 0.1931\n",
      "Epoch 71 Average -> G1 Loss: 1.7494566440582275, G2 Loss: 6.153997898101807, D1 Loss: 0.9578592777252197, D2 Loss: 0.17593710124492645\n",
      "Epoch 72/100\n",
      "Batch 1, G1 Loss: 1.3277, G2 Loss: 7.5623, D1 Loss: 0.7998, D2 Loss: 0.0263\n",
      "Batch 2, G1 Loss: 1.4640, G2 Loss: 1.6478, D1 Loss: 0.9569, D2 Loss: 0.4544\n",
      "Batch 3, G1 Loss: 1.4417, G2 Loss: 11.3530, D1 Loss: 1.1883, D2 Loss: 0.3831\n",
      "Batch 4, G1 Loss: 1.0537, G2 Loss: 9.7901, D1 Loss: 1.2932, D2 Loss: 0.3059\n",
      "Batch 5, G1 Loss: 1.9983, G2 Loss: 5.9383, D1 Loss: 1.2131, D2 Loss: 0.0485\n",
      "Batch 6, G1 Loss: 0.9356, G2 Loss: 3.9558, D1 Loss: 1.0036, D2 Loss: 0.0584\n",
      "Batch 7, G1 Loss: 1.7773, G2 Loss: 0.8023, D1 Loss: 0.7628, D2 Loss: 1.2379\n",
      "Batch 8, G1 Loss: 1.9405, G2 Loss: 23.0130, D1 Loss: 0.9813, D2 Loss: 1.2378\n",
      "Batch 9, G1 Loss: 0.6732, G2 Loss: 21.8894, D1 Loss: 1.4451, D2 Loss: 0.2303\n",
      "Batch 10, G1 Loss: 2.5939, G2 Loss: 14.7734, D1 Loss: 1.1094, D2 Loss: 0.0785\n",
      "Batch 11, G1 Loss: 1.5689, G2 Loss: 7.2541, D1 Loss: 0.8014, D2 Loss: 0.0765\n",
      "Batch 12, G1 Loss: 1.4074, G2 Loss: 2.5889, D1 Loss: 0.7477, D2 Loss: 0.2948\n",
      "Batch 13, G1 Loss: 2.9876, G2 Loss: 5.2535, D1 Loss: 0.8737, D2 Loss: 0.0703\n",
      "Batch 14, G1 Loss: 0.6684, G2 Loss: 5.1779, D1 Loss: 1.3389, D2 Loss: 0.1684\n",
      "Batch 15, G1 Loss: 2.8796, G2 Loss: 6.2822, D1 Loss: 0.9284, D2 Loss: 0.2274\n",
      "Batch 16, G1 Loss: 0.9889, G2 Loss: 3.1075, D1 Loss: 1.0646, D2 Loss: 0.1466\n",
      "Batch 17, G1 Loss: 1.5220, G2 Loss: 1.7675, D1 Loss: 0.9432, D2 Loss: 0.3216\n",
      "Batch 18, G1 Loss: 1.6169, G2 Loss: 7.2835, D1 Loss: 0.9668, D2 Loss: 0.4181\n",
      "Batch 19, G1 Loss: 1.0737, G2 Loss: 5.7380, D1 Loss: 0.9701, D2 Loss: 0.0808\n",
      "Batch 20, G1 Loss: 2.0122, G2 Loss: 9.8937, D1 Loss: 1.0440, D2 Loss: 0.1011\n",
      "Batch 21, G1 Loss: 1.6557, G2 Loss: 1.6332, D1 Loss: 0.6867, D2 Loss: 0.3775\n",
      "Batch 22, G1 Loss: 1.2288, G2 Loss: 8.5569, D1 Loss: 0.8643, D2 Loss: 0.0455\n",
      "Batch 23, G1 Loss: 2.8302, G2 Loss: 7.0657, D1 Loss: 1.0329, D2 Loss: 0.0878\n",
      "Batch 24, G1 Loss: 0.9764, G2 Loss: 3.6972, D1 Loss: 1.2888, D2 Loss: 0.1322\n",
      "Batch 25, G1 Loss: 3.5224, G2 Loss: 2.7433, D1 Loss: 1.1832, D2 Loss: 0.2324\n",
      "Batch 26, G1 Loss: 1.4264, G2 Loss: 4.8842, D1 Loss: 0.7815, D2 Loss: 0.2630\n",
      "Batch 27, G1 Loss: 1.5211, G2 Loss: 4.0489, D1 Loss: 0.6791, D2 Loss: 0.0674\n",
      "Batch 28, G1 Loss: 1.9769, G2 Loss: 2.9601, D1 Loss: 0.6842, D2 Loss: 0.2050\n",
      "Batch 29, G1 Loss: 2.1658, G2 Loss: 4.1705, D1 Loss: 0.8480, D2 Loss: 0.1011\n",
      "Batch 30, G1 Loss: 1.0897, G2 Loss: 4.0037, D1 Loss: 1.0645, D2 Loss: 0.1341\n",
      "Batch 31, G1 Loss: 2.7486, G2 Loss: 3.8379, D1 Loss: 0.5149, D2 Loss: 0.1859\n",
      "Batch 32, G1 Loss: 1.7256, G2 Loss: 2.4543, D1 Loss: 0.8775, D2 Loss: 0.2527\n",
      "Epoch 72 Average -> G1 Loss: 1.7124710083007812, G2 Loss: 6.410245895385742, D1 Loss: 0.966817319393158, D2 Loss: 0.251602441072464\n",
      "Epoch 73/100\n",
      "Batch 1, G1 Loss: 1.3409, G2 Loss: 8.5756, D1 Loss: 0.9603, D2 Loss: 0.5816\n",
      "Batch 2, G1 Loss: 1.1773, G2 Loss: 2.8413, D1 Loss: 0.9968, D2 Loss: 0.1483\n",
      "Batch 3, G1 Loss: 2.4461, G2 Loss: 3.7255, D1 Loss: 1.5381, D2 Loss: 0.0756\n",
      "Batch 4, G1 Loss: 0.5713, G2 Loss: 6.7590, D1 Loss: 1.8528, D2 Loss: 0.0155\n",
      "Batch 5, G1 Loss: 3.1119, G2 Loss: 1.8692, D1 Loss: 1.1644, D2 Loss: 0.3170\n",
      "Batch 6, G1 Loss: 1.6326, G2 Loss: 8.8646, D1 Loss: 1.0915, D2 Loss: 0.6843\n",
      "Batch 7, G1 Loss: 1.0643, G2 Loss: 5.4323, D1 Loss: 1.0343, D2 Loss: 0.1956\n",
      "Batch 8, G1 Loss: 2.4379, G2 Loss: 2.2109, D1 Loss: 1.1037, D2 Loss: 0.2388\n",
      "Batch 9, G1 Loss: 0.7459, G2 Loss: 6.7275, D1 Loss: 1.4172, D2 Loss: 0.0585\n",
      "Batch 10, G1 Loss: 2.3974, G2 Loss: 6.0607, D1 Loss: 1.0567, D2 Loss: 0.0991\n",
      "Batch 11, G1 Loss: 1.3873, G2 Loss: 3.5865, D1 Loss: 1.1060, D2 Loss: 0.1652\n",
      "Batch 12, G1 Loss: 0.8044, G2 Loss: 2.9675, D1 Loss: 1.5754, D2 Loss: 0.1611\n",
      "Batch 13, G1 Loss: 2.8366, G2 Loss: 2.1960, D1 Loss: 1.3345, D2 Loss: 0.3030\n",
      "Batch 14, G1 Loss: 1.1825, G2 Loss: 11.4815, D1 Loss: 0.9756, D2 Loss: 0.4577\n",
      "Batch 15, G1 Loss: 1.2164, G2 Loss: 8.1410, D1 Loss: 0.7845, D2 Loss: 0.1626\n",
      "Batch 16, G1 Loss: 2.6900, G2 Loss: 3.6891, D1 Loss: 0.9666, D2 Loss: 0.0794\n",
      "Batch 17, G1 Loss: 0.8571, G2 Loss: 2.1660, D1 Loss: 1.0827, D2 Loss: 0.2150\n",
      "Batch 18, G1 Loss: 2.7708, G2 Loss: 6.9153, D1 Loss: 1.0038, D2 Loss: 0.0794\n",
      "Batch 19, G1 Loss: 1.2909, G2 Loss: 8.8696, D1 Loss: 0.8200, D2 Loss: 0.1333\n",
      "Batch 20, G1 Loss: 1.1181, G2 Loss: 3.7731, D1 Loss: 0.8962, D2 Loss: 0.0787\n",
      "Batch 21, G1 Loss: 2.7495, G2 Loss: 5.1884, D1 Loss: 1.0937, D2 Loss: 0.0412\n",
      "Batch 22, G1 Loss: 1.0933, G2 Loss: 1.6644, D1 Loss: 1.0430, D2 Loss: 0.4241\n",
      "Batch 23, G1 Loss: 2.0345, G2 Loss: 13.3481, D1 Loss: 0.7978, D2 Loss: 0.1150\n",
      "Batch 24, G1 Loss: 1.7153, G2 Loss: 15.0727, D1 Loss: 0.6404, D2 Loss: 0.2596\n",
      "Batch 25, G1 Loss: 1.8331, G2 Loss: 11.8788, D1 Loss: 0.7406, D2 Loss: 0.0415\n",
      "Batch 26, G1 Loss: 1.8669, G2 Loss: 11.8674, D1 Loss: 0.9521, D2 Loss: 0.0313\n",
      "Batch 27, G1 Loss: 1.0556, G2 Loss: 4.1865, D1 Loss: 1.0223, D2 Loss: 0.0289\n",
      "Batch 28, G1 Loss: 2.4687, G2 Loss: 8.6732, D1 Loss: 1.1966, D2 Loss: 0.0063\n",
      "Batch 29, G1 Loss: 1.4976, G2 Loss: 4.8492, D1 Loss: 1.0156, D2 Loss: 0.0229\n",
      "Batch 30, G1 Loss: 1.2363, G2 Loss: 1.5135, D1 Loss: 0.8865, D2 Loss: 0.4370\n",
      "Batch 31, G1 Loss: 2.6570, G2 Loss: 12.5611, D1 Loss: 0.9442, D2 Loss: 0.1111\n",
      "Batch 32, G1 Loss: 1.2024, G2 Loss: 12.8225, D1 Loss: 1.0602, D2 Loss: 0.0918\n",
      "Epoch 73 Average -> G1 Loss: 1.7028135061264038, G2 Loss: 6.577441692352295, D1 Loss: 1.0673249959945679, D2 Loss: 0.18313777446746826\n",
      "Epoch 74/100\n",
      "Batch 1, G1 Loss: 1.5978, G2 Loss: 11.7710, D1 Loss: 1.0024, D2 Loss: 0.0533\n",
      "Batch 2, G1 Loss: 1.8831, G2 Loss: 11.1568, D1 Loss: 1.4240, D2 Loss: 0.1148\n",
      "Batch 3, G1 Loss: 0.8419, G2 Loss: 8.5750, D1 Loss: 2.0114, D2 Loss: 0.0095\n",
      "Batch 4, G1 Loss: 2.1989, G2 Loss: 3.2099, D1 Loss: 1.2874, D2 Loss: 0.0768\n",
      "Batch 5, G1 Loss: 1.6862, G2 Loss: 4.3053, D1 Loss: 1.0898, D2 Loss: 0.0474\n",
      "Batch 6, G1 Loss: 1.2595, G2 Loss: 3.4339, D1 Loss: 0.9769, D2 Loss: 0.1333\n",
      "Batch 7, G1 Loss: 2.2098, G2 Loss: 4.7448, D1 Loss: 0.6593, D2 Loss: 0.1586\n",
      "Batch 8, G1 Loss: 1.5529, G2 Loss: 4.9201, D1 Loss: 0.7018, D2 Loss: 0.0330\n",
      "Batch 9, G1 Loss: 1.8111, G2 Loss: 4.2509, D1 Loss: 0.7396, D2 Loss: 0.0515\n",
      "Batch 10, G1 Loss: 1.4478, G2 Loss: 4.2005, D1 Loss: 0.9780, D2 Loss: 0.2103\n",
      "Batch 11, G1 Loss: 1.4743, G2 Loss: 2.0527, D1 Loss: 0.9093, D2 Loss: 0.3261\n",
      "Batch 12, G1 Loss: 1.3591, G2 Loss: 10.1908, D1 Loss: 0.9220, D2 Loss: 0.7395\n",
      "Batch 13, G1 Loss: 1.9241, G2 Loss: 6.3263, D1 Loss: 1.0246, D2 Loss: 0.0414\n",
      "Batch 14, G1 Loss: 1.0531, G2 Loss: 2.3260, D1 Loss: 1.0664, D2 Loss: 0.2079\n",
      "Batch 15, G1 Loss: 2.1260, G2 Loss: 4.3427, D1 Loss: 0.9589, D2 Loss: 0.0481\n",
      "Batch 16, G1 Loss: 1.3105, G2 Loss: 4.3739, D1 Loss: 0.8523, D2 Loss: 0.1133\n",
      "Batch 17, G1 Loss: 2.2540, G2 Loss: 2.7169, D1 Loss: 0.7975, D2 Loss: 0.1262\n",
      "Batch 18, G1 Loss: 1.4292, G2 Loss: 3.8991, D1 Loss: 0.7868, D2 Loss: 0.0405\n",
      "Batch 19, G1 Loss: 1.7189, G2 Loss: 7.1595, D1 Loss: 1.2673, D2 Loss: 0.0078\n",
      "Batch 20, G1 Loss: 0.9470, G2 Loss: 10.3568, D1 Loss: 1.0448, D2 Loss: 0.0551\n",
      "Batch 21, G1 Loss: 3.4965, G2 Loss: 4.5409, D1 Loss: 1.0738, D2 Loss: 0.0372\n",
      "Batch 22, G1 Loss: 1.4797, G2 Loss: 2.8254, D1 Loss: 0.8756, D2 Loss: 0.1150\n",
      "Batch 23, G1 Loss: 1.0904, G2 Loss: 3.6820, D1 Loss: 0.8642, D2 Loss: 0.0803\n",
      "Batch 24, G1 Loss: 2.4800, G2 Loss: 3.9582, D1 Loss: 0.7127, D2 Loss: 0.0733\n",
      "Batch 25, G1 Loss: 1.5706, G2 Loss: 5.3816, D1 Loss: 0.8081, D2 Loss: 0.0587\n",
      "Batch 26, G1 Loss: 1.3335, G2 Loss: 5.6621, D1 Loss: 1.2006, D2 Loss: 0.1768\n",
      "Batch 27, G1 Loss: 1.4332, G2 Loss: 3.9820, D1 Loss: 1.2058, D2 Loss: 0.0967\n",
      "Batch 28, G1 Loss: 1.8141, G2 Loss: 1.5279, D1 Loss: 1.1388, D2 Loss: 0.7000\n",
      "Batch 29, G1 Loss: 0.9785, G2 Loss: 18.9979, D1 Loss: 1.0800, D2 Loss: 0.2454\n",
      "Batch 30, G1 Loss: 2.2630, G2 Loss: 20.3952, D1 Loss: 1.0681, D2 Loss: 0.4972\n",
      "Batch 31, G1 Loss: 0.6489, G2 Loss: 13.6358, D1 Loss: 1.5469, D2 Loss: 0.0596\n",
      "Batch 32, G1 Loss: 2.9482, G2 Loss: 8.3324, D1 Loss: 1.4174, D2 Loss: 0.0274\n",
      "Epoch 74 Average -> G1 Loss: 1.675676703453064, G2 Loss: 6.476069927215576, D1 Loss: 1.046633005142212, D2 Loss: 0.14880964159965515\n",
      "Epoch 75/100\n",
      "Batch 1, G1 Loss: 1.1627, G2 Loss: 5.7554, D1 Loss: 0.9366, D2 Loss: 0.0188\n",
      "Batch 2, G1 Loss: 1.3178, G2 Loss: 0.3199, D1 Loss: 0.8195, D2 Loss: 2.6074\n",
      "Batch 3, G1 Loss: 2.8147, G2 Loss: 26.1231, D1 Loss: 0.8907, D2 Loss: 4.9618\n",
      "Batch 4, G1 Loss: 0.9568, G2 Loss: 16.9178, D1 Loss: 0.9410, D2 Loss: 0.7119\n",
      "Batch 5, G1 Loss: 2.5649, G2 Loss: 3.5194, D1 Loss: 1.4157, D2 Loss: 0.0746\n",
      "Batch 6, G1 Loss: 0.5748, G2 Loss: 0.4434, D1 Loss: 1.9401, D2 Loss: 2.1737\n",
      "Batch 7, G1 Loss: 2.6769, G2 Loss: 28.3225, D1 Loss: 1.3347, D2 Loss: 4.8178\n",
      "Batch 8, G1 Loss: 1.4074, G2 Loss: 20.7495, D1 Loss: 1.0527, D2 Loss: 0.1386\n",
      "Batch 9, G1 Loss: 1.4826, G2 Loss: 8.2648, D1 Loss: 0.7621, D2 Loss: 0.1329\n",
      "Batch 10, G1 Loss: 2.2298, G2 Loss: 1.5547, D1 Loss: 0.8489, D2 Loss: 0.9503\n",
      "Batch 11, G1 Loss: 1.3190, G2 Loss: 17.5444, D1 Loss: 0.7276, D2 Loss: 4.1806\n",
      "Batch 12, G1 Loss: 2.6214, G2 Loss: 5.2958, D1 Loss: 0.5192, D2 Loss: 0.0432\n",
      "Batch 13, G1 Loss: 2.2470, G2 Loss: 0.4952, D1 Loss: 0.5863, D2 Loss: 2.4307\n",
      "Batch 14, G1 Loss: 1.1281, G2 Loss: 13.9642, D1 Loss: 0.8341, D2 Loss: 0.7634\n",
      "Batch 15, G1 Loss: 1.8236, G2 Loss: 13.1725, D1 Loss: 1.1034, D2 Loss: 0.3814\n",
      "Batch 16, G1 Loss: 0.8796, G2 Loss: 8.2367, D1 Loss: 1.4785, D2 Loss: 0.4108\n",
      "Batch 17, G1 Loss: 1.4694, G2 Loss: 1.7812, D1 Loss: 1.1828, D2 Loss: 0.5307\n",
      "Batch 18, G1 Loss: 1.3257, G2 Loss: 2.9484, D1 Loss: 1.2283, D2 Loss: 0.2487\n",
      "Batch 19, G1 Loss: 1.7115, G2 Loss: 6.7720, D1 Loss: 0.9109, D2 Loss: 0.3246\n",
      "Batch 20, G1 Loss: 1.2796, G2 Loss: 2.9390, D1 Loss: 0.9065, D2 Loss: 0.2868\n",
      "Batch 21, G1 Loss: 1.8103, G2 Loss: 1.6626, D1 Loss: 0.8649, D2 Loss: 0.5537\n",
      "Batch 22, G1 Loss: 1.5050, G2 Loss: 6.8683, D1 Loss: 0.9160, D2 Loss: 0.2093\n",
      "Batch 23, G1 Loss: 1.8945, G2 Loss: 8.1516, D1 Loss: 0.7966, D2 Loss: 0.5978\n",
      "Batch 24, G1 Loss: 1.2648, G2 Loss: 2.9356, D1 Loss: 0.9395, D2 Loss: 0.4202\n",
      "Batch 25, G1 Loss: 2.0478, G2 Loss: 3.4019, D1 Loss: 0.7218, D2 Loss: 0.1479\n",
      "Batch 26, G1 Loss: 1.7933, G2 Loss: 5.4962, D1 Loss: 0.5917, D2 Loss: 0.2873\n",
      "Batch 27, G1 Loss: 1.4971, G2 Loss: 2.7023, D1 Loss: 1.0122, D2 Loss: 0.3137\n",
      "Batch 28, G1 Loss: 1.6030, G2 Loss: 4.6985, D1 Loss: 0.8240, D2 Loss: 0.4214\n",
      "Batch 29, G1 Loss: 2.1412, G2 Loss: 3.4845, D1 Loss: 0.7660, D2 Loss: 0.2486\n",
      "Batch 30, G1 Loss: 1.3282, G2 Loss: 3.1647, D1 Loss: 1.0780, D2 Loss: 0.3936\n",
      "Batch 31, G1 Loss: 1.8193, G2 Loss: 3.1463, D1 Loss: 0.8424, D2 Loss: 0.2154\n",
      "Batch 32, G1 Loss: 1.6637, G2 Loss: 5.8759, D1 Loss: 0.9861, D2 Loss: 0.2727\n",
      "Epoch 75 Average -> G1 Loss: 1.6675465106964111, G2 Loss: 7.397131443023682, D1 Loss: 0.9612053632736206, D2 Loss: 0.9459448456764221\n",
      "Epoch 76/100\n",
      "Batch 1, G1 Loss: 1.5430, G2 Loss: 7.1385, D1 Loss: 0.6588, D2 Loss: 0.1293\n",
      "Batch 2, G1 Loss: 2.2962, G2 Loss: 1.3385, D1 Loss: 0.6624, D2 Loss: 1.3332\n",
      "Batch 3, G1 Loss: 1.5714, G2 Loss: 15.5466, D1 Loss: 0.8472, D2 Loss: 1.3218\n",
      "Batch 4, G1 Loss: 1.3589, G2 Loss: 12.6890, D1 Loss: 0.8923, D2 Loss: 0.0891\n",
      "Batch 5, G1 Loss: 1.9259, G2 Loss: 8.0771, D1 Loss: 1.0261, D2 Loss: 0.0347\n",
      "Batch 6, G1 Loss: 0.8431, G2 Loss: 4.5599, D1 Loss: 1.4042, D2 Loss: 0.0772\n",
      "Batch 7, G1 Loss: 2.3350, G2 Loss: 1.5569, D1 Loss: 1.3630, D2 Loss: 0.6967\n",
      "Batch 8, G1 Loss: 0.7909, G2 Loss: 6.8476, D1 Loss: 1.6664, D2 Loss: 0.0240\n",
      "Batch 9, G1 Loss: 2.5318, G2 Loss: 8.5859, D1 Loss: 0.9823, D2 Loss: 0.1414\n",
      "Batch 10, G1 Loss: 1.3714, G2 Loss: 6.9626, D1 Loss: 0.9671, D2 Loss: 0.1601\n",
      "Batch 11, G1 Loss: 1.5374, G2 Loss: 2.5244, D1 Loss: 0.8492, D2 Loss: 0.3688\n",
      "Batch 12, G1 Loss: 2.0340, G2 Loss: 3.7111, D1 Loss: 0.7236, D2 Loss: 0.1818\n",
      "Batch 13, G1 Loss: 1.0843, G2 Loss: 5.0311, D1 Loss: 0.9912, D2 Loss: 0.3386\n",
      "Batch 14, G1 Loss: 2.2252, G2 Loss: 2.9315, D1 Loss: 0.9530, D2 Loss: 0.3691\n",
      "Batch 15, G1 Loss: 1.0548, G2 Loss: 6.6706, D1 Loss: 1.2273, D2 Loss: 0.0441\n",
      "Batch 16, G1 Loss: 1.8103, G2 Loss: 1.4017, D1 Loss: 0.9619, D2 Loss: 0.8870\n",
      "Batch 17, G1 Loss: 1.5707, G2 Loss: 11.8185, D1 Loss: 0.7256, D2 Loss: 0.8450\n",
      "Batch 18, G1 Loss: 2.2234, G2 Loss: 12.3432, D1 Loss: 0.6000, D2 Loss: 0.1962\n",
      "Batch 19, G1 Loss: 1.3387, G2 Loss: 5.8784, D1 Loss: 0.6044, D2 Loss: 0.0819\n",
      "Batch 20, G1 Loss: 2.7243, G2 Loss: 3.8124, D1 Loss: 0.6682, D2 Loss: 0.1231\n",
      "Batch 21, G1 Loss: 1.5827, G2 Loss: 1.4561, D1 Loss: 0.6932, D2 Loss: 0.9397\n",
      "Batch 22, G1 Loss: 1.7015, G2 Loss: 8.9181, D1 Loss: 0.6928, D2 Loss: 1.2112\n",
      "Batch 23, G1 Loss: 2.1787, G2 Loss: 6.5769, D1 Loss: 0.6038, D2 Loss: 0.1762\n",
      "Batch 24, G1 Loss: 2.2171, G2 Loss: 2.6522, D1 Loss: 0.7075, D2 Loss: 0.2038\n",
      "Batch 25, G1 Loss: 1.5894, G2 Loss: 2.6492, D1 Loss: 0.9305, D2 Loss: 0.3868\n",
      "Batch 26, G1 Loss: 1.9154, G2 Loss: 3.1953, D1 Loss: 0.9407, D2 Loss: 0.1431\n",
      "Batch 27, G1 Loss: 1.1608, G2 Loss: 5.1037, D1 Loss: 1.1403, D2 Loss: 0.0561\n",
      "Batch 28, G1 Loss: 2.4981, G2 Loss: 5.3751, D1 Loss: 0.7106, D2 Loss: 0.1244\n",
      "Batch 29, G1 Loss: 1.9299, G2 Loss: 1.4686, D1 Loss: 0.6378, D2 Loss: 0.6473\n",
      "Batch 30, G1 Loss: 1.3366, G2 Loss: 7.5428, D1 Loss: 1.2046, D2 Loss: 0.3729\n",
      "Batch 31, G1 Loss: 1.5408, G2 Loss: 6.3905, D1 Loss: 0.7838, D2 Loss: 0.2814\n",
      "Batch 32, G1 Loss: 1.5991, G2 Loss: 2.5522, D1 Loss: 0.8395, D2 Loss: 0.2330\n",
      "Epoch 76 Average -> G1 Loss: 1.7319003343582153, G2 Loss: 5.728318214416504, D1 Loss: 0.8956038951873779, D2 Loss: 0.3818432092666626\n",
      "Epoch 77/100\n",
      "Batch 1, G1 Loss: 1.8086, G2 Loss: 3.7818, D1 Loss: 0.9382, D2 Loss: 0.1114\n",
      "Batch 2, G1 Loss: 1.5399, G2 Loss: 3.2395, D1 Loss: 0.9064, D2 Loss: 0.1675\n",
      "Batch 3, G1 Loss: 2.0958, G2 Loss: 3.8391, D1 Loss: 0.9808, D2 Loss: 0.1119\n",
      "Batch 4, G1 Loss: 1.2186, G2 Loss: 8.4744, D1 Loss: 1.0890, D2 Loss: 0.2166\n",
      "Batch 5, G1 Loss: 2.5357, G2 Loss: 5.2272, D1 Loss: 1.1657, D2 Loss: 0.0743\n",
      "Batch 6, G1 Loss: 0.6342, G2 Loss: 1.6750, D1 Loss: 1.4947, D2 Loss: 0.5149\n",
      "Batch 7, G1 Loss: 4.1086, G2 Loss: 7.1449, D1 Loss: 1.5014, D2 Loss: 0.1159\n",
      "Batch 8, G1 Loss: 0.8885, G2 Loss: 7.5462, D1 Loss: 1.1035, D2 Loss: 0.3335\n",
      "Batch 9, G1 Loss: 2.3983, G2 Loss: 4.7489, D1 Loss: 0.5404, D2 Loss: 0.1423\n",
      "Batch 10, G1 Loss: 2.6216, G2 Loss: 2.8465, D1 Loss: 0.5639, D2 Loss: 0.2051\n",
      "Batch 11, G1 Loss: 1.5167, G2 Loss: 3.0289, D1 Loss: 0.5266, D2 Loss: 0.1776\n",
      "Batch 12, G1 Loss: 2.0674, G2 Loss: 3.5946, D1 Loss: 0.4739, D2 Loss: 0.2140\n",
      "Batch 13, G1 Loss: 2.7038, G2 Loss: 4.4158, D1 Loss: 0.4994, D2 Loss: 0.3637\n",
      "Batch 14, G1 Loss: 1.7051, G2 Loss: 2.0018, D1 Loss: 0.6219, D2 Loss: 0.4748\n",
      "Batch 15, G1 Loss: 2.0615, G2 Loss: 7.5331, D1 Loss: 0.6233, D2 Loss: 0.2283\n",
      "Batch 16, G1 Loss: 1.7171, G2 Loss: 14.1063, D1 Loss: 0.9010, D2 Loss: 0.3719\n",
      "Batch 17, G1 Loss: 1.7063, G2 Loss: 12.3175, D1 Loss: 1.3234, D2 Loss: 0.0164\n",
      "Batch 18, G1 Loss: 1.7020, G2 Loss: 6.0949, D1 Loss: 0.9752, D2 Loss: 0.0626\n",
      "Batch 19, G1 Loss: 1.7042, G2 Loss: 3.4536, D1 Loss: 1.1877, D2 Loss: 0.3135\n",
      "Batch 20, G1 Loss: 1.5097, G2 Loss: 3.9712, D1 Loss: 1.2054, D2 Loss: 0.1552\n",
      "Batch 21, G1 Loss: 2.2256, G2 Loss: 4.6028, D1 Loss: 0.8716, D2 Loss: 0.0429\n",
      "Batch 22, G1 Loss: 1.6617, G2 Loss: 4.4001, D1 Loss: 0.7927, D2 Loss: 0.1536\n",
      "Batch 23, G1 Loss: 1.5425, G2 Loss: 2.3443, D1 Loss: 1.0766, D2 Loss: 0.5387\n",
      "Batch 24, G1 Loss: 1.2956, G2 Loss: 3.4234, D1 Loss: 1.1799, D2 Loss: 0.2377\n",
      "Batch 25, G1 Loss: 1.8056, G2 Loss: 3.7412, D1 Loss: 1.0811, D2 Loss: 0.2230\n",
      "Batch 26, G1 Loss: 0.6848, G2 Loss: 2.6488, D1 Loss: 1.6188, D2 Loss: 0.2741\n",
      "Batch 27, G1 Loss: 2.9701, G2 Loss: 3.6963, D1 Loss: 1.6754, D2 Loss: 0.1095\n",
      "Batch 28, G1 Loss: 0.7382, G2 Loss: 4.4161, D1 Loss: 1.3519, D2 Loss: 0.2594\n",
      "Batch 29, G1 Loss: 2.2585, G2 Loss: 4.1581, D1 Loss: 0.9603, D2 Loss: 0.1160\n",
      "Batch 30, G1 Loss: 1.5654, G2 Loss: 2.9952, D1 Loss: 0.6684, D2 Loss: 0.1668\n",
      "Batch 31, G1 Loss: 1.8826, G2 Loss: 2.1126, D1 Loss: 0.7658, D2 Loss: 0.3036\n",
      "Batch 32, G1 Loss: 0.8649, G2 Loss: 6.4028, D1 Loss: 1.3024, D2 Loss: 0.3354\n",
      "Epoch 77 Average -> G1 Loss: 1.8043465614318848, G2 Loss: 4.811964988708496, D1 Loss: 0.998958945274353, D2 Loss: 0.22287511825561523\n",
      "Epoch 78/100\n",
      "Batch 1, G1 Loss: 2.1595, G2 Loss: 5.4548, D1 Loss: 1.1953, D2 Loss: 0.2829\n",
      "Batch 2, G1 Loss: 2.0825, G2 Loss: 2.5143, D1 Loss: 0.7313, D2 Loss: 0.2187\n",
      "Batch 3, G1 Loss: 1.0085, G2 Loss: 3.4732, D1 Loss: 1.2719, D2 Loss: 0.1222\n",
      "Batch 4, G1 Loss: 3.2280, G2 Loss: 4.7342, D1 Loss: 0.9042, D2 Loss: 0.1407\n",
      "Batch 5, G1 Loss: 1.2696, G2 Loss: 3.5199, D1 Loss: 0.9055, D2 Loss: 0.3130\n",
      "Batch 6, G1 Loss: 1.7455, G2 Loss: 8.3878, D1 Loss: 0.8366, D2 Loss: 0.0729\n",
      "Batch 7, G1 Loss: 1.8550, G2 Loss: 8.0586, D1 Loss: 0.7226, D2 Loss: 0.1737\n",
      "Batch 8, G1 Loss: 1.1422, G2 Loss: 2.2702, D1 Loss: 0.9963, D2 Loss: 0.2379\n",
      "Batch 9, G1 Loss: 2.6772, G2 Loss: 4.8371, D1 Loss: 1.1413, D2 Loss: 0.1325\n",
      "Batch 10, G1 Loss: 0.8923, G2 Loss: 5.8897, D1 Loss: 1.1431, D2 Loss: 0.2510\n",
      "Batch 11, G1 Loss: 2.0037, G2 Loss: 3.6373, D1 Loss: 0.8025, D2 Loss: 0.0882\n",
      "Batch 12, G1 Loss: 1.5066, G2 Loss: 2.6901, D1 Loss: 0.6926, D2 Loss: 0.1925\n",
      "Batch 13, G1 Loss: 1.4287, G2 Loss: 4.6936, D1 Loss: 0.7379, D2 Loss: 0.1398\n",
      "Batch 14, G1 Loss: 2.2101, G2 Loss: 4.6252, D1 Loss: 1.1059, D2 Loss: 0.1649\n",
      "Batch 15, G1 Loss: 1.0274, G2 Loss: 5.6219, D1 Loss: 1.1913, D2 Loss: 0.0764\n",
      "Batch 16, G1 Loss: 2.2887, G2 Loss: 6.7031, D1 Loss: 0.7857, D2 Loss: 0.0825\n",
      "Batch 17, G1 Loss: 1.7778, G2 Loss: 2.3747, D1 Loss: 0.8536, D2 Loss: 0.3014\n",
      "Batch 18, G1 Loss: 1.2819, G2 Loss: 4.2325, D1 Loss: 0.9554, D2 Loss: 0.3659\n",
      "Batch 19, G1 Loss: 2.1371, G2 Loss: 2.6842, D1 Loss: 0.6471, D2 Loss: 0.2151\n",
      "Batch 20, G1 Loss: 2.4103, G2 Loss: 4.3765, D1 Loss: 0.7522, D2 Loss: 0.1364\n",
      "Batch 21, G1 Loss: 0.9831, G2 Loss: 3.7597, D1 Loss: 1.0659, D2 Loss: 0.3334\n",
      "Batch 22, G1 Loss: 2.2189, G2 Loss: 2.9482, D1 Loss: 0.8011, D2 Loss: 0.2558\n",
      "Batch 23, G1 Loss: 1.5247, G2 Loss: 8.3499, D1 Loss: 1.1690, D2 Loss: 0.1901\n",
      "Batch 24, G1 Loss: 1.1378, G2 Loss: 3.5416, D1 Loss: 1.2227, D2 Loss: 0.1523\n",
      "Batch 25, G1 Loss: 2.2091, G2 Loss: 2.4304, D1 Loss: 1.2231, D2 Loss: 0.4157\n",
      "Batch 26, G1 Loss: 1.1068, G2 Loss: 7.8242, D1 Loss: 1.2182, D2 Loss: 0.6232\n",
      "Batch 27, G1 Loss: 1.7071, G2 Loss: 9.4677, D1 Loss: 1.0106, D2 Loss: 0.1195\n",
      "Batch 28, G1 Loss: 1.8023, G2 Loss: 5.5947, D1 Loss: 0.9125, D2 Loss: 0.0327\n",
      "Batch 29, G1 Loss: 1.1976, G2 Loss: 3.0121, D1 Loss: 0.9635, D2 Loss: 0.2032\n",
      "Batch 30, G1 Loss: 2.0031, G2 Loss: 3.1035, D1 Loss: 0.8399, D2 Loss: 0.1479\n",
      "Batch 31, G1 Loss: 1.5949, G2 Loss: 4.8402, D1 Loss: 0.8599, D2 Loss: 0.2478\n",
      "Batch 32, G1 Loss: 1.1415, G2 Loss: 2.8399, D1 Loss: 0.9130, D2 Loss: 0.1997\n",
      "Epoch 78 Average -> G1 Loss: 1.7112468481063843, G2 Loss: 4.640347480773926, D1 Loss: 0.9553607702255249, D2 Loss: 0.20718424022197723\n",
      "Epoch 79/100\n",
      "Batch 1, G1 Loss: 1.9849, G2 Loss: 4.0474, D1 Loss: 0.6639, D2 Loss: 0.0898\n",
      "Batch 2, G1 Loss: 1.5556, G2 Loss: 4.5022, D1 Loss: 0.8093, D2 Loss: 0.1035\n",
      "Batch 3, G1 Loss: 1.4545, G2 Loss: 3.4120, D1 Loss: 0.8620, D2 Loss: 0.1114\n",
      "Batch 4, G1 Loss: 1.3711, G2 Loss: 3.8538, D1 Loss: 1.1573, D2 Loss: 0.0886\n",
      "Batch 5, G1 Loss: 1.6359, G2 Loss: 4.3693, D1 Loss: 0.9158, D2 Loss: 0.2345\n",
      "Batch 6, G1 Loss: 1.4310, G2 Loss: 5.5595, D1 Loss: 0.9468, D2 Loss: 0.0588\n",
      "Batch 7, G1 Loss: 1.8924, G2 Loss: 2.0922, D1 Loss: 0.5994, D2 Loss: 0.2855\n",
      "Batch 8, G1 Loss: 1.9249, G2 Loss: 6.3245, D1 Loss: 0.6059, D2 Loss: 0.2648\n",
      "Batch 9, G1 Loss: 1.3804, G2 Loss: 5.0225, D1 Loss: 0.7621, D2 Loss: 0.0729\n",
      "Batch 10, G1 Loss: 2.1406, G2 Loss: 2.8519, D1 Loss: 0.7677, D2 Loss: 0.1581\n",
      "Batch 11, G1 Loss: 0.7958, G2 Loss: 3.1327, D1 Loss: 1.3772, D2 Loss: 0.1482\n",
      "Batch 12, G1 Loss: 2.5304, G2 Loss: 4.8179, D1 Loss: 1.5140, D2 Loss: 0.1591\n",
      "Batch 13, G1 Loss: 0.7299, G2 Loss: 6.7584, D1 Loss: 1.4401, D2 Loss: 0.2394\n",
      "Batch 14, G1 Loss: 2.4168, G2 Loss: 3.4550, D1 Loss: 1.5792, D2 Loss: 0.1515\n",
      "Batch 15, G1 Loss: 1.0771, G2 Loss: 2.0525, D1 Loss: 0.9733, D2 Loss: 0.4803\n",
      "Batch 16, G1 Loss: 1.8354, G2 Loss: 6.9742, D1 Loss: 0.9332, D2 Loss: 0.5404\n",
      "Batch 17, G1 Loss: 1.3421, G2 Loss: 4.3923, D1 Loss: 1.1904, D2 Loss: 0.1556\n",
      "Batch 18, G1 Loss: 1.6026, G2 Loss: 2.4248, D1 Loss: 0.9055, D2 Loss: 0.1709\n",
      "Batch 19, G1 Loss: 1.8144, G2 Loss: 4.3915, D1 Loss: 1.0324, D2 Loss: 0.0564\n",
      "Batch 20, G1 Loss: 1.3749, G2 Loss: 8.9193, D1 Loss: 0.9898, D2 Loss: 0.2693\n",
      "Batch 21, G1 Loss: 1.9602, G2 Loss: 2.0026, D1 Loss: 0.5973, D2 Loss: 0.2659\n",
      "Batch 22, G1 Loss: 2.4794, G2 Loss: 5.5789, D1 Loss: 0.5079, D2 Loss: 0.0795\n",
      "Batch 23, G1 Loss: 1.8404, G2 Loss: 5.3644, D1 Loss: 0.6428, D2 Loss: 0.1572\n",
      "Batch 24, G1 Loss: 2.1806, G2 Loss: 3.7377, D1 Loss: 0.4905, D2 Loss: 0.1551\n",
      "Batch 25, G1 Loss: 1.8784, G2 Loss: 1.9811, D1 Loss: 0.6519, D2 Loss: 0.3240\n",
      "Batch 26, G1 Loss: 1.9471, G2 Loss: 5.7104, D1 Loss: 0.6381, D2 Loss: 0.1212\n",
      "Batch 27, G1 Loss: 1.7281, G2 Loss: 4.9031, D1 Loss: 0.5784, D2 Loss: 0.1522\n",
      "Batch 28, G1 Loss: 1.7987, G2 Loss: 3.3458, D1 Loss: 0.6164, D2 Loss: 0.1763\n",
      "Batch 29, G1 Loss: 2.0867, G2 Loss: 10.6600, D1 Loss: 0.8337, D2 Loss: 0.0733\n",
      "Batch 30, G1 Loss: 1.0874, G2 Loss: 5.7731, D1 Loss: 0.9554, D2 Loss: 0.0968\n",
      "Batch 31, G1 Loss: 2.5871, G2 Loss: 2.8432, D1 Loss: 1.1143, D2 Loss: 0.3493\n",
      "Batch 32, G1 Loss: 1.1007, G2 Loss: 4.5134, D1 Loss: 0.9813, D2 Loss: 0.1423\n",
      "Epoch 79 Average -> G1 Loss: 1.7176623344421387, G2 Loss: 4.555238246917725, D1 Loss: 0.8947808146476746, D2 Loss: 0.1853824108839035\n",
      "Epoch 80/100\n",
      "Batch 1, G1 Loss: 2.6046, G2 Loss: 5.0745, D1 Loss: 0.7304, D2 Loss: 0.0833\n",
      "Batch 2, G1 Loss: 1.9974, G2 Loss: 3.7525, D1 Loss: 0.5899, D2 Loss: 0.2014\n",
      "Batch 3, G1 Loss: 2.1700, G2 Loss: 2.4154, D1 Loss: 0.5412, D2 Loss: 0.2037\n",
      "Batch 4, G1 Loss: 1.8647, G2 Loss: 4.4031, D1 Loss: 0.9946, D2 Loss: 0.3452\n",
      "Batch 5, G1 Loss: 1.1862, G2 Loss: 6.6237, D1 Loss: 0.8866, D2 Loss: 0.0250\n",
      "Batch 6, G1 Loss: 4.3257, G2 Loss: 8.6353, D1 Loss: 1.4881, D2 Loss: 0.0423\n",
      "Batch 7, G1 Loss: 1.4575, G2 Loss: 2.0817, D1 Loss: 0.7895, D2 Loss: 0.4484\n",
      "Batch 8, G1 Loss: 1.9799, G2 Loss: 7.8870, D1 Loss: 0.6681, D2 Loss: 0.0946\n",
      "Batch 9, G1 Loss: 1.9438, G2 Loss: 7.2866, D1 Loss: 0.8191, D2 Loss: 0.2095\n",
      "Batch 10, G1 Loss: 1.8530, G2 Loss: 4.5847, D1 Loss: 1.2000, D2 Loss: 0.1036\n",
      "Batch 11, G1 Loss: 1.7928, G2 Loss: 3.3562, D1 Loss: 0.9670, D2 Loss: 0.1541\n",
      "Batch 12, G1 Loss: 1.3524, G2 Loss: 4.0128, D1 Loss: 0.9791, D2 Loss: 0.1058\n",
      "Batch 13, G1 Loss: 2.9721, G2 Loss: 4.5627, D1 Loss: 0.7616, D2 Loss: 0.0711\n",
      "Batch 14, G1 Loss: 1.5622, G2 Loss: 5.5958, D1 Loss: 0.6849, D2 Loss: 0.0666\n",
      "Batch 15, G1 Loss: 1.6327, G2 Loss: 3.3890, D1 Loss: 1.1143, D2 Loss: 0.1215\n",
      "Batch 16, G1 Loss: 2.5207, G2 Loss: 2.9042, D1 Loss: 0.8828, D2 Loss: 0.2411\n",
      "Batch 17, G1 Loss: 1.0059, G2 Loss: 5.8837, D1 Loss: 1.1536, D2 Loss: 0.2932\n",
      "Batch 18, G1 Loss: 3.1355, G2 Loss: 4.6020, D1 Loss: 1.2455, D2 Loss: 0.1072\n",
      "Batch 19, G1 Loss: 1.2593, G2 Loss: 5.0612, D1 Loss: 1.0514, D2 Loss: 0.0508\n",
      "Batch 20, G1 Loss: 1.1755, G2 Loss: 3.8610, D1 Loss: 0.8911, D2 Loss: 0.1677\n",
      "Batch 21, G1 Loss: 3.2424, G2 Loss: 3.9024, D1 Loss: 1.1312, D2 Loss: 0.0672\n",
      "Batch 22, G1 Loss: 1.0936, G2 Loss: 4.5557, D1 Loss: 1.1116, D2 Loss: 0.2170\n",
      "Batch 23, G1 Loss: 1.6413, G2 Loss: 3.5909, D1 Loss: 0.9241, D2 Loss: 0.1113\n",
      "Batch 24, G1 Loss: 1.7858, G2 Loss: 4.4068, D1 Loss: 0.9929, D2 Loss: 0.1398\n",
      "Batch 25, G1 Loss: 1.1936, G2 Loss: 6.8693, D1 Loss: 0.8694, D2 Loss: 0.0205\n",
      "Batch 26, G1 Loss: 2.5917, G2 Loss: 2.0747, D1 Loss: 0.7519, D2 Loss: 0.4465\n",
      "Batch 27, G1 Loss: 1.7721, G2 Loss: 9.0841, D1 Loss: 0.5844, D2 Loss: 0.6054\n",
      "Batch 28, G1 Loss: 1.6024, G2 Loss: 7.8677, D1 Loss: 0.5931, D2 Loss: 0.0516\n",
      "Batch 29, G1 Loss: 2.3049, G2 Loss: 5.1150, D1 Loss: 0.6030, D2 Loss: 0.0851\n",
      "Batch 30, G1 Loss: 1.1817, G2 Loss: 3.0186, D1 Loss: 0.8982, D2 Loss: 0.2312\n",
      "Batch 31, G1 Loss: 2.5041, G2 Loss: 3.6051, D1 Loss: 0.7641, D2 Loss: 0.1290\n",
      "Batch 32, G1 Loss: 1.3992, G2 Loss: 4.5850, D1 Loss: 0.7081, D2 Loss: 0.4515\n",
      "Epoch 80 Average -> G1 Loss: 1.9407739639282227, G2 Loss: 4.832765579223633, D1 Loss: 0.8865855932235718, D2 Loss: 0.17788231372833252\n",
      "Epoch 81/100\n",
      "Batch 1, G1 Loss: 2.2802, G2 Loss: 4.3094, D1 Loss: 0.6544, D2 Loss: 0.0899\n",
      "Batch 2, G1 Loss: 2.1525, G2 Loss: 3.2842, D1 Loss: 0.6098, D2 Loss: 0.0968\n",
      "Batch 3, G1 Loss: 1.6315, G2 Loss: 4.9019, D1 Loss: 0.5685, D2 Loss: 0.0960\n",
      "Batch 4, G1 Loss: 2.4890, G2 Loss: 3.5149, D1 Loss: 0.5657, D2 Loss: 0.1358\n",
      "Batch 5, G1 Loss: 1.9067, G2 Loss: 3.4695, D1 Loss: 0.5772, D2 Loss: 0.3542\n",
      "Batch 6, G1 Loss: 2.0633, G2 Loss: 2.8498, D1 Loss: 0.6208, D2 Loss: 0.2677\n",
      "Batch 7, G1 Loss: 1.7073, G2 Loss: 5.8156, D1 Loss: 0.6954, D2 Loss: 0.1497\n",
      "Batch 8, G1 Loss: 1.9726, G2 Loss: 5.0327, D1 Loss: 0.6821, D2 Loss: 0.0415\n",
      "Batch 9, G1 Loss: 1.6259, G2 Loss: 3.8254, D1 Loss: 0.7722, D2 Loss: 0.0720\n",
      "Batch 10, G1 Loss: 1.8208, G2 Loss: 3.7196, D1 Loss: 0.8667, D2 Loss: 0.1562\n",
      "Batch 11, G1 Loss: 1.2969, G2 Loss: 4.6019, D1 Loss: 1.3089, D2 Loss: 0.1186\n",
      "Batch 12, G1 Loss: 2.1070, G2 Loss: 6.2905, D1 Loss: 1.3223, D2 Loss: 0.0556\n",
      "Batch 13, G1 Loss: 1.0854, G2 Loss: 2.6362, D1 Loss: 1.1704, D2 Loss: 0.3682\n",
      "Batch 14, G1 Loss: 2.4378, G2 Loss: 6.4081, D1 Loss: 0.8766, D2 Loss: 0.2557\n",
      "Batch 15, G1 Loss: 1.5224, G2 Loss: 6.0974, D1 Loss: 0.8482, D2 Loss: 0.1021\n",
      "Batch 16, G1 Loss: 1.5631, G2 Loss: 3.9720, D1 Loss: 0.9242, D2 Loss: 0.0459\n",
      "Batch 17, G1 Loss: 1.6607, G2 Loss: 3.5133, D1 Loss: 0.9300, D2 Loss: 0.1354\n",
      "Batch 18, G1 Loss: 1.2953, G2 Loss: 3.2095, D1 Loss: 0.9179, D2 Loss: 0.1241\n",
      "Batch 19, G1 Loss: 2.2927, G2 Loss: 4.5018, D1 Loss: 0.7031, D2 Loss: 0.1318\n",
      "Batch 20, G1 Loss: 1.6976, G2 Loss: 4.7658, D1 Loss: 0.7756, D2 Loss: 0.0686\n",
      "Batch 21, G1 Loss: 1.2364, G2 Loss: 4.4799, D1 Loss: 0.9286, D2 Loss: 0.1583\n",
      "Batch 22, G1 Loss: 2.2380, G2 Loss: 4.1467, D1 Loss: 0.6633, D2 Loss: 0.0954\n",
      "Batch 23, G1 Loss: 1.6644, G2 Loss: 1.6293, D1 Loss: 0.8315, D2 Loss: 0.4527\n",
      "Batch 24, G1 Loss: 1.6808, G2 Loss: 9.5434, D1 Loss: 0.6832, D2 Loss: 0.4206\n",
      "Batch 25, G1 Loss: 1.8213, G2 Loss: 8.2263, D1 Loss: 0.7122, D2 Loss: 0.1698\n",
      "Batch 26, G1 Loss: 2.0647, G2 Loss: 6.3013, D1 Loss: 0.7788, D2 Loss: 0.1012\n",
      "Batch 27, G1 Loss: 1.3317, G2 Loss: 4.3919, D1 Loss: 0.8045, D2 Loss: 0.0626\n",
      "Batch 28, G1 Loss: 3.1433, G2 Loss: 2.2732, D1 Loss: 0.7627, D2 Loss: 0.3662\n",
      "Batch 29, G1 Loss: 1.0453, G2 Loss: 5.8792, D1 Loss: 0.9863, D2 Loss: 0.2653\n",
      "Batch 30, G1 Loss: 2.7474, G2 Loss: 5.6721, D1 Loss: 0.7496, D2 Loss: 0.0627\n",
      "Batch 31, G1 Loss: 1.9120, G2 Loss: 4.8228, D1 Loss: 0.4507, D2 Loss: 0.0988\n",
      "Batch 32, G1 Loss: 2.8308, G2 Loss: 2.2234, D1 Loss: 0.4858, D2 Loss: 0.2320\n",
      "Epoch 81 Average -> G1 Loss: 1.8851498365402222, G2 Loss: 4.572150230407715, D1 Loss: 0.788358747959137, D2 Loss: 0.16723552346229553\n",
      "Epoch 82/100\n",
      "Batch 1, G1 Loss: 1.7701, G2 Loss: 4.6148, D1 Loss: 0.5209, D2 Loss: 0.2622\n",
      "Batch 2, G1 Loss: 1.8929, G2 Loss: 4.7818, D1 Loss: 0.4843, D2 Loss: 0.1711\n",
      "Batch 3, G1 Loss: 2.5383, G2 Loss: 2.4406, D1 Loss: 0.9411, D2 Loss: 0.3686\n",
      "Batch 4, G1 Loss: 1.2679, G2 Loss: 5.0210, D1 Loss: 0.8747, D2 Loss: 0.4045\n",
      "Batch 5, G1 Loss: 3.0828, G2 Loss: 3.9198, D1 Loss: 0.4862, D2 Loss: 0.1686\n",
      "Batch 6, G1 Loss: 2.1647, G2 Loss: 4.2620, D1 Loss: 0.6830, D2 Loss: 0.0869\n",
      "Batch 7, G1 Loss: 1.0750, G2 Loss: 3.5845, D1 Loss: 0.9796, D2 Loss: 0.1276\n",
      "Batch 8, G1 Loss: 3.7474, G2 Loss: 1.7554, D1 Loss: 1.0701, D2 Loss: 0.4766\n",
      "Batch 9, G1 Loss: 1.4035, G2 Loss: 9.9830, D1 Loss: 1.0002, D2 Loss: 0.3503\n",
      "Batch 10, G1 Loss: 1.7929, G2 Loss: 8.5617, D1 Loss: 0.8968, D2 Loss: 0.1967\n",
      "Batch 11, G1 Loss: 1.9216, G2 Loss: 5.5420, D1 Loss: 0.7291, D2 Loss: 0.0797\n",
      "Batch 12, G1 Loss: 2.5910, G2 Loss: 3.0249, D1 Loss: 0.7337, D2 Loss: 0.2776\n",
      "Batch 13, G1 Loss: 1.6783, G2 Loss: 6.0723, D1 Loss: 0.8143, D2 Loss: 0.0677\n",
      "Batch 14, G1 Loss: 1.5934, G2 Loss: 6.7156, D1 Loss: 0.6120, D2 Loss: 0.1722\n",
      "Batch 15, G1 Loss: 3.2093, G2 Loss: 6.8721, D1 Loss: 0.5261, D2 Loss: 0.0219\n",
      "Batch 16, G1 Loss: 1.2927, G2 Loss: 4.5913, D1 Loss: 0.8680, D2 Loss: 0.0360\n",
      "Batch 17, G1 Loss: 2.4180, G2 Loss: 2.5709, D1 Loss: 0.8669, D2 Loss: 0.1925\n",
      "Batch 18, G1 Loss: 0.9563, G2 Loss: 4.7065, D1 Loss: 1.2347, D2 Loss: 0.0358\n",
      "Batch 19, G1 Loss: 2.1139, G2 Loss: 5.3933, D1 Loss: 1.0932, D2 Loss: 0.1138\n",
      "Batch 20, G1 Loss: 1.3716, G2 Loss: 4.1040, D1 Loss: 1.1823, D2 Loss: 0.0673\n",
      "Batch 21, G1 Loss: 1.6868, G2 Loss: 3.4494, D1 Loss: 0.9884, D2 Loss: 0.0868\n",
      "Batch 22, G1 Loss: 1.6345, G2 Loss: 4.0690, D1 Loss: 0.7625, D2 Loss: 0.2301\n",
      "Batch 23, G1 Loss: 2.0795, G2 Loss: 7.0377, D1 Loss: 0.4992, D2 Loss: 0.1121\n",
      "Batch 24, G1 Loss: 2.0495, G2 Loss: 8.5414, D1 Loss: 0.4786, D2 Loss: 0.0622\n",
      "Batch 25, G1 Loss: 2.2919, G2 Loss: 1.2657, D1 Loss: 0.7201, D2 Loss: 0.7125\n",
      "Batch 26, G1 Loss: 0.8726, G2 Loss: 12.8426, D1 Loss: 1.0270, D2 Loss: 0.5405\n",
      "Batch 27, G1 Loss: 4.2988, G2 Loss: 13.5212, D1 Loss: 1.4257, D2 Loss: 0.2897\n",
      "Batch 28, G1 Loss: 1.7182, G2 Loss: 11.8485, D1 Loss: 0.5923, D2 Loss: 0.0642\n",
      "Batch 29, G1 Loss: 0.9024, G2 Loss: 5.1300, D1 Loss: 1.1501, D2 Loss: 0.0272\n",
      "Batch 30, G1 Loss: 4.0452, G2 Loss: 3.8608, D1 Loss: 1.3622, D2 Loss: 0.1045\n",
      "Batch 31, G1 Loss: 1.6016, G2 Loss: 3.7794, D1 Loss: 0.8081, D2 Loss: 0.1026\n",
      "Batch 32, G1 Loss: 1.2998, G2 Loss: 4.7784, D1 Loss: 1.0129, D2 Loss: 0.0777\n",
      "Epoch 82 Average -> G1 Loss: 2.011326551437378, G2 Loss: 5.582549095153809, D1 Loss: 0.8570104837417603, D2 Loss: 0.1902344524860382\n",
      "Epoch 83/100\n",
      "Batch 1, G1 Loss: 3.5837, G2 Loss: 5.0099, D1 Loss: 0.6812, D2 Loss: 0.1252\n",
      "Batch 2, G1 Loss: 1.8049, G2 Loss: 2.7236, D1 Loss: 0.8060, D2 Loss: 0.2073\n",
      "Batch 3, G1 Loss: 1.7123, G2 Loss: 5.4475, D1 Loss: 0.6954, D2 Loss: 0.1479\n",
      "Batch 4, G1 Loss: 2.5645, G2 Loss: 4.6348, D1 Loss: 0.8150, D2 Loss: 0.1223\n",
      "Batch 5, G1 Loss: 1.4618, G2 Loss: 4.5503, D1 Loss: 0.7453, D2 Loss: 0.0662\n",
      "Batch 6, G1 Loss: 2.5893, G2 Loss: 5.8594, D1 Loss: 0.7073, D2 Loss: 0.0343\n",
      "Batch 7, G1 Loss: 1.8481, G2 Loss: 1.8081, D1 Loss: 0.6413, D2 Loss: 0.4668\n",
      "Batch 8, G1 Loss: 1.5876, G2 Loss: 8.2907, D1 Loss: 0.7271, D2 Loss: 0.6114\n",
      "Batch 9, G1 Loss: 1.7741, G2 Loss: 7.6960, D1 Loss: 0.7588, D2 Loss: 0.1498\n",
      "Batch 10, G1 Loss: 2.5389, G2 Loss: 4.1638, D1 Loss: 0.7393, D2 Loss: 0.1251\n",
      "Batch 11, G1 Loss: 0.9542, G2 Loss: 2.7676, D1 Loss: 1.0999, D2 Loss: 0.3552\n",
      "Batch 12, G1 Loss: 3.0445, G2 Loss: 4.3031, D1 Loss: 1.1184, D2 Loss: 0.0688\n",
      "Batch 13, G1 Loss: 1.3479, G2 Loss: 3.8187, D1 Loss: 0.8033, D2 Loss: 0.0866\n",
      "Batch 14, G1 Loss: 0.9780, G2 Loss: 4.1314, D1 Loss: 1.1399, D2 Loss: 0.1518\n",
      "Batch 15, G1 Loss: 3.4272, G2 Loss: 2.8002, D1 Loss: 1.9621, D2 Loss: 0.1743\n",
      "Batch 16, G1 Loss: 0.9071, G2 Loss: 8.6216, D1 Loss: 1.4067, D2 Loss: 0.0162\n",
      "Batch 17, G1 Loss: 2.2478, G2 Loss: 10.1773, D1 Loss: 0.8488, D2 Loss: 0.1676\n",
      "Batch 18, G1 Loss: 1.5099, G2 Loss: 4.8586, D1 Loss: 0.9635, D2 Loss: 0.1195\n",
      "Batch 19, G1 Loss: 1.8758, G2 Loss: 4.8289, D1 Loss: 0.6404, D2 Loss: 0.1337\n",
      "Batch 20, G1 Loss: 1.0527, G2 Loss: 2.7167, D1 Loss: 0.9592, D2 Loss: 0.2829\n",
      "Batch 21, G1 Loss: 3.0842, G2 Loss: 5.7723, D1 Loss: 0.8764, D2 Loss: 0.0328\n",
      "Batch 22, G1 Loss: 0.9349, G2 Loss: 6.0972, D1 Loss: 1.2759, D2 Loss: 0.2105\n",
      "Batch 23, G1 Loss: 2.0977, G2 Loss: 3.5143, D1 Loss: 0.7936, D2 Loss: 0.1228\n",
      "Batch 24, G1 Loss: 1.2069, G2 Loss: 3.7900, D1 Loss: 1.0584, D2 Loss: 0.1215\n",
      "Batch 25, G1 Loss: 1.5011, G2 Loss: 4.0759, D1 Loss: 0.9845, D2 Loss: 0.0930\n",
      "Batch 26, G1 Loss: 1.2676, G2 Loss: 4.0032, D1 Loss: 0.8825, D2 Loss: 0.0903\n",
      "Batch 27, G1 Loss: 2.1364, G2 Loss: 7.6966, D1 Loss: 0.6917, D2 Loss: 0.1281\n",
      "Batch 28, G1 Loss: 1.8843, G2 Loss: 3.3913, D1 Loss: 0.8633, D2 Loss: 0.1617\n",
      "Batch 29, G1 Loss: 1.2206, G2 Loss: 4.0236, D1 Loss: 0.8162, D2 Loss: 0.0941\n",
      "Batch 30, G1 Loss: 1.9176, G2 Loss: 4.9484, D1 Loss: 0.5100, D2 Loss: 0.1287\n",
      "Batch 31, G1 Loss: 2.4232, G2 Loss: 6.8929, D1 Loss: 0.9654, D2 Loss: 0.0222\n",
      "Batch 32, G1 Loss: 1.2348, G2 Loss: 3.1594, D1 Loss: 1.0301, D2 Loss: 0.1919\n",
      "Epoch 83 Average -> G1 Loss: 1.8662383556365967, G2 Loss: 4.892910003662109, D1 Loss: 0.9064571261405945, D2 Loss: 0.15657556056976318\n",
      "Epoch 84/100\n",
      "Batch 1, G1 Loss: 2.0388, G2 Loss: 6.2120, D1 Loss: 0.7457, D2 Loss: 0.0913\n",
      "Batch 2, G1 Loss: 2.2464, G2 Loss: 4.9703, D1 Loss: 0.8751, D2 Loss: 0.2368\n",
      "Batch 3, G1 Loss: 1.4748, G2 Loss: 2.9236, D1 Loss: 0.6763, D2 Loss: 0.2180\n",
      "Batch 4, G1 Loss: 2.9143, G2 Loss: 3.7548, D1 Loss: 0.6162, D2 Loss: 0.1792\n",
      "Batch 5, G1 Loss: 2.3293, G2 Loss: 4.8236, D1 Loss: 0.4834, D2 Loss: 0.0701\n",
      "Batch 6, G1 Loss: 1.0550, G2 Loss: 7.8744, D1 Loss: 1.0613, D2 Loss: 0.0438\n",
      "Batch 7, G1 Loss: 3.9013, G2 Loss: 5.0188, D1 Loss: 1.0626, D2 Loss: 0.0584\n",
      "Batch 8, G1 Loss: 0.7759, G2 Loss: 6.2277, D1 Loss: 1.1908, D2 Loss: 0.0389\n",
      "Batch 9, G1 Loss: 3.5669, G2 Loss: 2.1132, D1 Loss: 1.3927, D2 Loss: 0.3860\n",
      "Batch 10, G1 Loss: 0.8628, G2 Loss: 7.8532, D1 Loss: 1.3838, D2 Loss: 0.4521\n",
      "Batch 11, G1 Loss: 2.8481, G2 Loss: 6.3887, D1 Loss: 0.9984, D2 Loss: 0.0518\n",
      "Batch 12, G1 Loss: 0.9814, G2 Loss: 3.3894, D1 Loss: 1.0474, D2 Loss: 0.1615\n",
      "Batch 13, G1 Loss: 2.3454, G2 Loss: 2.6828, D1 Loss: 0.8699, D2 Loss: 0.2116\n",
      "Batch 14, G1 Loss: 1.5583, G2 Loss: 4.6945, D1 Loss: 0.8847, D2 Loss: 0.1074\n",
      "Batch 15, G1 Loss: 1.7041, G2 Loss: 4.9819, D1 Loss: 0.6288, D2 Loss: 0.0779\n",
      "Batch 16, G1 Loss: 2.8107, G2 Loss: 5.0035, D1 Loss: 0.7600, D2 Loss: 0.0780\n",
      "Batch 17, G1 Loss: 1.2351, G2 Loss: 3.9943, D1 Loss: 0.6409, D2 Loss: 0.1845\n",
      "Batch 18, G1 Loss: 2.5598, G2 Loss: 2.4663, D1 Loss: 0.4082, D2 Loss: 0.2297\n",
      "Batch 19, G1 Loss: 3.0209, G2 Loss: 5.1282, D1 Loss: 0.4286, D2 Loss: 0.0746\n",
      "Batch 20, G1 Loss: 2.0106, G2 Loss: 5.3832, D1 Loss: 0.3647, D2 Loss: 0.0780\n",
      "Batch 21, G1 Loss: 1.6592, G2 Loss: 3.5520, D1 Loss: 0.4888, D2 Loss: 0.1401\n",
      "Batch 22, G1 Loss: 2.9946, G2 Loss: 3.0296, D1 Loss: 0.4762, D2 Loss: 0.3090\n",
      "Batch 23, G1 Loss: 2.5194, G2 Loss: 3.4370, D1 Loss: 0.4150, D2 Loss: 0.1035\n",
      "Batch 24, G1 Loss: 1.5111, G2 Loss: 8.7156, D1 Loss: 0.6682, D2 Loss: 0.1514\n",
      "Batch 25, G1 Loss: 3.1407, G2 Loss: 2.3614, D1 Loss: 0.6921, D2 Loss: 0.3342\n",
      "Batch 26, G1 Loss: 1.3480, G2 Loss: 5.5318, D1 Loss: 1.0991, D2 Loss: 0.0874\n",
      "Batch 27, G1 Loss: 2.7360, G2 Loss: 5.5725, D1 Loss: 0.8304, D2 Loss: 0.0367\n",
      "Batch 28, G1 Loss: 1.5411, G2 Loss: 4.0977, D1 Loss: 0.9533, D2 Loss: 0.1420\n",
      "Batch 29, G1 Loss: 2.9318, G2 Loss: 4.8208, D1 Loss: 0.8956, D2 Loss: 0.1206\n",
      "Batch 30, G1 Loss: 1.5391, G2 Loss: 4.6933, D1 Loss: 0.9463, D2 Loss: 0.1675\n",
      "Batch 31, G1 Loss: 1.3120, G2 Loss: 6.1286, D1 Loss: 0.9976, D2 Loss: 0.0331\n",
      "Batch 32, G1 Loss: 2.5432, G2 Loss: 3.1314, D1 Loss: 0.6316, D2 Loss: 0.1534\n",
      "Epoch 84 Average -> G1 Loss: 2.1255075931549072, G2 Loss: 4.717372894287109, D1 Loss: 0.8004232048988342, D2 Loss: 0.1502673625946045\n",
      "Epoch 85/100\n",
      "Batch 1, G1 Loss: 1.4634, G2 Loss: 4.2738, D1 Loss: 0.8652, D2 Loss: 0.1117\n",
      "Batch 2, G1 Loss: 1.5604, G2 Loss: 5.3213, D1 Loss: 0.8111, D2 Loss: 0.1202\n",
      "Batch 3, G1 Loss: 1.7271, G2 Loss: 3.7847, D1 Loss: 1.0955, D2 Loss: 0.1418\n",
      "Batch 4, G1 Loss: 1.8209, G2 Loss: 4.1663, D1 Loss: 1.1193, D2 Loss: 0.0526\n",
      "Batch 5, G1 Loss: 0.9513, G2 Loss: 3.9862, D1 Loss: 1.1770, D2 Loss: 0.1346\n",
      "Batch 6, G1 Loss: 3.6120, G2 Loss: 5.4643, D1 Loss: 2.2445, D2 Loss: 0.1126\n",
      "Batch 7, G1 Loss: 0.4850, G2 Loss: 8.1772, D1 Loss: 2.1142, D2 Loss: 0.1329\n",
      "Batch 8, G1 Loss: 2.9143, G2 Loss: 2.1073, D1 Loss: 0.9573, D2 Loss: 0.3657\n",
      "Batch 9, G1 Loss: 1.6962, G2 Loss: 7.8585, D1 Loss: 0.6743, D2 Loss: 0.3308\n",
      "Batch 10, G1 Loss: 1.6276, G2 Loss: 6.7679, D1 Loss: 0.7570, D2 Loss: 0.0633\n",
      "Batch 11, G1 Loss: 1.3537, G2 Loss: 4.8308, D1 Loss: 0.6835, D2 Loss: 0.0622\n",
      "Batch 12, G1 Loss: 2.4396, G2 Loss: 3.3451, D1 Loss: 0.7984, D2 Loss: 0.1684\n",
      "Batch 13, G1 Loss: 1.0426, G2 Loss: 4.7881, D1 Loss: 0.9595, D2 Loss: 0.1125\n",
      "Batch 14, G1 Loss: 1.9307, G2 Loss: 6.1397, D1 Loss: 0.8288, D2 Loss: 0.1725\n",
      "Batch 15, G1 Loss: 1.5696, G2 Loss: 1.7400, D1 Loss: 1.1409, D2 Loss: 0.4264\n",
      "Batch 16, G1 Loss: 1.2465, G2 Loss: 11.5050, D1 Loss: 0.9268, D2 Loss: 0.8867\n",
      "Batch 17, G1 Loss: 1.8379, G2 Loss: 6.6924, D1 Loss: 0.7698, D2 Loss: 0.0908\n",
      "Batch 18, G1 Loss: 1.6992, G2 Loss: 4.0821, D1 Loss: 0.7035, D2 Loss: 0.0641\n",
      "Batch 19, G1 Loss: 1.5491, G2 Loss: 3.4329, D1 Loss: 0.7494, D2 Loss: 0.1140\n",
      "Batch 20, G1 Loss: 2.0460, G2 Loss: 2.5905, D1 Loss: 0.6763, D2 Loss: 0.2600\n",
      "Batch 21, G1 Loss: 1.4284, G2 Loss: 3.6211, D1 Loss: 0.6399, D2 Loss: 0.1428\n",
      "Batch 22, G1 Loss: 2.1486, G2 Loss: 3.7897, D1 Loss: 0.7094, D2 Loss: 0.0962\n",
      "Batch 23, G1 Loss: 1.6056, G2 Loss: 5.2094, D1 Loss: 0.7620, D2 Loss: 0.1170\n",
      "Batch 24, G1 Loss: 1.3029, G2 Loss: 8.7272, D1 Loss: 0.7837, D2 Loss: 0.0049\n",
      "Batch 25, G1 Loss: 2.1947, G2 Loss: 3.0575, D1 Loss: 0.6431, D2 Loss: 0.1253\n",
      "Batch 26, G1 Loss: 1.3656, G2 Loss: 5.1159, D1 Loss: 0.8089, D2 Loss: 0.0327\n",
      "Batch 27, G1 Loss: 2.0886, G2 Loss: 5.4047, D1 Loss: 0.6881, D2 Loss: 0.1625\n",
      "Batch 28, G1 Loss: 1.6525, G2 Loss: 2.2871, D1 Loss: 0.6290, D2 Loss: 0.2202\n",
      "Batch 29, G1 Loss: 1.5533, G2 Loss: 6.4489, D1 Loss: 0.9086, D2 Loss: 0.0266\n",
      "Batch 30, G1 Loss: 1.8496, G2 Loss: 5.9319, D1 Loss: 0.8433, D2 Loss: 0.2937\n",
      "Batch 31, G1 Loss: 1.2410, G2 Loss: 3.4013, D1 Loss: 0.9488, D2 Loss: 0.1087\n",
      "Batch 32, G1 Loss: 2.1266, G2 Loss: 2.6283, D1 Loss: 0.8468, D2 Loss: 0.1876\n",
      "Epoch 85 Average -> G1 Loss: 1.7228236198425293, G2 Loss: 4.896151542663574, D1 Loss: 0.9145028591156006, D2 Loss: 0.17006388306617737\n",
      "Epoch 86/100\n",
      "Batch 1, G1 Loss: 1.7661, G2 Loss: 5.5982, D1 Loss: 0.6877, D2 Loss: 0.0934\n",
      "Batch 2, G1 Loss: 1.1755, G2 Loss: 7.8515, D1 Loss: 1.0383, D2 Loss: 0.1364\n",
      "Batch 3, G1 Loss: 2.4887, G2 Loss: 6.0071, D1 Loss: 1.0776, D2 Loss: 0.0260\n",
      "Batch 4, G1 Loss: 0.9019, G2 Loss: 2.7742, D1 Loss: 1.0861, D2 Loss: 0.1815\n",
      "Batch 5, G1 Loss: 2.1899, G2 Loss: 5.3455, D1 Loss: 0.7758, D2 Loss: 0.0540\n",
      "Batch 6, G1 Loss: 1.4851, G2 Loss: 5.6560, D1 Loss: 1.0095, D2 Loss: 0.1766\n",
      "Batch 7, G1 Loss: 1.5977, G2 Loss: 2.8381, D1 Loss: 0.9074, D2 Loss: 0.1221\n",
      "Batch 8, G1 Loss: 1.6270, G2 Loss: 4.1216, D1 Loss: 0.8087, D2 Loss: 0.1568\n",
      "Batch 9, G1 Loss: 1.7284, G2 Loss: 4.3984, D1 Loss: 0.9820, D2 Loss: 0.1455\n",
      "Batch 10, G1 Loss: 1.1840, G2 Loss: 7.4748, D1 Loss: 0.8073, D2 Loss: 0.0366\n",
      "Batch 11, G1 Loss: 2.5665, G2 Loss: 1.9629, D1 Loss: 0.6586, D2 Loss: 0.2961\n",
      "Batch 12, G1 Loss: 1.5059, G2 Loss: 7.9046, D1 Loss: 0.7326, D2 Loss: 0.2856\n",
      "Batch 13, G1 Loss: 1.8561, G2 Loss: 6.3248, D1 Loss: 0.6936, D2 Loss: 0.0666\n",
      "Batch 14, G1 Loss: 2.3531, G2 Loss: 4.6314, D1 Loss: 0.6206, D2 Loss: 0.0422\n",
      "Batch 15, G1 Loss: 2.2486, G2 Loss: 3.5007, D1 Loss: 0.4779, D2 Loss: 0.1210\n",
      "Batch 16, G1 Loss: 1.1672, G2 Loss: 5.0868, D1 Loss: 1.1747, D2 Loss: 0.0350\n",
      "Batch 17, G1 Loss: 3.4682, G2 Loss: 7.3687, D1 Loss: 0.6465, D2 Loss: 0.0312\n",
      "Batch 18, G1 Loss: 1.9682, G2 Loss: 3.2667, D1 Loss: 0.5593, D2 Loss: 0.1517\n",
      "Batch 19, G1 Loss: 1.1522, G2 Loss: 4.1587, D1 Loss: 0.9217, D2 Loss: 0.1290\n",
      "Batch 20, G1 Loss: 2.2618, G2 Loss: 3.2980, D1 Loss: 0.6952, D2 Loss: 0.1176\n",
      "Batch 21, G1 Loss: 2.4066, G2 Loss: 5.3773, D1 Loss: 0.6538, D2 Loss: 0.2312\n",
      "Batch 22, G1 Loss: 1.2609, G2 Loss: 4.9322, D1 Loss: 0.9009, D2 Loss: 0.0371\n",
      "Batch 23, G1 Loss: 2.1990, G2 Loss: 3.9574, D1 Loss: 0.5682, D2 Loss: 0.0772\n",
      "Batch 24, G1 Loss: 2.3377, G2 Loss: 3.0317, D1 Loss: 0.6778, D2 Loss: 0.1723\n",
      "Batch 25, G1 Loss: 1.2953, G2 Loss: 5.7808, D1 Loss: 0.6790, D2 Loss: 0.2703\n",
      "Batch 26, G1 Loss: 3.0378, G2 Loss: 3.8724, D1 Loss: 0.8848, D2 Loss: 0.1068\n",
      "Batch 27, G1 Loss: 1.1986, G2 Loss: 3.6637, D1 Loss: 0.8183, D2 Loss: 0.1198\n",
      "Batch 28, G1 Loss: 2.0589, G2 Loss: 3.1145, D1 Loss: 0.6659, D2 Loss: 0.1204\n",
      "Batch 29, G1 Loss: 1.5338, G2 Loss: 5.7645, D1 Loss: 0.9730, D2 Loss: 0.0944\n",
      "Batch 30, G1 Loss: 1.3321, G2 Loss: 4.4377, D1 Loss: 1.3720, D2 Loss: 0.2386\n",
      "Batch 31, G1 Loss: 1.4823, G2 Loss: 3.7010, D1 Loss: 1.3111, D2 Loss: 0.0838\n",
      "Batch 32, G1 Loss: 1.3349, G2 Loss: 5.1442, D1 Loss: 1.1107, D2 Loss: 0.0628\n",
      "Epoch 86 Average -> G1 Loss: 1.8178110122680664, G2 Loss: 4.760819911956787, D1 Loss: 0.8430199027061462, D2 Loss: 0.1256144493818283\n",
      "Epoch 87/100\n",
      "Batch 1, G1 Loss: 1.9820, G2 Loss: 3.9898, D1 Loss: 0.7709, D2 Loss: 0.0671\n",
      "Batch 2, G1 Loss: 1.4959, G2 Loss: 3.1380, D1 Loss: 0.8588, D2 Loss: 0.1214\n",
      "Batch 3, G1 Loss: 1.6118, G2 Loss: 4.4984, D1 Loss: 0.9141, D2 Loss: 0.0661\n",
      "Batch 4, G1 Loss: 1.6909, G2 Loss: 4.8691, D1 Loss: 0.7217, D2 Loss: 0.0802\n",
      "Batch 5, G1 Loss: 1.4643, G2 Loss: 4.3041, D1 Loss: 1.1397, D2 Loss: 0.1468\n",
      "Batch 6, G1 Loss: 1.6295, G2 Loss: 4.0184, D1 Loss: 1.2300, D2 Loss: 0.1029\n",
      "Batch 7, G1 Loss: 1.3209, G2 Loss: 1.3318, D1 Loss: 1.0163, D2 Loss: 0.8414\n",
      "Batch 8, G1 Loss: 2.1070, G2 Loss: 17.6327, D1 Loss: 0.7035, D2 Loss: 1.7720\n",
      "Batch 9, G1 Loss: 1.2169, G2 Loss: 12.7315, D1 Loss: 0.8577, D2 Loss: 0.1329\n",
      "Batch 10, G1 Loss: 1.8308, G2 Loss: 6.5083, D1 Loss: 0.8728, D2 Loss: 0.0827\n",
      "Batch 11, G1 Loss: 1.5965, G2 Loss: 3.0677, D1 Loss: 0.9302, D2 Loss: 0.1099\n",
      "Batch 12, G1 Loss: 1.3013, G2 Loss: 2.5225, D1 Loss: 1.0286, D2 Loss: 0.2146\n",
      "Batch 13, G1 Loss: 1.7493, G2 Loss: 5.5559, D1 Loss: 1.0426, D2 Loss: 0.0574\n",
      "Batch 14, G1 Loss: 1.3244, G2 Loss: 4.5927, D1 Loss: 1.0930, D2 Loss: 0.2080\n",
      "Batch 15, G1 Loss: 1.1546, G2 Loss: 1.8972, D1 Loss: 0.9751, D2 Loss: 0.2589\n",
      "Batch 16, G1 Loss: 2.7758, G2 Loss: 6.3430, D1 Loss: 0.7460, D2 Loss: 0.1626\n",
      "Batch 17, G1 Loss: 1.2331, G2 Loss: 5.6920, D1 Loss: 0.8252, D2 Loss: 0.0181\n",
      "Batch 18, G1 Loss: 2.7455, G2 Loss: 2.5955, D1 Loss: 0.7079, D2 Loss: 0.1349\n",
      "Batch 19, G1 Loss: 1.5701, G2 Loss: 5.0498, D1 Loss: 0.6182, D2 Loss: 0.1278\n",
      "Batch 20, G1 Loss: 1.9229, G2 Loss: 4.7097, D1 Loss: 0.7403, D2 Loss: 0.0690\n",
      "Batch 21, G1 Loss: 1.8603, G2 Loss: 1.5903, D1 Loss: 0.5322, D2 Loss: 0.5120\n",
      "Batch 22, G1 Loss: 1.7525, G2 Loss: 12.8388, D1 Loss: 0.8409, D2 Loss: 0.7833\n",
      "Batch 23, G1 Loss: 1.7885, G2 Loss: 9.6345, D1 Loss: 0.9243, D2 Loss: 0.0941\n",
      "Batch 24, G1 Loss: 1.4041, G2 Loss: 4.6648, D1 Loss: 0.9859, D2 Loss: 0.0280\n",
      "Batch 25, G1 Loss: 2.2157, G2 Loss: 3.1204, D1 Loss: 0.7434, D2 Loss: 0.0911\n",
      "Batch 26, G1 Loss: 1.4191, G2 Loss: 3.9707, D1 Loss: 0.9158, D2 Loss: 0.0306\n",
      "Batch 27, G1 Loss: 1.7134, G2 Loss: 2.9509, D1 Loss: 1.0608, D2 Loss: 0.1837\n",
      "Batch 28, G1 Loss: 1.5674, G2 Loss: 6.4947, D1 Loss: 0.7956, D2 Loss: 0.1048\n",
      "Batch 29, G1 Loss: 2.1938, G2 Loss: 5.2965, D1 Loss: 0.8401, D2 Loss: 0.0459\n",
      "Batch 30, G1 Loss: 1.3689, G2 Loss: 3.0695, D1 Loss: 0.8054, D2 Loss: 0.1210\n",
      "Batch 31, G1 Loss: 2.0589, G2 Loss: 5.1255, D1 Loss: 0.6919, D2 Loss: 0.1061\n",
      "Batch 32, G1 Loss: 1.7925, G2 Loss: 4.5298, D1 Loss: 0.7283, D2 Loss: 0.2021\n",
      "Epoch 87 Average -> G1 Loss: 1.7143306732177734, G2 Loss: 5.260451316833496, D1 Loss: 0.8642926216125488, D2 Loss: 0.22116400301456451\n",
      "Epoch 88/100\n",
      "Batch 1, G1 Loss: 1.5099, G2 Loss: 2.8801, D1 Loss: 0.6590, D2 Loss: 0.1474\n",
      "Batch 2, G1 Loss: 1.9630, G2 Loss: 5.9619, D1 Loss: 0.6137, D2 Loss: 0.1961\n",
      "Batch 3, G1 Loss: 2.2267, G2 Loss: 4.4071, D1 Loss: 0.6442, D2 Loss: 0.0416\n",
      "Batch 4, G1 Loss: 1.5461, G2 Loss: 3.8228, D1 Loss: 0.8748, D2 Loss: 0.1196\n",
      "Batch 5, G1 Loss: 2.2913, G2 Loss: 3.7927, D1 Loss: 0.6650, D2 Loss: 0.1551\n",
      "Batch 6, G1 Loss: 2.5431, G2 Loss: 4.9642, D1 Loss: 0.4078, D2 Loss: 0.0257\n",
      "Batch 7, G1 Loss: 1.3629, G2 Loss: 4.4491, D1 Loss: 0.6361, D2 Loss: 0.1026\n",
      "Batch 8, G1 Loss: 3.6340, G2 Loss: 4.3754, D1 Loss: 0.9126, D2 Loss: 0.0775\n",
      "Batch 9, G1 Loss: 1.4474, G2 Loss: 3.7258, D1 Loss: 0.6148, D2 Loss: 0.0747\n",
      "Batch 10, G1 Loss: 2.7342, G2 Loss: 5.4652, D1 Loss: 0.6566, D2 Loss: 0.3717\n",
      "Batch 11, G1 Loss: 1.7955, G2 Loss: 2.2109, D1 Loss: 0.4822, D2 Loss: 0.3715\n",
      "Batch 12, G1 Loss: 2.5068, G2 Loss: 10.2977, D1 Loss: 0.3977, D2 Loss: 0.3775\n",
      "Batch 13, G1 Loss: 1.7511, G2 Loss: 7.7562, D1 Loss: 0.6428, D2 Loss: 0.0581\n",
      "Batch 14, G1 Loss: 2.0030, G2 Loss: 4.9147, D1 Loss: 0.6209, D2 Loss: 0.0289\n",
      "Batch 15, G1 Loss: 2.3430, G2 Loss: 3.0954, D1 Loss: 0.5647, D2 Loss: 0.1566\n",
      "Batch 16, G1 Loss: 1.5110, G2 Loss: 3.8630, D1 Loss: 0.9244, D2 Loss: 0.0793\n",
      "Batch 17, G1 Loss: 2.0129, G2 Loss: 5.8538, D1 Loss: 0.8558, D2 Loss: 0.1818\n",
      "Batch 18, G1 Loss: 1.4110, G2 Loss: 4.5082, D1 Loss: 0.8313, D2 Loss: 0.3479\n",
      "Batch 19, G1 Loss: 2.1250, G2 Loss: 1.6391, D1 Loss: 0.7046, D2 Loss: 0.5816\n",
      "Batch 20, G1 Loss: 1.9078, G2 Loss: 12.3181, D1 Loss: 0.6648, D2 Loss: 0.4721\n",
      "Batch 21, G1 Loss: 1.5881, G2 Loss: 11.8108, D1 Loss: 0.9956, D2 Loss: 0.3199\n",
      "Batch 22, G1 Loss: 1.7264, G2 Loss: 6.6537, D1 Loss: 1.2802, D2 Loss: 0.2227\n",
      "Batch 23, G1 Loss: 1.6770, G2 Loss: 1.2835, D1 Loss: 0.7379, D2 Loss: 0.5672\n",
      "Batch 24, G1 Loss: 2.0265, G2 Loss: 10.4332, D1 Loss: 0.8278, D2 Loss: 0.1598\n",
      "Batch 25, G1 Loss: 1.2531, G2 Loss: 11.4004, D1 Loss: 0.9191, D2 Loss: 0.2709\n",
      "Batch 26, G1 Loss: 2.4099, G2 Loss: 9.5441, D1 Loss: 0.7217, D2 Loss: 0.0165\n",
      "Batch 27, G1 Loss: 1.2156, G2 Loss: 6.8434, D1 Loss: 0.9800, D2 Loss: 0.0294\n",
      "Batch 28, G1 Loss: 1.6580, G2 Loss: 3.5929, D1 Loss: 0.8469, D2 Loss: 0.0575\n",
      "Batch 29, G1 Loss: 1.5075, G2 Loss: 3.0836, D1 Loss: 0.8061, D2 Loss: 0.1520\n",
      "Batch 30, G1 Loss: 1.3647, G2 Loss: 5.2584, D1 Loss: 1.0960, D2 Loss: 0.1556\n",
      "Batch 31, G1 Loss: 1.5614, G2 Loss: 11.6758, D1 Loss: 0.8686, D2 Loss: 0.0816\n",
      "Batch 32, G1 Loss: 1.3192, G2 Loss: 4.4619, D1 Loss: 1.1036, D2 Loss: 0.0707\n",
      "Epoch 88 Average -> G1 Loss: 1.8729051351547241, G2 Loss: 5.823230266571045, D1 Loss: 0.7674199342727661, D2 Loss: 0.1897331178188324\n",
      "Epoch 89/100\n",
      "Batch 1, G1 Loss: 1.2359, G2 Loss: 2.2765, D1 Loss: 0.9976, D2 Loss: 0.2688\n",
      "Batch 2, G1 Loss: 2.8049, G2 Loss: 7.4605, D1 Loss: 1.1350, D2 Loss: 0.1428\n",
      "Batch 3, G1 Loss: 0.6421, G2 Loss: 7.1621, D1 Loss: 1.3819, D2 Loss: 0.2097\n",
      "Batch 4, G1 Loss: 4.8058, G2 Loss: 4.4733, D1 Loss: 0.7153, D2 Loss: 0.0300\n",
      "Batch 5, G1 Loss: 4.1418, G2 Loss: 2.4707, D1 Loss: 0.6043, D2 Loss: 0.2135\n",
      "Batch 6, G1 Loss: 1.4075, G2 Loss: 5.8511, D1 Loss: 0.4958, D2 Loss: 0.1139\n",
      "Batch 7, G1 Loss: 2.3133, G2 Loss: 6.0165, D1 Loss: 0.3816, D2 Loss: 0.0557\n",
      "Batch 8, G1 Loss: 2.6449, G2 Loss: 3.8196, D1 Loss: 0.8946, D2 Loss: 0.0696\n",
      "Batch 9, G1 Loss: 1.1662, G2 Loss: 2.4674, D1 Loss: 0.9345, D2 Loss: 0.1901\n",
      "Batch 10, G1 Loss: 5.3933, G2 Loss: 4.7427, D1 Loss: 0.8395, D2 Loss: 0.0487\n",
      "Batch 11, G1 Loss: 3.3655, G2 Loss: 5.0272, D1 Loss: 0.8318, D2 Loss: 0.0410\n",
      "Batch 12, G1 Loss: 0.9193, G2 Loss: 5.7789, D1 Loss: 1.3620, D2 Loss: 0.0550\n",
      "Batch 13, G1 Loss: 4.9573, G2 Loss: 4.3106, D1 Loss: 0.9585, D2 Loss: 0.1071\n",
      "Batch 14, G1 Loss: 2.7876, G2 Loss: 4.8616, D1 Loss: 0.5862, D2 Loss: 0.0441\n",
      "Batch 15, G1 Loss: 1.4729, G2 Loss: 4.0633, D1 Loss: 0.5914, D2 Loss: 0.0677\n",
      "Batch 16, G1 Loss: 1.9521, G2 Loss: 1.6473, D1 Loss: 0.4173, D2 Loss: 0.5424\n",
      "Batch 17, G1 Loss: 3.2518, G2 Loss: 12.8385, D1 Loss: 0.3692, D2 Loss: 0.4988\n",
      "Batch 18, G1 Loss: 2.4683, G2 Loss: 12.5884, D1 Loss: 0.4611, D2 Loss: 0.3520\n",
      "Batch 19, G1 Loss: 1.3572, G2 Loss: 7.4918, D1 Loss: 0.6376, D2 Loss: 0.0898\n",
      "Batch 20, G1 Loss: 3.3372, G2 Loss: 3.8331, D1 Loss: 0.4728, D2 Loss: 0.0800\n",
      "Batch 21, G1 Loss: 2.3985, G2 Loss: 2.3241, D1 Loss: 0.6160, D2 Loss: 0.2149\n",
      "Batch 22, G1 Loss: 1.2177, G2 Loss: 6.4454, D1 Loss: 0.8520, D2 Loss: 0.0212\n",
      "Batch 23, G1 Loss: 2.4633, G2 Loss: 6.9510, D1 Loss: 0.6084, D2 Loss: 0.0023\n",
      "Batch 24, G1 Loss: 2.5766, G2 Loss: 6.0038, D1 Loss: 0.3036, D2 Loss: 0.0220\n",
      "Batch 25, G1 Loss: 2.1793, G2 Loss: 3.9581, D1 Loss: 0.2834, D2 Loss: 0.0683\n",
      "Batch 26, G1 Loss: 2.5584, G2 Loss: 3.5752, D1 Loss: 0.4764, D2 Loss: 0.0972\n",
      "Batch 27, G1 Loss: 1.8777, G2 Loss: 3.0931, D1 Loss: 0.3694, D2 Loss: 0.1294\n",
      "Batch 28, G1 Loss: 3.0113, G2 Loss: 6.0477, D1 Loss: 0.3052, D2 Loss: 0.0623\n",
      "Batch 29, G1 Loss: 3.3303, G2 Loss: 8.1886, D1 Loss: 0.1413, D2 Loss: 0.0394\n",
      "Batch 30, G1 Loss: 3.0097, G2 Loss: 5.9856, D1 Loss: 0.3129, D2 Loss: 0.0832\n",
      "Batch 31, G1 Loss: 2.6850, G2 Loss: 3.0445, D1 Loss: 0.2403, D2 Loss: 0.1695\n",
      "Batch 32, G1 Loss: 2.5350, G2 Loss: 4.3597, D1 Loss: 0.1715, D2 Loss: 0.0671\n",
      "Epoch 89 Average -> G1 Loss: 2.5708653926849365, G2 Loss: 5.2861857414245605, D1 Loss: 0.6171379685401917, D2 Loss: 0.13117024302482605\n",
      "Epoch 90/100\n",
      "Batch 1, G1 Loss: 3.4278, G2 Loss: 4.4399, D1 Loss: 0.1495, D2 Loss: 0.1134\n",
      "Batch 2, G1 Loss: 4.0115, G2 Loss: 4.3652, D1 Loss: 0.2339, D2 Loss: 0.1024\n",
      "Batch 3, G1 Loss: 2.9442, G2 Loss: 4.1091, D1 Loss: 0.3880, D2 Loss: 0.2294\n",
      "Batch 4, G1 Loss: 1.5242, G2 Loss: 3.6028, D1 Loss: 0.8337, D2 Loss: 0.1517\n",
      "Batch 5, G1 Loss: 6.1507, G2 Loss: 6.7825, D1 Loss: 0.3916, D2 Loss: 0.1216\n",
      "Batch 6, G1 Loss: 5.1120, G2 Loss: 4.8441, D1 Loss: 0.6182, D2 Loss: 0.0327\n",
      "Batch 7, G1 Loss: 1.3961, G2 Loss: 3.5845, D1 Loss: 1.1560, D2 Loss: 0.1237\n",
      "Batch 8, G1 Loss: 5.2234, G2 Loss: 9.5392, D1 Loss: 0.2188, D2 Loss: 0.0423\n",
      "Batch 9, G1 Loss: 3.9237, G2 Loss: 5.5265, D1 Loss: 0.2770, D2 Loss: 0.1790\n",
      "Batch 10, G1 Loss: 0.9539, G2 Loss: 1.6541, D1 Loss: 1.4831, D2 Loss: 0.7829\n",
      "Batch 11, G1 Loss: 5.6645, G2 Loss: 16.4151, D1 Loss: 1.2759, D2 Loss: 0.1933\n",
      "Batch 12, G1 Loss: 3.5423, G2 Loss: 18.2593, D1 Loss: 0.4364, D2 Loss: 0.4250\n",
      "Batch 13, G1 Loss: 1.2454, G2 Loss: 13.2612, D1 Loss: 0.8288, D2 Loss: 0.0970\n",
      "Batch 14, G1 Loss: 2.6992, G2 Loss: 6.4847, D1 Loss: 0.4362, D2 Loss: 0.0876\n",
      "Batch 15, G1 Loss: 2.7573, G2 Loss: 6.2084, D1 Loss: 0.4472, D2 Loss: 0.0640\n",
      "Batch 16, G1 Loss: 2.1080, G2 Loss: 0.1723, D1 Loss: 0.6388, D2 Loss: 3.7618\n",
      "Batch 17, G1 Loss: 2.0311, G2 Loss: 28.1132, D1 Loss: 0.6477, D2 Loss: 1.2901\n",
      "Batch 18, G1 Loss: 2.6622, G2 Loss: 30.8302, D1 Loss: 0.9948, D2 Loss: 0.3757\n",
      "Batch 19, G1 Loss: 1.2374, G2 Loss: 26.9729, D1 Loss: 1.0127, D2 Loss: 0.0386\n",
      "Batch 20, G1 Loss: 3.9973, G2 Loss: 22.0395, D1 Loss: 1.0267, D2 Loss: 0.3533\n",
      "Batch 21, G1 Loss: 1.7587, G2 Loss: 17.4991, D1 Loss: 0.8778, D2 Loss: 0.0615\n",
      "Batch 22, G1 Loss: 1.3663, G2 Loss: 13.5673, D1 Loss: 0.7311, D2 Loss: 0.0073\n",
      "Batch 23, G1 Loss: 3.0135, G2 Loss: 9.7579, D1 Loss: 0.7800, D2 Loss: 0.0057\n",
      "Batch 24, G1 Loss: 1.4987, G2 Loss: 6.7921, D1 Loss: 1.0014, D2 Loss: 0.0055\n",
      "Batch 25, G1 Loss: 1.4936, G2 Loss: 3.5316, D1 Loss: 0.9124, D2 Loss: 0.1393\n",
      "Batch 26, G1 Loss: 2.6732, G2 Loss: 6.9010, D1 Loss: 0.9154, D2 Loss: 0.0036\n",
      "Batch 27, G1 Loss: 1.1926, G2 Loss: 6.8570, D1 Loss: 0.8521, D2 Loss: 0.0067\n",
      "Batch 28, G1 Loss: 2.4670, G2 Loss: 6.7841, D1 Loss: 0.7666, D2 Loss: 0.0128\n",
      "Batch 29, G1 Loss: 2.3186, G2 Loss: 6.0660, D1 Loss: 0.4714, D2 Loss: 0.0170\n",
      "Batch 30, G1 Loss: 2.0711, G2 Loss: 3.4803, D1 Loss: 0.4003, D2 Loss: 0.0761\n",
      "Batch 31, G1 Loss: 2.4139, G2 Loss: 3.9173, D1 Loss: 0.5620, D2 Loss: 0.0900\n",
      "Batch 32, G1 Loss: 1.8075, G2 Loss: 5.3622, D1 Loss: 0.4640, D2 Loss: 0.0336\n",
      "Epoch 90 Average -> G1 Loss: 2.7089645862579346, G2 Loss: 9.616263389587402, D1 Loss: 0.6946660280227661, D2 Loss: 0.28201642632484436\n",
      "Epoch 91/100\n",
      "Batch 1, G1 Loss: 3.1648, G2 Loss: 4.6100, D1 Loss: 0.5954, D2 Loss: 0.1856\n",
      "Batch 2, G1 Loss: 1.3482, G2 Loss: 3.5595, D1 Loss: 0.5831, D2 Loss: 0.0954\n",
      "Batch 3, G1 Loss: 2.9651, G2 Loss: 5.8998, D1 Loss: 0.5836, D2 Loss: 0.0190\n",
      "Batch 4, G1 Loss: 2.2877, G2 Loss: 0.8955, D1 Loss: 0.4179, D2 Loss: 0.9777\n",
      "Batch 5, G1 Loss: 2.4338, G2 Loss: 26.7200, D1 Loss: 0.3444, D2 Loss: 2.6200\n",
      "Batch 6, G1 Loss: 3.7082, G2 Loss: 24.0671, D1 Loss: 0.2822, D2 Loss: 0.4818\n",
      "Batch 7, G1 Loss: 2.7651, G2 Loss: 18.9403, D1 Loss: 0.2264, D2 Loss: 0.0075\n",
      "Batch 8, G1 Loss: 1.9600, G2 Loss: 13.1162, D1 Loss: 0.4178, D2 Loss: 0.0035\n",
      "Batch 9, G1 Loss: 2.4367, G2 Loss: 6.7052, D1 Loss: 0.3569, D2 Loss: 0.0353\n",
      "Batch 10, G1 Loss: 2.5962, G2 Loss: 1.7853, D1 Loss: 0.3577, D2 Loss: 0.4961\n",
      "Batch 11, G1 Loss: 2.1511, G2 Loss: 6.4322, D1 Loss: 0.6368, D2 Loss: 0.1376\n",
      "Batch 12, G1 Loss: 1.3599, G2 Loss: 6.0570, D1 Loss: 0.6829, D2 Loss: 0.0567\n",
      "Batch 13, G1 Loss: 2.9316, G2 Loss: 7.7548, D1 Loss: 1.2073, D2 Loss: 0.0153\n",
      "Batch 14, G1 Loss: 0.9347, G2 Loss: 2.2657, D1 Loss: 1.6215, D2 Loss: 0.3294\n",
      "Batch 15, G1 Loss: 2.9085, G2 Loss: 6.0331, D1 Loss: 1.1517, D2 Loss: 0.1305\n",
      "Batch 16, G1 Loss: 3.0003, G2 Loss: 5.8225, D1 Loss: 0.4910, D2 Loss: 0.2075\n",
      "Batch 17, G1 Loss: 1.4681, G2 Loss: 1.5332, D1 Loss: 0.7076, D2 Loss: 0.5657\n",
      "Batch 18, G1 Loss: 2.9387, G2 Loss: 9.7927, D1 Loss: 0.3318, D2 Loss: 0.3950\n",
      "Batch 19, G1 Loss: 3.4138, G2 Loss: 9.7638, D1 Loss: 0.4841, D2 Loss: 0.1619\n",
      "Batch 20, G1 Loss: 1.5079, G2 Loss: 5.5555, D1 Loss: 0.5742, D2 Loss: 0.0473\n",
      "Batch 21, G1 Loss: 1.9856, G2 Loss: 2.1222, D1 Loss: 0.5650, D2 Loss: 0.2595\n",
      "Batch 22, G1 Loss: 2.0841, G2 Loss: 5.9056, D1 Loss: 0.6856, D2 Loss: 0.1897\n",
      "Batch 23, G1 Loss: 1.6599, G2 Loss: 6.9358, D1 Loss: 0.6605, D2 Loss: 0.1096\n",
      "Batch 24, G1 Loss: 1.9709, G2 Loss: 3.5938, D1 Loss: 0.5286, D2 Loss: 0.1054\n",
      "Batch 25, G1 Loss: 2.2026, G2 Loss: 0.4994, D1 Loss: 0.5459, D2 Loss: 1.6626\n",
      "Batch 26, G1 Loss: 1.9138, G2 Loss: 22.9460, D1 Loss: 0.4414, D2 Loss: 6.0553\n",
      "Batch 27, G1 Loss: 2.4276, G2 Loss: 18.0397, D1 Loss: 0.4091, D2 Loss: 0.3376\n",
      "Batch 28, G1 Loss: 2.3584, G2 Loss: 10.4341, D1 Loss: 0.3605, D2 Loss: 0.0556\n",
      "Batch 29, G1 Loss: 2.8538, G2 Loss: 4.7686, D1 Loss: 0.2277, D2 Loss: 0.0647\n",
      "Batch 30, G1 Loss: 3.1598, G2 Loss: 1.7447, D1 Loss: 0.2220, D2 Loss: 1.0293\n",
      "Batch 31, G1 Loss: 2.3654, G2 Loss: 12.4906, D1 Loss: 0.3904, D2 Loss: 0.7394\n",
      "Batch 32, G1 Loss: 2.1649, G2 Loss: 10.6885, D1 Loss: 0.4327, D2 Loss: 0.2909\n",
      "Epoch 91 Average -> G1 Loss: 2.357102870941162, G2 Loss: 8.358699798583984, D1 Loss: 0.5476231575012207, D2 Loss: 0.5583847165107727\n",
      "Epoch 92/100\n",
      "Batch 1, G1 Loss: 1.9947, G2 Loss: 6.5155, D1 Loss: 0.3366, D2 Loss: 0.0848\n",
      "Batch 2, G1 Loss: 2.8559, G2 Loss: 5.9290, D1 Loss: 0.4820, D2 Loss: 0.0277\n",
      "Batch 3, G1 Loss: 2.4838, G2 Loss: 1.6050, D1 Loss: 0.3435, D2 Loss: 0.5420\n",
      "Batch 4, G1 Loss: 2.5793, G2 Loss: 6.7966, D1 Loss: 0.6322, D2 Loss: 0.0401\n",
      "Batch 5, G1 Loss: 1.0545, G2 Loss: 9.6355, D1 Loss: 1.7340, D2 Loss: 0.1965\n",
      "Batch 6, G1 Loss: 4.8722, G2 Loss: 6.4822, D1 Loss: 0.7115, D2 Loss: 0.0483\n",
      "Batch 7, G1 Loss: 3.5326, G2 Loss: 4.5854, D1 Loss: 0.3558, D2 Loss: 0.0906\n",
      "Batch 8, G1 Loss: 0.8251, G2 Loss: 4.0108, D1 Loss: 1.3678, D2 Loss: 0.2330\n",
      "Batch 9, G1 Loss: 4.8024, G2 Loss: 5.2432, D1 Loss: 0.7203, D2 Loss: 0.1125\n",
      "Batch 10, G1 Loss: 3.3546, G2 Loss: 1.7437, D1 Loss: 0.3452, D2 Loss: 0.5068\n",
      "Batch 11, G1 Loss: 1.0610, G2 Loss: 7.3869, D1 Loss: 0.8603, D2 Loss: 0.2915\n",
      "Batch 12, G1 Loss: 2.9505, G2 Loss: 8.2500, D1 Loss: 0.7791, D2 Loss: 0.3781\n",
      "Batch 13, G1 Loss: 1.8772, G2 Loss: 3.4998, D1 Loss: 0.8342, D2 Loss: 0.0763\n",
      "Batch 14, G1 Loss: 1.4011, G2 Loss: 2.6598, D1 Loss: 0.6905, D2 Loss: 0.2103\n",
      "Batch 15, G1 Loss: 2.9383, G2 Loss: 4.9078, D1 Loss: 0.6829, D2 Loss: 0.1836\n",
      "Batch 16, G1 Loss: 2.2315, G2 Loss: 6.3050, D1 Loss: 0.5254, D2 Loss: 0.1039\n",
      "Batch 17, G1 Loss: 1.7412, G2 Loss: 1.8971, D1 Loss: 0.5076, D2 Loss: 0.5130\n",
      "Batch 18, G1 Loss: 2.5810, G2 Loss: 8.3598, D1 Loss: 0.5767, D2 Loss: 0.5237\n",
      "Batch 19, G1 Loss: 1.8689, G2 Loss: 7.2177, D1 Loss: 0.5802, D2 Loss: 0.2680\n",
      "Batch 20, G1 Loss: 1.8603, G2 Loss: 3.1140, D1 Loss: 0.5209, D2 Loss: 0.1895\n",
      "Batch 21, G1 Loss: 2.6154, G2 Loss: 2.7940, D1 Loss: 0.5961, D2 Loss: 0.2142\n",
      "Batch 22, G1 Loss: 1.9052, G2 Loss: 4.6154, D1 Loss: 0.4926, D2 Loss: 0.0782\n",
      "Batch 23, G1 Loss: 2.1120, G2 Loss: 4.6653, D1 Loss: 0.5369, D2 Loss: 0.1401\n",
      "Batch 24, G1 Loss: 2.7505, G2 Loss: 3.6980, D1 Loss: 0.5337, D2 Loss: 0.1309\n",
      "Batch 25, G1 Loss: 1.5193, G2 Loss: 4.4659, D1 Loss: 0.8061, D2 Loss: 0.0391\n",
      "Batch 26, G1 Loss: 2.0860, G2 Loss: 3.9947, D1 Loss: 0.5539, D2 Loss: 0.0838\n",
      "Batch 27, G1 Loss: 2.6461, G2 Loss: 1.6922, D1 Loss: 0.7568, D2 Loss: 0.3675\n",
      "Batch 28, G1 Loss: 1.6677, G2 Loss: 9.6965, D1 Loss: 0.6023, D2 Loss: 0.2688\n",
      "Batch 29, G1 Loss: 2.7500, G2 Loss: 9.1205, D1 Loss: 0.5384, D2 Loss: 0.1800\n",
      "Batch 30, G1 Loss: 1.7996, G2 Loss: 5.9339, D1 Loss: 0.6319, D2 Loss: 0.0572\n",
      "Batch 31, G1 Loss: 2.1422, G2 Loss: 2.3245, D1 Loss: 0.7253, D2 Loss: 0.3547\n",
      "Batch 32, G1 Loss: 2.0046, G2 Loss: 4.4564, D1 Loss: 0.5083, D2 Loss: 0.1862\n",
      "Epoch 92 Average -> G1 Loss: 2.339517116546631, G2 Loss: 5.112570762634277, D1 Loss: 0.6521589159965515, D2 Loss: 0.21003170311450958\n",
      "Epoch 93/100\n",
      "Batch 1, G1 Loss: 2.0664, G2 Loss: 4.8730, D1 Loss: 0.3969, D2 Loss: 0.0940\n",
      "Batch 2, G1 Loss: 2.6530, G2 Loss: 9.3105, D1 Loss: 0.4571, D2 Loss: 0.0822\n",
      "Batch 3, G1 Loss: 1.3912, G2 Loss: 3.1157, D1 Loss: 0.7814, D2 Loss: 0.1487\n",
      "Batch 4, G1 Loss: 2.8740, G2 Loss: 4.4361, D1 Loss: 0.8910, D2 Loss: 0.1096\n",
      "Batch 5, G1 Loss: 1.2017, G2 Loss: 2.6844, D1 Loss: 1.2847, D2 Loss: 0.2578\n",
      "Batch 6, G1 Loss: 2.3891, G2 Loss: 3.2499, D1 Loss: 0.7452, D2 Loss: 0.2472\n",
      "Batch 7, G1 Loss: 2.0602, G2 Loss: 3.7024, D1 Loss: 0.5635, D2 Loss: 0.2816\n",
      "Batch 8, G1 Loss: 1.9398, G2 Loss: 3.8121, D1 Loss: 0.5403, D2 Loss: 0.2106\n",
      "Batch 9, G1 Loss: 2.6872, G2 Loss: 6.5344, D1 Loss: 0.4951, D2 Loss: 0.0814\n",
      "Batch 10, G1 Loss: 2.0251, G2 Loss: 7.4406, D1 Loss: 0.6066, D2 Loss: 0.0433\n",
      "Batch 11, G1 Loss: 2.6182, G2 Loss: 1.8595, D1 Loss: 0.5152, D2 Loss: 0.5183\n",
      "Batch 12, G1 Loss: 3.0406, G2 Loss: 10.9495, D1 Loss: 0.2591, D2 Loss: 0.2942\n",
      "Batch 13, G1 Loss: 1.3769, G2 Loss: 11.6768, D1 Loss: 0.5947, D2 Loss: 0.1925\n",
      "Batch 14, G1 Loss: 3.7960, G2 Loss: 8.2788, D1 Loss: 0.7624, D2 Loss: 0.0636\n",
      "Batch 15, G1 Loss: 2.3232, G2 Loss: 4.5664, D1 Loss: 0.3961, D2 Loss: 0.0337\n",
      "Batch 16, G1 Loss: 1.8158, G2 Loss: 2.5919, D1 Loss: 0.5748, D2 Loss: 0.2351\n",
      "Batch 17, G1 Loss: 2.1862, G2 Loss: 5.0108, D1 Loss: 0.4580, D2 Loss: 0.1134\n",
      "Batch 18, G1 Loss: 2.1952, G2 Loss: 4.2369, D1 Loss: 0.2576, D2 Loss: 0.1568\n",
      "Batch 19, G1 Loss: 3.6437, G2 Loss: 6.5669, D1 Loss: 0.4878, D2 Loss: 0.0366\n",
      "Batch 20, G1 Loss: 2.1756, G2 Loss: 4.0566, D1 Loss: 0.4551, D2 Loss: 0.1527\n",
      "Batch 21, G1 Loss: 1.6368, G2 Loss: 3.5315, D1 Loss: 0.7915, D2 Loss: 0.0975\n",
      "Batch 22, G1 Loss: 4.6417, G2 Loss: 4.8696, D1 Loss: 1.2995, D2 Loss: 0.1526\n",
      "Batch 23, G1 Loss: 1.8731, G2 Loss: 3.8443, D1 Loss: 0.6123, D2 Loss: 0.1351\n",
      "Batch 24, G1 Loss: 2.0286, G2 Loss: 2.7234, D1 Loss: 0.8065, D2 Loss: 0.1864\n",
      "Batch 25, G1 Loss: 1.9735, G2 Loss: 4.2031, D1 Loss: 0.5827, D2 Loss: 0.0926\n",
      "Batch 26, G1 Loss: 2.6630, G2 Loss: 4.8655, D1 Loss: 0.4388, D2 Loss: 0.1591\n",
      "Batch 27, G1 Loss: 1.9391, G2 Loss: 6.4801, D1 Loss: 0.5738, D2 Loss: 0.0383\n",
      "Batch 28, G1 Loss: 1.4777, G2 Loss: 5.7844, D1 Loss: 0.7734, D2 Loss: 0.0348\n",
      "Batch 29, G1 Loss: 3.7300, G2 Loss: 1.3980, D1 Loss: 0.8151, D2 Loss: 0.6152\n",
      "Batch 30, G1 Loss: 1.0632, G2 Loss: 10.4455, D1 Loss: 0.9927, D2 Loss: 0.4083\n",
      "Batch 31, G1 Loss: 3.3833, G2 Loss: 11.0042, D1 Loss: 0.6967, D2 Loss: 0.8468\n",
      "Batch 32, G1 Loss: 1.5210, G2 Loss: 6.0692, D1 Loss: 0.7584, D2 Loss: 0.0285\n",
      "Epoch 93 Average -> G1 Loss: 2.3246898651123047, G2 Loss: 5.442872524261475, D1 Loss: 0.6457558870315552, D2 Loss: 0.1921311914920807\n",
      "Epoch 94/100\n",
      "Batch 1, G1 Loss: 1.2956, G2 Loss: 3.1287, D1 Loss: 0.9532, D2 Loss: 0.2194\n",
      "Batch 2, G1 Loss: 2.7290, G2 Loss: 3.6978, D1 Loss: 1.5646, D2 Loss: 0.1195\n",
      "Batch 3, G1 Loss: 0.5853, G2 Loss: 4.8109, D1 Loss: 2.3498, D2 Loss: 0.1115\n",
      "Batch 4, G1 Loss: 3.6834, G2 Loss: 4.7726, D1 Loss: 1.0650, D2 Loss: 0.0820\n",
      "Batch 5, G1 Loss: 1.9251, G2 Loss: 5.7856, D1 Loss: 0.6658, D2 Loss: 0.0857\n",
      "Batch 6, G1 Loss: 1.4331, G2 Loss: 5.6163, D1 Loss: 0.8219, D2 Loss: 0.0511\n",
      "Batch 7, G1 Loss: 2.3747, G2 Loss: 3.2867, D1 Loss: 0.5725, D2 Loss: 0.0946\n",
      "Batch 8, G1 Loss: 1.9036, G2 Loss: 3.7337, D1 Loss: 0.7174, D2 Loss: 0.1463\n",
      "Batch 9, G1 Loss: 1.7474, G2 Loss: 4.1487, D1 Loss: 0.9250, D2 Loss: 0.1572\n",
      "Batch 10, G1 Loss: 1.5032, G2 Loss: 4.3837, D1 Loss: 1.0093, D2 Loss: 0.1305\n",
      "Batch 11, G1 Loss: 2.1139, G2 Loss: 6.0800, D1 Loss: 0.9756, D2 Loss: 0.0518\n",
      "Batch 12, G1 Loss: 1.3793, G2 Loss: 2.7970, D1 Loss: 0.8417, D2 Loss: 0.2240\n",
      "Batch 13, G1 Loss: 2.0835, G2 Loss: 4.1726, D1 Loss: 0.7964, D2 Loss: 0.2304\n",
      "Batch 14, G1 Loss: 2.5833, G2 Loss: 4.9199, D1 Loss: 0.4843, D2 Loss: 0.1823\n",
      "Batch 15, G1 Loss: 1.8558, G2 Loss: 2.6759, D1 Loss: 0.6154, D2 Loss: 0.1945\n",
      "Batch 16, G1 Loss: 1.4531, G2 Loss: 3.6758, D1 Loss: 0.8354, D2 Loss: 0.1611\n",
      "Batch 17, G1 Loss: 2.3396, G2 Loss: 4.2989, D1 Loss: 0.9171, D2 Loss: 0.1027\n",
      "Batch 18, G1 Loss: 3.1388, G2 Loss: 5.8787, D1 Loss: 0.3713, D2 Loss: 0.0456\n",
      "Batch 19, G1 Loss: 1.1022, G2 Loss: 6.1492, D1 Loss: 0.8143, D2 Loss: 0.1858\n",
      "Batch 20, G1 Loss: 3.8877, G2 Loss: 1.2623, D1 Loss: 0.4592, D2 Loss: 0.7650\n",
      "Batch 21, G1 Loss: 3.1742, G2 Loss: 12.7541, D1 Loss: 0.4891, D2 Loss: 1.6009\n",
      "Batch 22, G1 Loss: 1.0956, G2 Loss: 9.7951, D1 Loss: 0.7743, D2 Loss: 0.3577\n",
      "Batch 23, G1 Loss: 3.0352, G2 Loss: 4.3592, D1 Loss: 0.5283, D2 Loss: 0.0800\n",
      "Batch 24, G1 Loss: 3.2075, G2 Loss: 1.9413, D1 Loss: 0.2455, D2 Loss: 0.5081\n",
      "Batch 25, G1 Loss: 2.3439, G2 Loss: 7.3778, D1 Loss: 0.7766, D2 Loss: 0.2730\n",
      "Batch 26, G1 Loss: 1.2166, G2 Loss: 5.3682, D1 Loss: 0.8707, D2 Loss: 0.1428\n",
      "Batch 27, G1 Loss: 3.5000, G2 Loss: 3.5402, D1 Loss: 1.0809, D2 Loss: 0.1104\n",
      "Batch 28, G1 Loss: 2.0426, G2 Loss: 3.1680, D1 Loss: 0.5022, D2 Loss: 0.1643\n",
      "Batch 29, G1 Loss: 2.1607, G2 Loss: 4.1320, D1 Loss: 0.5372, D2 Loss: 0.0956\n",
      "Batch 30, G1 Loss: 1.7089, G2 Loss: 10.8740, D1 Loss: 0.7849, D2 Loss: 0.0668\n",
      "Batch 31, G1 Loss: 3.0039, G2 Loss: 3.4997, D1 Loss: 0.5743, D2 Loss: 0.1766\n",
      "Batch 32, G1 Loss: 2.4698, G2 Loss: 2.1662, D1 Loss: 0.7391, D2 Loss: 0.2789\n",
      "Epoch 94 Average -> G1 Loss: 2.1898951530456543, G2 Loss: 4.820331573486328, D1 Loss: 0.8018248081207275, D2 Loss: 0.22488120198249817\n",
      "Epoch 95/100\n",
      "Batch 1, G1 Loss: 0.7626, G2 Loss: 5.3376, D1 Loss: 1.3276, D2 Loss: 0.1774\n",
      "Batch 2, G1 Loss: 4.1817, G2 Loss: 4.5843, D1 Loss: 1.2370, D2 Loss: 0.2056\n",
      "Batch 3, G1 Loss: 1.5737, G2 Loss: 2.5360, D1 Loss: 0.7343, D2 Loss: 0.2349\n",
      "Batch 4, G1 Loss: 1.2931, G2 Loss: 3.4752, D1 Loss: 0.6881, D2 Loss: 0.0770\n",
      "Batch 5, G1 Loss: 3.0074, G2 Loss: 10.8475, D1 Loss: 0.8249, D2 Loss: 0.2214\n",
      "Batch 6, G1 Loss: 1.1883, G2 Loss: 8.8500, D1 Loss: 0.9273, D2 Loss: 0.0514\n",
      "Batch 7, G1 Loss: 2.4100, G2 Loss: 2.4990, D1 Loss: 0.7569, D2 Loss: 0.1987\n",
      "Batch 8, G1 Loss: 1.7125, G2 Loss: 4.1674, D1 Loss: 0.6425, D2 Loss: 0.0661\n",
      "Batch 9, G1 Loss: 1.9129, G2 Loss: 4.7855, D1 Loss: 0.7608, D2 Loss: 0.0975\n",
      "Batch 10, G1 Loss: 1.5440, G2 Loss: 3.8336, D1 Loss: 0.7008, D2 Loss: 0.1179\n",
      "Batch 11, G1 Loss: 2.3872, G2 Loss: 4.0298, D1 Loss: 0.9925, D2 Loss: 0.2259\n",
      "Batch 12, G1 Loss: 1.5291, G2 Loss: 3.3089, D1 Loss: 0.8567, D2 Loss: 0.2066\n",
      "Batch 13, G1 Loss: 1.6700, G2 Loss: 5.2184, D1 Loss: 0.6869, D2 Loss: 0.0420\n",
      "Batch 14, G1 Loss: 2.6706, G2 Loss: 5.0621, D1 Loss: 0.6966, D2 Loss: 0.0495\n",
      "Batch 15, G1 Loss: 1.5886, G2 Loss: 3.3976, D1 Loss: 0.6585, D2 Loss: 0.1022\n",
      "Batch 16, G1 Loss: 1.8927, G2 Loss: 4.0740, D1 Loss: 0.4735, D2 Loss: 0.0835\n",
      "Batch 17, G1 Loss: 3.1635, G2 Loss: 4.2313, D1 Loss: 0.5006, D2 Loss: 0.2119\n",
      "Batch 18, G1 Loss: 1.7065, G2 Loss: 2.8932, D1 Loss: 0.4815, D2 Loss: 0.2516\n",
      "Batch 19, G1 Loss: 2.2313, G2 Loss: 5.1891, D1 Loss: 0.4471, D2 Loss: 0.0613\n",
      "Batch 20, G1 Loss: 2.2817, G2 Loss: 4.8172, D1 Loss: 0.5590, D2 Loss: 0.0501\n",
      "Batch 21, G1 Loss: 1.6766, G2 Loss: 2.4021, D1 Loss: 0.5762, D2 Loss: 0.2652\n",
      "Batch 22, G1 Loss: 2.4457, G2 Loss: 5.6960, D1 Loss: 0.7296, D2 Loss: 0.1325\n",
      "Batch 23, G1 Loss: 1.2025, G2 Loss: 5.7676, D1 Loss: 0.7842, D2 Loss: 0.1097\n",
      "Batch 24, G1 Loss: 2.8269, G2 Loss: 3.0973, D1 Loss: 0.4665, D2 Loss: 0.1581\n",
      "Batch 25, G1 Loss: 2.1534, G2 Loss: 3.6687, D1 Loss: 0.6061, D2 Loss: 0.1050\n",
      "Batch 26, G1 Loss: 1.2982, G2 Loss: 3.0785, D1 Loss: 0.7708, D2 Loss: 0.2125\n",
      "Batch 27, G1 Loss: 3.0345, G2 Loss: 4.3201, D1 Loss: 0.6050, D2 Loss: 0.2553\n",
      "Batch 28, G1 Loss: 1.3255, G2 Loss: 8.0959, D1 Loss: 1.1037, D2 Loss: 0.0529\n",
      "Batch 29, G1 Loss: 2.0560, G2 Loss: 2.0932, D1 Loss: 0.8674, D2 Loss: 0.5702\n",
      "Batch 30, G1 Loss: 1.7355, G2 Loss: 8.8352, D1 Loss: 1.0795, D2 Loss: 0.1474\n",
      "Batch 31, G1 Loss: 1.5432, G2 Loss: 7.7204, D1 Loss: 0.8992, D2 Loss: 0.1392\n",
      "Batch 32, G1 Loss: 2.3056, G2 Loss: 3.3364, D1 Loss: 0.7414, D2 Loss: 0.1627\n",
      "Epoch 95 Average -> G1 Loss: 2.0097169876098633, G2 Loss: 4.726531505584717, D1 Loss: 0.7557146549224854, D2 Loss: 0.15761296451091766\n",
      "Epoch 96/100\n",
      "Batch 1, G1 Loss: 2.1020, G2 Loss: 3.9085, D1 Loss: 0.6288, D2 Loss: 0.0794\n",
      "Batch 2, G1 Loss: 1.1448, G2 Loss: 1.3168, D1 Loss: 1.0394, D2 Loss: 0.6572\n",
      "Batch 3, G1 Loss: 3.4174, G2 Loss: 18.6642, D1 Loss: 1.0103, D2 Loss: 0.5572\n",
      "Batch 4, G1 Loss: 0.5787, G2 Loss: 18.9865, D1 Loss: 1.8766, D2 Loss: 0.0886\n",
      "Batch 5, G1 Loss: 3.5360, G2 Loss: 15.1955, D1 Loss: 0.9696, D2 Loss: 0.0289\n",
      "Batch 6, G1 Loss: 2.2163, G2 Loss: 7.9780, D1 Loss: 0.6258, D2 Loss: 0.0377\n",
      "Batch 7, G1 Loss: 1.0378, G2 Loss: 2.0140, D1 Loss: 0.9583, D2 Loss: 0.2836\n",
      "Batch 8, G1 Loss: 2.5401, G2 Loss: 7.5691, D1 Loss: 1.1169, D2 Loss: 0.1090\n",
      "Batch 9, G1 Loss: 1.5303, G2 Loss: 8.0665, D1 Loss: 0.8588, D2 Loss: 0.0498\n",
      "Batch 10, G1 Loss: 1.4163, G2 Loss: 6.0288, D1 Loss: 1.0568, D2 Loss: 0.4184\n",
      "Batch 11, G1 Loss: 2.0968, G2 Loss: 1.9830, D1 Loss: 1.0052, D2 Loss: 0.3375\n",
      "Batch 12, G1 Loss: 1.4901, G2 Loss: 10.5563, D1 Loss: 0.9749, D2 Loss: 0.0660\n",
      "Batch 13, G1 Loss: 1.9360, G2 Loss: 11.6417, D1 Loss: 0.8023, D2 Loss: 0.1648\n",
      "Batch 14, G1 Loss: 2.1736, G2 Loss: 8.9311, D1 Loss: 0.6540, D2 Loss: 0.0095\n",
      "Batch 15, G1 Loss: 1.8009, G2 Loss: 8.3212, D1 Loss: 0.5997, D2 Loss: 0.0341\n",
      "Batch 16, G1 Loss: 2.5772, G2 Loss: 3.6943, D1 Loss: 0.7731, D2 Loss: 0.0644\n",
      "Batch 17, G1 Loss: 1.6085, G2 Loss: 7.2239, D1 Loss: 0.7854, D2 Loss: 0.0059\n",
      "Batch 18, G1 Loss: 1.8603, G2 Loss: 1.2010, D1 Loss: 0.6497, D2 Loss: 0.8587\n",
      "Batch 19, G1 Loss: 1.8254, G2 Loss: 22.5720, D1 Loss: 0.6042, D2 Loss: 0.5789\n",
      "Batch 20, G1 Loss: 2.6453, G2 Loss: 23.8406, D1 Loss: 0.5783, D2 Loss: 0.2346\n",
      "Batch 21, G1 Loss: 1.3016, G2 Loss: 18.5482, D1 Loss: 0.7690, D2 Loss: 0.0551\n",
      "Batch 22, G1 Loss: 2.5477, G2 Loss: 13.6186, D1 Loss: 0.6624, D2 Loss: 0.0248\n",
      "Batch 23, G1 Loss: 1.2261, G2 Loss: 10.2831, D1 Loss: 1.0255, D2 Loss: 0.0648\n",
      "Batch 24, G1 Loss: 2.6260, G2 Loss: 0.2407, D1 Loss: 0.9081, D2 Loss: 2.7356\n",
      "Batch 25, G1 Loss: 1.6903, G2 Loss: 29.5202, D1 Loss: 0.8037, D2 Loss: 1.6698\n",
      "Batch 26, G1 Loss: 2.4751, G2 Loss: 29.6633, D1 Loss: 0.7333, D2 Loss: 0.3336\n",
      "Batch 27, G1 Loss: 1.8900, G2 Loss: 21.4507, D1 Loss: 0.7149, D2 Loss: 0.1615\n",
      "Batch 28, G1 Loss: 1.8640, G2 Loss: 12.7269, D1 Loss: 0.6321, D2 Loss: 0.0418\n",
      "Batch 29, G1 Loss: 2.2027, G2 Loss: 6.8520, D1 Loss: 0.6721, D2 Loss: 0.0147\n",
      "Batch 30, G1 Loss: 2.0347, G2 Loss: 4.1587, D1 Loss: 0.5647, D2 Loss: 0.0893\n",
      "Batch 31, G1 Loss: 1.8373, G2 Loss: 2.7823, D1 Loss: 0.6769, D2 Loss: 0.1405\n",
      "Batch 32, G1 Loss: 2.1910, G2 Loss: 6.1968, D1 Loss: 0.8430, D2 Loss: 0.2084\n",
      "Epoch 96 Average -> G1 Loss: 1.9818755388259888, G2 Loss: 10.804203987121582, D1 Loss: 0.8304420709609985, D2 Loss: 0.3188823163509369\n",
      "Epoch 97/100\n",
      "Batch 1, G1 Loss: 1.1464, G2 Loss: 5.3765, D1 Loss: 1.2211, D2 Loss: 0.0624\n",
      "Batch 2, G1 Loss: 2.6513, G2 Loss: 2.9089, D1 Loss: 0.9277, D2 Loss: 0.2164\n",
      "Batch 3, G1 Loss: 0.7111, G2 Loss: 4.0509, D1 Loss: 1.5282, D2 Loss: 0.1007\n",
      "Batch 4, G1 Loss: 4.2565, G2 Loss: 3.0326, D1 Loss: 1.7233, D2 Loss: 0.2349\n",
      "Batch 5, G1 Loss: 1.3133, G2 Loss: 2.9266, D1 Loss: 0.7171, D2 Loss: 0.4681\n",
      "Batch 6, G1 Loss: 2.3359, G2 Loss: 4.8193, D1 Loss: 0.9191, D2 Loss: 0.2318\n",
      "Batch 7, G1 Loss: 0.6140, G2 Loss: 6.9254, D1 Loss: 1.5287, D2 Loss: 0.1399\n",
      "Batch 8, G1 Loss: 4.9533, G2 Loss: 0.7964, D1 Loss: 1.1729, D2 Loss: 1.0581\n",
      "Batch 9, G1 Loss: 2.8892, G2 Loss: 18.3488, D1 Loss: 0.3531, D2 Loss: 0.9078\n",
      "Batch 10, G1 Loss: 0.8239, G2 Loss: 18.1591, D1 Loss: 1.2885, D2 Loss: 0.3749\n",
      "Batch 11, G1 Loss: 4.2231, G2 Loss: 14.7987, D1 Loss: 0.8359, D2 Loss: 0.1094\n",
      "Batch 12, G1 Loss: 3.5883, G2 Loss: 8.7969, D1 Loss: 0.4886, D2 Loss: 0.0106\n",
      "Batch 13, G1 Loss: 0.8637, G2 Loss: 3.7053, D1 Loss: 1.0694, D2 Loss: 0.0569\n",
      "Batch 14, G1 Loss: 3.3104, G2 Loss: 2.0793, D1 Loss: 0.3555, D2 Loss: 0.3645\n",
      "Batch 15, G1 Loss: 3.6473, G2 Loss: 5.5572, D1 Loss: 0.4414, D2 Loss: 0.0565\n",
      "Batch 16, G1 Loss: 1.9896, G2 Loss: 6.1576, D1 Loss: 0.5877, D2 Loss: 0.0746\n",
      "Batch 17, G1 Loss: 1.4050, G2 Loss: 8.1807, D1 Loss: 0.7572, D2 Loss: 0.2072\n",
      "Batch 18, G1 Loss: 2.6209, G2 Loss: 2.4777, D1 Loss: 0.6272, D2 Loss: 0.4001\n",
      "Batch 19, G1 Loss: 2.6235, G2 Loss: 3.0899, D1 Loss: 0.5591, D2 Loss: 0.1680\n",
      "Batch 20, G1 Loss: 1.3613, G2 Loss: 5.8129, D1 Loss: 0.8572, D2 Loss: 0.1749\n",
      "Batch 21, G1 Loss: 1.9211, G2 Loss: 8.8856, D1 Loss: 0.6985, D2 Loss: 0.0308\n",
      "Batch 22, G1 Loss: 2.6561, G2 Loss: 7.2701, D1 Loss: 0.9358, D2 Loss: 0.0656\n",
      "Batch 23, G1 Loss: 0.9907, G2 Loss: 0.7742, D1 Loss: 1.3889, D2 Loss: 1.2067\n",
      "Batch 24, G1 Loss: 2.3827, G2 Loss: 19.1128, D1 Loss: 1.0164, D2 Loss: 1.6902\n",
      "Batch 25, G1 Loss: 1.2571, G2 Loss: 14.2844, D1 Loss: 1.0327, D2 Loss: 0.2103\n",
      "Batch 26, G1 Loss: 1.8572, G2 Loss: 9.5555, D1 Loss: 1.2879, D2 Loss: 0.1389\n",
      "Batch 27, G1 Loss: 1.1440, G2 Loss: 5.3247, D1 Loss: 1.1622, D2 Loss: 0.1416\n",
      "Batch 28, G1 Loss: 2.2649, G2 Loss: 3.1908, D1 Loss: 1.0281, D2 Loss: 0.1529\n",
      "Batch 29, G1 Loss: 2.1698, G2 Loss: 1.9948, D1 Loss: 0.4978, D2 Loss: 0.3085\n",
      "Batch 30, G1 Loss: 1.1305, G2 Loss: 5.2355, D1 Loss: 0.9965, D2 Loss: 0.0213\n",
      "Batch 31, G1 Loss: 3.7613, G2 Loss: 6.5237, D1 Loss: 0.9582, D2 Loss: 0.0742\n",
      "Batch 32, G1 Loss: 2.4754, G2 Loss: 8.0039, D1 Loss: 0.6624, D2 Loss: 0.0255\n",
      "Epoch 97 Average -> G1 Loss: 2.229346513748169, G2 Loss: 6.8173980712890625, D1 Loss: 0.9257614016532898, D2 Loss: 0.29638269543647766\n",
      "Epoch 98/100\n",
      "Batch 1, G1 Loss: 0.7853, G2 Loss: 3.8728, D1 Loss: 1.4203, D2 Loss: 0.0667\n",
      "Batch 2, G1 Loss: 4.5976, G2 Loss: 2.7264, D1 Loss: 0.5181, D2 Loss: 0.3335\n",
      "Batch 3, G1 Loss: 3.4759, G2 Loss: 4.3919, D1 Loss: 0.5569, D2 Loss: 0.1611\n",
      "Batch 4, G1 Loss: 1.1448, G2 Loss: 4.7580, D1 Loss: 0.9083, D2 Loss: 0.2260\n",
      "Batch 5, G1 Loss: 2.9570, G2 Loss: 7.1788, D1 Loss: 0.7459, D2 Loss: 0.0288\n",
      "Batch 6, G1 Loss: 1.7108, G2 Loss: 4.3167, D1 Loss: 0.5289, D2 Loss: 0.0395\n",
      "Batch 7, G1 Loss: 1.8960, G2 Loss: 2.2578, D1 Loss: 0.7409, D2 Loss: 0.3426\n",
      "Batch 8, G1 Loss: 2.6323, G2 Loss: 5.1297, D1 Loss: 1.1251, D2 Loss: 0.1029\n",
      "Batch 9, G1 Loss: 0.9894, G2 Loss: 6.8692, D1 Loss: 1.3329, D2 Loss: 0.0759\n",
      "Batch 10, G1 Loss: 2.1367, G2 Loss: 10.1290, D1 Loss: 0.8241, D2 Loss: 0.2238\n",
      "Batch 11, G1 Loss: 1.6996, G2 Loss: 1.9514, D1 Loss: 0.8097, D2 Loss: 0.4989\n",
      "Batch 12, G1 Loss: 1.4937, G2 Loss: 7.6029, D1 Loss: 0.8976, D2 Loss: 0.1103\n",
      "Batch 13, G1 Loss: 2.3403, G2 Loss: 10.1662, D1 Loss: 0.8275, D2 Loss: 0.1235\n",
      "Batch 14, G1 Loss: 1.4932, G2 Loss: 8.3368, D1 Loss: 0.9952, D2 Loss: 0.0288\n",
      "Batch 15, G1 Loss: 1.7187, G2 Loss: 6.4090, D1 Loss: 0.8037, D2 Loss: 0.1519\n",
      "Batch 16, G1 Loss: 2.2039, G2 Loss: 1.5203, D1 Loss: 0.5766, D2 Loss: 0.6498\n",
      "Batch 17, G1 Loss: 2.1163, G2 Loss: 10.5307, D1 Loss: 0.5129, D2 Loss: 0.1478\n",
      "Batch 18, G1 Loss: 2.0950, G2 Loss: 14.4821, D1 Loss: 0.5683, D2 Loss: 0.5698\n",
      "Batch 19, G1 Loss: 2.3327, G2 Loss: 8.8949, D1 Loss: 0.4631, D2 Loss: 0.0518\n",
      "Batch 20, G1 Loss: 2.1624, G2 Loss: 4.2055, D1 Loss: 0.4906, D2 Loss: 0.0814\n",
      "Batch 21, G1 Loss: 2.0529, G2 Loss: 9.1834, D1 Loss: 0.5319, D2 Loss: 0.1097\n",
      "Batch 22, G1 Loss: 2.3113, G2 Loss: 1.9093, D1 Loss: 0.4146, D2 Loss: 0.4817\n",
      "Batch 23, G1 Loss: 2.4277, G2 Loss: 9.2371, D1 Loss: 0.5643, D2 Loss: 0.0770\n",
      "Batch 24, G1 Loss: 1.4125, G2 Loss: 9.8291, D1 Loss: 0.7008, D2 Loss: 0.2109\n",
      "Batch 25, G1 Loss: 3.0940, G2 Loss: 10.9293, D1 Loss: 0.7233, D2 Loss: 0.1479\n",
      "Batch 26, G1 Loss: 1.4891, G2 Loss: 2.7464, D1 Loss: 0.7331, D2 Loss: 0.1651\n",
      "Batch 27, G1 Loss: 2.8215, G2 Loss: 4.8323, D1 Loss: 0.6337, D2 Loss: 0.1458\n",
      "Batch 28, G1 Loss: 2.0983, G2 Loss: 3.8350, D1 Loss: 0.4094, D2 Loss: 0.1068\n",
      "Batch 29, G1 Loss: 1.8957, G2 Loss: 1.5510, D1 Loss: 0.6505, D2 Loss: 0.6321\n",
      "Batch 30, G1 Loss: 2.5960, G2 Loss: 12.6837, D1 Loss: 0.6058, D2 Loss: 0.5368\n",
      "Batch 31, G1 Loss: 2.2117, G2 Loss: 14.4066, D1 Loss: 0.5597, D2 Loss: 0.4913\n",
      "Batch 32, G1 Loss: 1.2272, G2 Loss: 8.9395, D1 Loss: 0.9499, D2 Loss: 0.0080\n",
      "Epoch 98 Average -> G1 Loss: 2.1131114959716797, G2 Loss: 6.744149684906006, D1 Loss: 0.7226110100746155, D2 Loss: 0.22273962199687958\n",
      "Epoch 99/100\n",
      "Batch 1, G1 Loss: 2.7913, G2 Loss: 8.0774, D1 Loss: 0.8416, D2 Loss: 0.0606\n",
      "Batch 2, G1 Loss: 2.0209, G2 Loss: 1.5918, D1 Loss: 0.6973, D2 Loss: 0.8840\n",
      "Batch 3, G1 Loss: 1.1805, G2 Loss: 12.0885, D1 Loss: 1.0766, D2 Loss: 0.4106\n",
      "Batch 4, G1 Loss: 2.3292, G2 Loss: 14.0082, D1 Loss: 1.0517, D2 Loss: 0.1615\n",
      "Batch 5, G1 Loss: 1.7394, G2 Loss: 9.7792, D1 Loss: 0.5724, D2 Loss: 0.0648\n",
      "Batch 6, G1 Loss: 1.6943, G2 Loss: 7.6657, D1 Loss: 0.7288, D2 Loss: 0.0143\n",
      "Batch 7, G1 Loss: 1.7930, G2 Loss: 6.2047, D1 Loss: 1.3009, D2 Loss: 0.0273\n",
      "Batch 8, G1 Loss: 0.7727, G2 Loss: 0.9319, D1 Loss: 1.3827, D2 Loss: 1.3647\n",
      "Batch 9, G1 Loss: 3.5349, G2 Loss: 16.6517, D1 Loss: 1.2964, D2 Loss: 1.7234\n",
      "Batch 10, G1 Loss: 1.2152, G2 Loss: 15.2090, D1 Loss: 0.8190, D2 Loss: 0.2866\n",
      "Batch 11, G1 Loss: 1.7332, G2 Loss: 11.4771, D1 Loss: 0.4986, D2 Loss: 0.0268\n",
      "Batch 12, G1 Loss: 2.5783, G2 Loss: 7.1998, D1 Loss: 0.7476, D2 Loss: 0.0428\n",
      "Batch 13, G1 Loss: 1.5538, G2 Loss: 3.3809, D1 Loss: 0.9213, D2 Loss: 0.2113\n",
      "Batch 14, G1 Loss: 2.3985, G2 Loss: 3.2476, D1 Loss: 0.4323, D2 Loss: 0.1333\n",
      "Batch 15, G1 Loss: 3.9330, G2 Loss: 5.7112, D1 Loss: 0.8986, D2 Loss: 0.4357\n",
      "Batch 16, G1 Loss: 0.9326, G2 Loss: 2.1080, D1 Loss: 0.8669, D2 Loss: 0.3251\n",
      "Batch 17, G1 Loss: 4.0319, G2 Loss: 6.4472, D1 Loss: 0.2816, D2 Loss: 0.0942\n",
      "Batch 18, G1 Loss: 3.9216, G2 Loss: 11.8523, D1 Loss: 0.5455, D2 Loss: 0.1576\n",
      "Batch 19, G1 Loss: 2.0041, G2 Loss: 2.9587, D1 Loss: 0.3717, D2 Loss: 0.0976\n",
      "Batch 20, G1 Loss: 2.0754, G2 Loss: 5.2922, D1 Loss: 0.5243, D2 Loss: 0.0566\n",
      "Batch 21, G1 Loss: 2.0389, G2 Loss: 2.0297, D1 Loss: 0.5292, D2 Loss: 0.2768\n",
      "Batch 22, G1 Loss: 2.2735, G2 Loss: 6.3147, D1 Loss: 0.6146, D2 Loss: 0.2431\n",
      "Batch 23, G1 Loss: 1.9849, G2 Loss: 6.7821, D1 Loss: 0.6413, D2 Loss: 0.1144\n",
      "Batch 24, G1 Loss: 3.4638, G2 Loss: 4.1598, D1 Loss: 0.6109, D2 Loss: 0.1151\n",
      "Batch 25, G1 Loss: 1.4469, G2 Loss: 8.1819, D1 Loss: 0.9260, D2 Loss: 0.0417\n",
      "Batch 26, G1 Loss: 2.5163, G2 Loss: 2.4378, D1 Loss: 0.8901, D2 Loss: 0.2448\n",
      "Batch 27, G1 Loss: 1.2870, G2 Loss: 4.7424, D1 Loss: 0.9850, D2 Loss: 0.1064\n",
      "Batch 28, G1 Loss: 3.3453, G2 Loss: 5.6701, D1 Loss: 1.0079, D2 Loss: 0.0220\n",
      "Batch 29, G1 Loss: 1.1131, G2 Loss: 3.6682, D1 Loss: 1.0941, D2 Loss: 0.1717\n",
      "Batch 30, G1 Loss: 2.1313, G2 Loss: 3.3182, D1 Loss: 0.9050, D2 Loss: 0.1604\n",
      "Batch 31, G1 Loss: 2.4049, G2 Loss: 4.7102, D1 Loss: 1.0844, D2 Loss: 0.1196\n",
      "Batch 32, G1 Loss: 1.0609, G2 Loss: 4.6656, D1 Loss: 1.1528, D2 Loss: 0.1387\n",
      "Epoch 99 Average -> G1 Loss: 2.1656384468078613, G2 Loss: 6.5176191329956055, D1 Loss: 0.8217816948890686, D2 Loss: 0.26042377948760986\n",
      "Epoch 100/100\n",
      "Batch 1, G1 Loss: 3.3573, G2 Loss: 2.9671, D1 Loss: 0.7638, D2 Loss: 0.1461\n",
      "Batch 2, G1 Loss: 1.7707, G2 Loss: 2.8441, D1 Loss: 0.5143, D2 Loss: 0.1886\n",
      "Batch 3, G1 Loss: 1.3602, G2 Loss: 7.1215, D1 Loss: 0.7591, D2 Loss: 0.0425\n",
      "Batch 4, G1 Loss: 2.4605, G2 Loss: 6.8645, D1 Loss: 0.7623, D2 Loss: 0.1658\n",
      "Batch 5, G1 Loss: 1.2480, G2 Loss: 3.0945, D1 Loss: 1.2596, D2 Loss: 0.1365\n",
      "Batch 6, G1 Loss: 1.3722, G2 Loss: 2.9695, D1 Loss: 1.2477, D2 Loss: 0.2284\n",
      "Batch 7, G1 Loss: 1.7515, G2 Loss: 5.9241, D1 Loss: 1.0343, D2 Loss: 0.1138\n",
      "Batch 8, G1 Loss: 1.2346, G2 Loss: 3.8580, D1 Loss: 0.9236, D2 Loss: 0.1475\n",
      "Batch 9, G1 Loss: 2.8447, G2 Loss: 2.0465, D1 Loss: 0.7867, D2 Loss: 0.4038\n",
      "Batch 10, G1 Loss: 1.9696, G2 Loss: 9.1641, D1 Loss: 0.5774, D2 Loss: 0.1951\n",
      "Batch 11, G1 Loss: 1.5209, G2 Loss: 10.4280, D1 Loss: 0.5291, D2 Loss: 0.1081\n",
      "Batch 12, G1 Loss: 4.1361, G2 Loss: 8.5652, D1 Loss: 0.9166, D2 Loss: 0.1476\n",
      "Batch 13, G1 Loss: 0.7174, G2 Loss: 3.2565, D1 Loss: 1.4414, D2 Loss: 0.0783\n",
      "Batch 14, G1 Loss: 5.7292, G2 Loss: 2.3238, D1 Loss: 1.4940, D2 Loss: 0.1787\n",
      "Batch 15, G1 Loss: 2.6267, G2 Loss: 6.4702, D1 Loss: 0.4864, D2 Loss: 0.0142\n",
      "Batch 16, G1 Loss: 1.0634, G2 Loss: 10.0707, D1 Loss: 0.9269, D2 Loss: 0.0656\n",
      "Batch 17, G1 Loss: 2.5619, G2 Loss: 6.6237, D1 Loss: 0.5620, D2 Loss: 0.0369\n",
      "Batch 18, G1 Loss: 2.9533, G2 Loss: 4.3269, D1 Loss: 0.6155, D2 Loss: 0.0931\n",
      "Batch 19, G1 Loss: 1.5552, G2 Loss: 2.1834, D1 Loss: 0.7657, D2 Loss: 0.4396\n",
      "Batch 20, G1 Loss: 1.7476, G2 Loss: 9.0954, D1 Loss: 0.7740, D2 Loss: 0.1562\n",
      "Batch 21, G1 Loss: 2.5073, G2 Loss: 9.5668, D1 Loss: 0.7391, D2 Loss: 0.0797\n",
      "Batch 22, G1 Loss: 1.3154, G2 Loss: 6.2880, D1 Loss: 0.9065, D2 Loss: 0.3066\n",
      "Batch 23, G1 Loss: 2.1818, G2 Loss: 7.1924, D1 Loss: 0.7932, D2 Loss: 0.0118\n",
      "Batch 24, G1 Loss: 1.5869, G2 Loss: 12.1136, D1 Loss: 0.8407, D2 Loss: 0.0687\n",
      "Batch 25, G1 Loss: 1.9064, G2 Loss: 6.7751, D1 Loss: 0.7859, D2 Loss: 0.0247\n",
      "Batch 26, G1 Loss: 1.6404, G2 Loss: 0.3379, D1 Loss: 0.9138, D2 Loss: 2.3694\n",
      "Batch 27, G1 Loss: 2.0174, G2 Loss: 25.9840, D1 Loss: 0.6566, D2 Loss: 1.5228\n",
      "Batch 28, G1 Loss: 1.4998, G2 Loss: 27.5658, D1 Loss: 0.9608, D2 Loss: 1.3787\n",
      "Batch 29, G1 Loss: 1.8296, G2 Loss: 21.7666, D1 Loss: 0.8735, D2 Loss: 0.2069\n",
      "Batch 30, G1 Loss: 1.2937, G2 Loss: 15.4750, D1 Loss: 0.8420, D2 Loss: 0.0078\n",
      "Batch 31, G1 Loss: 2.4151, G2 Loss: 9.3612, D1 Loss: 1.0328, D2 Loss: 0.0944\n",
      "Batch 32, G1 Loss: 1.1646, G2 Loss: 4.1713, D1 Loss: 1.0992, D2 Loss: 0.0557\n",
      "Epoch 100 Average -> G1 Loss: 2.041862726211548, G2 Loss: 8.024860382080078, D1 Loss: 0.8620107173919678, D2 Loss: 0.2879277467727661\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.metrics import Mean\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa  # For Instance Normalization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 2e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "batch_size = 32\n",
    "EPOCHS = 100\n",
    "image_shape = (64, 64, 3)  # Shape of the generated face images\n",
    "sketch_shape = (64, 64, 1)  # Shape of the generated sketches\n",
    "output_dir = \"cycleGAN_output\"  # Directory to save generated images\n",
    "model_dir = \"saved_models\"  # Directory to save the model\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"fake_images\"), exist_ok=True)  # Directory for fake images\n",
    "os.makedirs(os.path.join(output_dir, \"fake_sketches\"), exist_ok=True)  # Directory for fake sketches\n",
    "\n",
    "# Loss functions\n",
    "pixel_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cycle_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "contextual_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Instance Normalization layer\n",
    "def instance_norm():\n",
    "    return tfa.layers.InstanceNormalization(axis=-1)\n",
    "\n",
    "# Define the generator (for both sketch-to-face and face-to-sketch)\n",
    "def build_generator(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Downsampling\n",
    "    x = layers.Conv2D(64, kernel_size=4, strides=2, padding='same')(inputs)  # 64x64 -> 32x32\n",
    "    x = instance_norm()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, kernel_size=4, strides=2, padding='same')(x)  # 32x32 -> 16x16\n",
    "    x = instance_norm()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(256, kernel_size=4, strides=2, padding='same')(x)  # 16x16 -> 8x8\n",
    "    x = instance_norm()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    # Upsampling\n",
    "    x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same')(x)  # 8x8 -> 16x16\n",
    "    x = instance_norm()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same')(x)  # 16x16 -> 32x32\n",
    "    x = instance_norm()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding='same')(x)  # 32x32 -> 64x64\n",
    "    x = instance_norm()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    # Output 1 channel for sketches, 3 channels for faces\n",
    "    output_channels = 1 if input_shape[-1] == 3 else 3  # Expect sketch if input is face, and vice versa\n",
    "    x = layers.Conv2D(output_channels, kernel_size=7, padding='same', activation='tanh')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, x)\n",
    "\n",
    "# Define the discriminator\n",
    "def build_discriminator(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, kernel_size=4, strides=2, padding='same')(inputs)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, kernel_size=4, strides=2, padding='same')(x)\n",
    "    x = instance_norm()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(256, kernel_size=4, strides=2, padding='same')(x)\n",
    "    x = instance_norm()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, x)\n",
    "\n",
    "# Instantiate models\n",
    "G1 = build_generator(image_shape)  # Face to sketch\n",
    "G2 = build_generator(sketch_shape)  # Sketch to face\n",
    "D1 = build_discriminator(sketch_shape)  # Discriminator for sketches\n",
    "D2 = build_discriminator(image_shape)  # Discriminator for real faces\n",
    "\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "# Build the optimizers with the model variables\n",
    "generator_optimizer.build(G1.trainable_variables + G2.trainable_variables)\n",
    "discriminator_optimizer.build(D1.trainable_variables + D2.trainable_variables)\n",
    "\n",
    "\n",
    "# CycleGAN loss functions\n",
    "def generator_loss(fake_output):\n",
    "    return contextual_loss(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = contextual_loss(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = contextual_loss(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def cycle_consistency_loss(real_image, cycled_image):\n",
    "    return cycle_loss(real_image, cycled_image)\n",
    "\n",
    "# Training step for CycleGAN\n",
    "@tf.function\n",
    "def train_step(real_faces, real_sketches):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generate sketches from faces and vice versa\n",
    "        fake_sketches = G1(real_faces, training=True)\n",
    "        fake_faces = G2(real_sketches, training=True)\n",
    "\n",
    "        # Reconstruct images\n",
    "        cycled_faces = G2(fake_sketches, training=True)\n",
    "        cycled_sketches = G1(fake_faces, training=True)\n",
    "\n",
    "        # Discriminator outputs\n",
    "        real_sketch_output = D1(real_sketches, training=True)\n",
    "        fake_sketch_output = D1(fake_sketches, training=True)\n",
    "\n",
    "        real_face_output = D2(real_faces, training=True)\n",
    "        fake_face_output = D2(fake_faces, training=True)\n",
    "\n",
    "        # Generator losses\n",
    "        G1_loss = generator_loss(fake_sketch_output) + cycle_consistency_loss(real_faces, cycled_faces)\n",
    "        G2_loss = generator_loss(fake_face_output) + cycle_consistency_loss(real_sketches, cycled_sketches)\n",
    "\n",
    "        # Discriminator losses\n",
    "        D1_loss = discriminator_loss(real_sketch_output, fake_sketch_output)\n",
    "        D2_loss = discriminator_loss(real_face_output, fake_face_output)\n",
    "\n",
    "    # Get gradients and apply them\n",
    "    G1_gradients = tape.gradient(G1_loss, G1.trainable_variables)\n",
    "    G2_gradients = tape.gradient(G2_loss, G2.trainable_variables)\n",
    "    D1_gradients = tape.gradient(D1_loss, D1.trainable_variables)\n",
    "    D2_gradients = tape.gradient(D2_loss, D2.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(G1_gradients, G1.trainable_variables))\n",
    "    generator_optimizer.apply_gradients(zip(G2_gradients, G2.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(D1_gradients, D1.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(D2_gradients, D2.trainable_variables))\n",
    "\n",
    "    return G1_loss, G2_loss, D1_loss, D2_loss  # Return the losses\n",
    "\n",
    "# Prepare dataset\n",
    "def load_data(sketch_dir, image_dir):\n",
    "    sketch_images = []\n",
    "    real_images = []\n",
    "    \n",
    "    for sketch_file in os.listdir(sketch_dir):\n",
    "        if sketch_file.endswith(\".png\") or sketch_file.endswith(\".jpg\"):\n",
    "            sketch_path = os.path.join(sketch_dir, sketch_file)\n",
    "            image_path = os.path.join(image_dir, sketch_file)  # Assuming matching filenames\n",
    "            \n",
    "            # Load and preprocess sketch (64x64 grayscale)\n",
    "            sketch = tf.keras.preprocessing.image.load_img(sketch_path, color_mode='grayscale', target_size=(64, 64))\n",
    "            sketch = tf.keras.preprocessing.image.img_to_array(sketch) / 255.0\n",
    "            sketch_images.append(sketch)\n",
    "\n",
    "            # Load and preprocess real image (64x64 RGB)\n",
    "            real_image = tf.keras.preprocessing.image.load_img(image_path, target_size=(64, 64))  # Resizing to 64x64\n",
    "            real_image = tf.keras.preprocessing.image.img_to_array(real_image) / 255.0\n",
    "            real_images.append(real_image)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    return np.array(sketch_images), np.array(real_images)\n",
    "\n",
    "sketches_folder = '/Users/zainabaslam/Local Docs/GenAI-A2/Dataset/train/sketches'  # Directory containing sketches\n",
    "images_folder = '/Users/zainabaslam/Local Docs/GenAI-A2/Dataset/train/photos'     \n",
    "train_sketches, train_images = load_data(sketches_folder, images_folder)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sketches, train_images))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    \n",
    "    # Reset the loss metrics\n",
    "    G1_loss_avg = Mean()\n",
    "    G2_loss_avg = Mean()\n",
    "    D1_loss_avg = Mean()\n",
    "    D2_loss_avg = Mean()\n",
    "\n",
    "    for step, (real_sketches, real_faces) in enumerate(train_dataset):\n",
    "        if step >= 32:  # Only run for 32 batches\n",
    "            break\n",
    "        \n",
    "        G1_loss, G2_loss, D1_loss, D2_loss = train_step(real_faces, real_sketches)\n",
    "\n",
    "        # Update average losses\n",
    "        G1_loss_avg.update_state(G1_loss)\n",
    "        G2_loss_avg.update_state(G2_loss)\n",
    "        D1_loss_avg.update_state(D1_loss)\n",
    "        D2_loss_avg.update_state(D2_loss)\n",
    "\n",
    "        # Print the losses for the current batch\n",
    "        print(f'Batch {step + 1}, G1 Loss: {G1_loss.numpy():.4f}, G2 Loss: {G2_loss.numpy():.4f}, D1 Loss: {D1_loss.numpy():.4f}, D2 Loss: {D2_loss.numpy():.4f}')\n",
    "\n",
    "    # Log epoch results\n",
    "    print(f'Epoch {epoch + 1} Average -> G1 Loss: {G1_loss_avg.result()}, G2 Loss: {G2_loss_avg.result()}, D1 Loss: {D1_loss_avg.result()}, D2 Loss: {D2_loss_avg.result()}')\n",
    "    \n",
    "    # Save fake images and sketches at the end of each epoch\n",
    "    fake_images = G2(real_sketches)  # Generate fake faces from real sketches\n",
    "    fake_sketches = G1(real_faces)  # Generate fake sketches from real faces\n",
    "\n",
    "    # Ensure to use the correct number of images to save\n",
    "    for i in range(min(batch_size, fake_images.shape[0])):  # Adjust to the actual number of generated images\n",
    "        # Save generated fake faces\n",
    "        img_array = (fake_images[i].numpy() * 255).astype(np.uint8)  # Convert tensor to numpy array\n",
    "        tf.keras.preprocessing.image.save_img(os.path.join(output_dir, \"fake_images\", f\"fake_face_epoch_{epoch + 1}_img_{i + 1}.png\"), img_array)\n",
    "\n",
    "        # Save generated fake sketches\n",
    "        sketch_array = (fake_sketches[i].numpy() * 255).astype(np.uint8)  # Convert tensor to numpy array\n",
    "        tf.keras.preprocessing.image.save_img(os.path.join(output_dir, \"fake_sketches\", f\"fake_sketch_epoch_{epoch + 1}_img_{i + 1}.png\"), sketch_array)\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb4c5bdf-af2d-49f2-b837-20e3f54bf4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/G1_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/G1_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/G2_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/G2_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/D1_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/D1_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/D2_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/D2_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming G1 and G2 are your generator models and D1 and D2 are your discriminator models\n",
    "\n",
    "# Save the generator and discriminator models\n",
    "G1.save('./saved_model/G1_model')\n",
    "G2.save('./saved_model/G2_model')\n",
    "D1.save('./saved_model/D1_model')\n",
    "D2.save('./saved_model/D2_model')\n",
    "\n",
    "print(\"Models saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "965b3da4-a5f5-4b59-8342-ac7f266d77b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/panda/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming G1 and G2 are your generator models and D1 and D2 are your discriminator models\n",
    "\n",
    "# Save the generator and discriminator models in HDF5 format\n",
    "G1.save('./saved_model/G1_model.h5')\n",
    "G2.save('./saved_model/G2_model.h5')\n",
    "D1.save('./saved_model/D1_model.h5')\n",
    "D2.save('./saved_model/D2_model.h5')\n",
    "\n",
    "print(\"Models saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8bedc5-8048-44e5-8784-fdab877cfbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
